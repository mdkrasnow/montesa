{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEACHÂ 1.0Â ReliabilityÂ ExamÂ MetricsÂ &Â ModelÂ Comparison\n",
    "\n",
    "17Â JuneÂ 2025  \n",
    "**Author:** Matt Krasnow\n",
    "\n",
    "This notebook reproduces the full statistical comparison of a series of AIâ€‘based TEACH classroomâ€‘observation evaluators described in the accompanying project plan.  It:\n",
    "\n",
    "1. **Loads** all modelâ€‘output CSVs and the human (\"master\") reference scores.  \n",
    "2. **Computes** pass/fail per segment, overall agreement, elementâ€‘level distances and other key metrics.  \n",
    "3. **Runs** formal significance tests (Cochranâ€™sÂ Q, pairwise McNemar, Friedman & paired *t* / Wilcoxon) to quantify performance gaps.  \n",
    "4. **Generates** publicationâ€‘quality visualisations (bar, line & box plots; domain breakdowns).  \n",
    "5. **Simulates** the full threeâ€‘segment TEACH reliability exam to estimate certification probabilities.  \n",
    "\n",
    "All code is modular so that adding new evaluator runs (e.g.Â `7â€‘â€¦csv`) automatically propagates through the analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0Â Â Environment &amp; prerequisites\n",
    "\n",
    "*PythonÂ â‰¥â€¯3.9,Â pandasÂ â‰¥â€¯1.5,Â numpyÂ â‰¥â€¯1.23,Â matplotlibÂ â‰¥â€¯3.7,Â SciPyÂ â‰¥â€¯1.11 andÂ statsmodelsÂ â‰¥â€¯0.14 are assumed.*  \n",
    "Install missing packages with:\n",
    "\n",
    "```bash\n",
    "pip install pandas numpy matplotlib scipy statsmodels\n",
    "```\n",
    "\n",
    "```\n",
    "new/\n",
    "  rawData/Peru/model_evaluation_data/*_evaluations.csv\n",
    "  formattedData/peru_cleaned_transcripts.csv            # full human dataset (optional, for crossâ€‘checks)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "080007c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# 1Â Â Imports &Â global settings\n",
    "# -------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob, os, json, itertools, random, warnings\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from statsmodels.stats.contingency_tables import cochrans_q\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# make plots a bit larger by default\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['figure.dpi'] = 120\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb104af9",
   "metadata": {},
   "source": [
    "## 2Â Â Configuration â€“ paths &Â basic helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b330ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚Â DetectedÂ 6Â evaluator result files:\n",
      "    Â» 0-TEST-Control_evaluations.csv\n",
      "    Â» 1-TEST-BaseEvaluator_evaluations.csv\n",
      "    Â» 2-TEST-Medium-Rubric_evaluations.csv\n",
      "    Â» 3-TEST-High-Rubric_evaluations.csv\n",
      "    Â» 4-TEST-High-Rubric-Reasoning_evaluations.csv\n",
      "    Â» 5-TEST-High-Rubric-Timestamped_evaluations.csv\n"
     ]
    }
   ],
   "source": [
    "# Root of the cloned repo / project (adjust if running elsewhere)\n",
    "ROOT_DIR = Path('.')                  # assume notebook is at project root\n",
    "EVAL_DIR = ROOT_DIR / 'new' / 'rawData' / 'Peru' / 'model_evaluation_data'\n",
    "HUMAN_FILE = ROOT_DIR / 'new' / 'formattedData' / 'test_only_peru_cleaned_transcripts.csv'\n",
    "\n",
    "assert EVAL_DIR.exists(), f\"Evaluation directory not found: {EVAL_DIR}\"\n",
    "assert HUMAN_FILE.exists(), f\"Human reference file not found: {HUMAN_FILE}\"\n",
    "\n",
    "# Retrieve all evaluator CSVs automatically --------------------------------------------------\n",
    "eval_paths = sorted(EVAL_DIR.glob('*_evaluations.csv'))\n",
    "if not eval_paths:\n",
    "    raise FileNotFoundError('No *_evaluations.csv files detected â€“ check path.')\n",
    "\n",
    "print(f\"ðŸ“‚Â DetectedÂ {len(eval_paths)}Â evaluator result files:\\n    Â» \" + \"\\n    Â» \".join([p.name for p in eval_paths]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62790f57",
   "metadata": {},
   "source": [
    "## 3Â Â UtilityÂ functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b60ede44",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# 3.1Â Score normalisation helpers          #\n",
    "############################################\n",
    "# Map highâ€‘inference letter codes to ints; keep numeric 1â€‘5 untouched.\n",
    "_LETTER_TO_INT = {'L': 1, 'M': 2, 'H': 3, 'Y': 1, 'N': 0, '': np.nan, np.nan: np.nan}\n",
    "\n",
    "def _as_numeric(x):\n",
    "    \"\"\"Convert rubric value to numeric for distance calc / Â±1 rule.\"\"\"\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    try:\n",
    "        return float(x)\n",
    "    except ValueError:\n",
    "        return _LETTER_TO_INT.get(str(x).strip(), np.nan)\n",
    "\n",
    "############################################\n",
    "# 3.2Â TEACH pass/fail logic per segment    #\n",
    "############################################\n",
    "TOT_TEACHER_COLS = [\n",
    "    'Teacher provides learning activity - 1st Snapshot',\n",
    "    'Teacher provides learning activity - 2nd Snapshot',\n",
    "    'Teacher provides learning activity - 3rd Snapshot',\n",
    "]\n",
    "TOT_STUDENTS_COLS = [\n",
    "    'Students are on task - 1st Snapshot',\n",
    "    'Students are on task - 2nd Snapshot',\n",
    "    'Students are on task - 3rd Snapshot',\n",
    "]\n",
    "\n",
    "# 9 highâ€‘inference domain columns -----------------------------------------\n",
    "DOMAIN_COLS = [\n",
    "    'Supportive Learning Environment',\n",
    "    'Positive Behavioral Expectations',\n",
    "    'Lesson Facilitation',\n",
    "    'Checks for understanding',\n",
    "    'Feedback',\n",
    "    'Critical Thinking',\n",
    "    'Autonomy',\n",
    "    'Perseverance',\n",
    "    'Social & Collaborative Skills',\n",
    "]\n",
    "\n",
    "def time_on_task_pass(row_model: pd.Series, row_human: pd.Series) -> bool:\n",
    "    \"\"\"Return True if ToT element passes: any â‰¥2 snapshots exactly match (teacherÂ +Â students).\"\"\"\n",
    "    matches = 0\n",
    "    for t_col, s_col in zip(TOT_TEACHER_COLS, TOT_STUDENTS_COLS):\n",
    "        if row_model[t_col] == row_human[t_col] and row_model[s_col] == row_human[s_col]:\n",
    "            matches += 1\n",
    "    return matches >= 2\n",
    "\n",
    "def domain_pass(model_val, human_val) -> bool:\n",
    "    \"\"\"Domain passes if numeric difference â‰¤1 (for 1â€‘5Â or L/M/HÂ after mapping).\"\"\"\n",
    "    mv = _as_numeric(model_val)\n",
    "    hv = _as_numeric(human_val)\n",
    "    if pd.isna(mv) or pd.isna(hv):\n",
    "        return False   # treat missing as fail (could be adjusted)\n",
    "    return abs(mv - hv) <= 1\n",
    "\n",
    "def segment_pass(row_model: pd.Series, row_human: pd.Series) -> bool:\n",
    "    \"\"\"Implements TEACH 8/10 reliability rule for one 15â€‘min segment.\"\"\"\n",
    "    passes = int(time_on_task_pass(row_model, row_human))\n",
    "    for col in DOMAIN_COLS:\n",
    "        passes += int(domain_pass(row_model[col], row_human[col]))\n",
    "    return passes >= 8\n",
    "\n",
    "############################################\n",
    "# 3.3Â Overall distance (strict mismatch)   #\n",
    "############################################\n",
    "ALL_COMPONENTS = TOT_TEACHER_COLS + TOT_STUDENTS_COLS + DOMAIN_COLS\n",
    "\n",
    "def strict_distance(row_model: pd.Series, row_human: pd.Series) -> float:\n",
    "    \"\"\"Simple fraction of rubric items that differ (0Â = perfect agreement, 1Â = none match).\"\"\"\n",
    "    mismatches = 0\n",
    "    comparisons = 0\n",
    "    for col in ALL_COMPONENTS:\n",
    "        if pd.isna(row_human[col]):\n",
    "            continue\n",
    "        comparisons += 1\n",
    "        mismatches += int(row_model[col] != row_human[col])\n",
    "    return mismatches / comparisons if comparisons else np.nan\n",
    "\n",
    "############################################\n",
    "# 3.4Â Exam simulation utility              #\n",
    "############################################\n",
    "def simulate_exam(segment_passes: np.ndarray, n_iter: int = 10000, seed: int = 42):\n",
    "    \"\"\"Monteâ€‘Carlo simulate 3â€‘segment TEACH exam â€“ returns firstâ€‘try & twoâ€‘try pass probs.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n_segments = len(segment_passes)\n",
    "    first_try = 0\n",
    "    second_try = 0\n",
    "    for _ in range(n_iter):\n",
    "        sample = rng.choice(segment_passes, size=3, replace=False)\n",
    "        if sample.all():\n",
    "            first_try += 1\n",
    "            second_try += 1\n",
    "            continue\n",
    "        # second attempt (draw 3 different segments)\n",
    "        remaining = rng.choice(segment_passes, size=3, replace=False)\n",
    "        if remaining.all():\n",
    "            second_try += 1\n",
    "    return first_try / n_iter, second_try / n_iter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2079d7f0",
   "metadata": {},
   "source": [
    "## 4Â Â Load human reference scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0cb271a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ…Â Human reference loaded with 84 rows and 73 columns.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>School_Clip</th>\n",
       "      <th>Teacher provides learning activity - 1st Snapshot</th>\n",
       "      <th>Students are on task - 1st Snapshot</th>\n",
       "      <th>Teacher provides learning activity - 2nd Snapshot</th>\n",
       "      <th>Students are on task - 2nd Snapshot</th>\n",
       "      <th>Teacher provides learning activity - 3rd Snapshot</th>\n",
       "      <th>Students are on task - 3rd Snapshot</th>\n",
       "      <th>Supportive Learning Environment</th>\n",
       "      <th>The teacher treats all students respectfully</th>\n",
       "      <th>The teacher uses positive language</th>\n",
       "      <th>...</th>\n",
       "      <th>Last Audio Transcript Language Probability</th>\n",
       "      <th>Last Audio Transcript Word Count</th>\n",
       "      <th>Last Audio Transcript Duration Seconds</th>\n",
       "      <th>Last Audio Transcript Speaker Count</th>\n",
       "      <th>Last Audio Transcript Has Audio Events</th>\n",
       "      <th>base_id</th>\n",
       "      <th>First Audio Transcript Estimated Duration Seconds</th>\n",
       "      <th>Last Audio Transcript Estimated Duration Seconds</th>\n",
       "      <th>score_distribution</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>256305 Clip 1</td>\n",
       "      <td>Y</td>\n",
       "      <td>H</td>\n",
       "      <td>Y</td>\n",
       "      <td>H</td>\n",
       "      <td>Y</td>\n",
       "      <td>H</td>\n",
       "      <td>4</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>256305</td>\n",
       "      <td>8.0</td>\n",
       "      <td>510.8</td>\n",
       "      <td>{\"M\": 12, \"H\": 10, \"L\": 9, \"2\": 5, \"Y\": 3, \"4\"...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>256305 Clip 2</td>\n",
       "      <td>Y</td>\n",
       "      <td>H</td>\n",
       "      <td>N</td>\n",
       "      <td>n</td>\n",
       "      <td>Y</td>\n",
       "      <td>M</td>\n",
       "      <td>4</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1229</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>256305</td>\n",
       "      <td>8.0</td>\n",
       "      <td>491.6</td>\n",
       "      <td>{\"M\": 12, \"L\": 9, \"H\": 7, \"2\": 6, \"n\": 3, \"4\":...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 73 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     School_Clip Teacher provides learning activity - 1st Snapshot  \\\n",
       "0  256305 Clip 1                                                 Y   \n",
       "1  256305 Clip 2                                                 Y   \n",
       "\n",
       "  Students are on task - 1st Snapshot  \\\n",
       "0                                   H   \n",
       "1                                   H   \n",
       "\n",
       "  Teacher provides learning activity - 2nd Snapshot  \\\n",
       "0                                                 Y   \n",
       "1                                                 N   \n",
       "\n",
       "  Students are on task - 2nd Snapshot  \\\n",
       "0                                   H   \n",
       "1                                   n   \n",
       "\n",
       "  Teacher provides learning activity - 3rd Snapshot  \\\n",
       "0                                                 Y   \n",
       "1                                                 Y   \n",
       "\n",
       "  Students are on task - 3rd Snapshot  Supportive Learning Environment  \\\n",
       "0                                   H                                4   \n",
       "1                                   M                                4   \n",
       "\n",
       "  The teacher treats all students respectfully  \\\n",
       "0                                            H   \n",
       "1                                            H   \n",
       "\n",
       "  The teacher uses positive language  ...  \\\n",
       "0                                  H  ...   \n",
       "1                                  H  ...   \n",
       "\n",
       "  Last Audio Transcript Language Probability Last Audio Transcript Word Count  \\\n",
       "0                                          0                             1277   \n",
       "1                                          0                             1229   \n",
       "\n",
       "   Last Audio Transcript Duration Seconds Last Audio Transcript Speaker Count  \\\n",
       "0                                     0.0                                   0   \n",
       "1                                     0.0                                   0   \n",
       "\n",
       "  Last Audio Transcript Has Audio Events base_id  \\\n",
       "0                                  False  256305   \n",
       "1                                  False  256305   \n",
       "\n",
       "   First Audio Transcript Estimated Duration Seconds  \\\n",
       "0                                                8.0   \n",
       "1                                                8.0   \n",
       "\n",
       "  Last Audio Transcript Estimated Duration Seconds  \\\n",
       "0                                            510.8   \n",
       "1                                            491.6   \n",
       "\n",
       "                                  score_distribution split  \n",
       "0  {\"M\": 12, \"H\": 10, \"L\": 9, \"2\": 5, \"Y\": 3, \"4\"...  test  \n",
       "1  {\"M\": 12, \"L\": 9, \"H\": 7, \"2\": 6, \"n\": 3, \"4\":...  test  \n",
       "\n",
       "[2 rows x 73 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_df = pd.read_csv(HUMAN_FILE)\n",
    "print(f\"âœ…Â Human reference loaded with {human_df.shape[0]} rows and {human_df.shape[1]} columns.\")\n",
    "human_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a6af94",
   "metadata": {},
   "source": [
    "## 5Â Â Aggregate evaluator results &Â compute metrics\n",
    "\n",
    "The next cell loops through every `*_evaluations.csv`, aligns rows via the `School_Clip` identifier, and calculates:\n",
    "\n",
    "* **`segment_pass`** â€“ booleanÂ (â‰¥8Â ofÂ 10 elements correct) for each 15â€‘min clip.  \n",
    "* **`distance`** â€“ strict mismatch rate across all rubric components.  \n",
    "* **`overall_pass_rate`** â€“ proportion of clips passed by the model.  \n",
    "* **`mean_distance`** â€“ average strict distance.  \n",
    "* **`first_try_cert` / `two_try_cert`** â€“ probability of clearing a full 3â€‘segment exam in one or at most two attempts (10â€¯k Monteâ€‘Carlo).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb719828",
   "metadata": {},
   "outputs": [],
   "source": [
    "component_names = ALL_COMPONENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "623638e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”— 0-TEST-Control_evaluations: merged 84 common segments.\n",
      "ðŸ”— 1-TEST-BaseEvaluator_evaluations: merged 84 common segments.\n",
      "ðŸ”— 2-TEST-Medium-Rubric_evaluations: merged 84 common segments.\n",
      "ðŸ”— 3-TEST-High-Rubric_evaluations: merged 84 common segments.\n",
      "ðŸ”— 4-TEST-High-Rubric-Reasoning_evaluations: merged 84 common segments.\n",
      "ðŸ”— 5-TEST-High-Rubric-Timestamped_evaluations: merged 84 common segments.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>n_segments</th>\n",
       "      <th>segment_pass_rate</th>\n",
       "      <th>mean_strict_distance</th>\n",
       "      <th>exam_pass_prob_first_try</th>\n",
       "      <th>exam_pass_prob_two_tries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0-TEST-Control_evaluations</td>\n",
       "      <td>84</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.503175</td>\n",
       "      <td>0.2629</td>\n",
       "      <td>0.4597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1-TEST-BaseEvaluator_evaluations</td>\n",
       "      <td>84</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.657937</td>\n",
       "      <td>0.2646</td>\n",
       "      <td>0.4553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2-TEST-Medium-Rubric_evaluations</td>\n",
       "      <td>84</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.592063</td>\n",
       "      <td>0.3578</td>\n",
       "      <td>0.5847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3-TEST-High-Rubric_evaluations</td>\n",
       "      <td>84</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.517460</td>\n",
       "      <td>0.5851</td>\n",
       "      <td>0.8191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4-TEST-High-Rubric-Reasoning_evaluations</td>\n",
       "      <td>84</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.513492</td>\n",
       "      <td>0.5653</td>\n",
       "      <td>0.8041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5-TEST-High-Rubric-Timestamped_evaluations</td>\n",
       "      <td>84</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.519841</td>\n",
       "      <td>0.5299</td>\n",
       "      <td>0.7754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        model  n_segments  segment_pass_rate  \\\n",
       "0                  0-TEST-Control_evaluations          84           0.642857   \n",
       "1            1-TEST-BaseEvaluator_evaluations          84           0.642857   \n",
       "2            2-TEST-Medium-Rubric_evaluations          84           0.714286   \n",
       "3              3-TEST-High-Rubric_evaluations          84           0.833333   \n",
       "4    4-TEST-High-Rubric-Reasoning_evaluations          84           0.821429   \n",
       "5  5-TEST-High-Rubric-Timestamped_evaluations          84           0.809524   \n",
       "\n",
       "   mean_strict_distance  exam_pass_prob_first_try  exam_pass_prob_two_tries  \n",
       "0              0.503175                    0.2629                    0.4597  \n",
       "1              0.657937                    0.2646                    0.4553  \n",
       "2              0.592063                    0.3578                    0.5847  \n",
       "3              0.517460                    0.5851                    0.8191  \n",
       "4              0.513492                    0.5653                    0.8041  \n",
       "5              0.519841                    0.5299                    0.7754  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_records = []            # per-model summary rows\n",
    "per_segment_matrix = {}       # model â†’ bool array of segment passes (for stats)\n",
    "\n",
    "for path in eval_paths:\n",
    "    model_name = path.stem\n",
    "    model_df = pd.read_csv(path)\n",
    "\n",
    "    # Ensure common key column exists in both dataframes ------------------\n",
    "    key_col = 'School_Clip'\n",
    "    assert key_col in model_df.columns and key_col in human_df.columns, \"Key column missing.\"\n",
    "\n",
    "    # Index both on the key and restrict to the same segments -------------\n",
    "    model_idx = model_df.set_index(key_col)\n",
    "    human_idx = human_df.set_index(key_col)\n",
    "    common_keys = model_idx.index.intersection(human_idx.index)\n",
    "    print(f\"ðŸ”— {model_name}: merged {len(common_keys)} common segments.\")\n",
    "\n",
    "    # Pull out exactly the columns we care about (no suffixes) ------------\n",
    "    m_vals = model_idx.loc[common_keys, component_names]\n",
    "    h_vals = human_idx.loc[common_keys, component_names]\n",
    "\n",
    "    # Compute per-segment pass/fail and strict distance --------------------\n",
    "    passes = []\n",
    "    distances = []\n",
    "    for m_row, h_row in zip(m_vals.itertuples(index=False), h_vals.itertuples(index=False)):\n",
    "        row_m = pd.Series(m_row, index=component_names)\n",
    "        row_h = pd.Series(h_row, index=component_names)\n",
    "        passes.append(segment_pass(row_m, row_h))\n",
    "        distances.append(strict_distance(row_m, row_h))\n",
    "\n",
    "    passes = np.array(passes, dtype=bool)\n",
    "    distances = np.array(distances, dtype=float)\n",
    "    first_try, two_try = simulate_exam(passes)\n",
    "\n",
    "    # Build summary record ---------------------------------------------------\n",
    "    record = {\n",
    "        'model': model_name,\n",
    "        'n_segments': len(passes),\n",
    "        'segment_pass_rate': passes.mean(),\n",
    "        'mean_strict_distance': np.nanmean(distances),\n",
    "        'exam_pass_prob_first_try': first_try,\n",
    "        'exam_pass_prob_two_tries': two_try,\n",
    "    }\n",
    "    model_records.append(record)\n",
    "    per_segment_matrix[model_name] = passes\n",
    "\n",
    "# Final summary DataFrame -----------------------------------------------\n",
    "summary_df = pd.DataFrame(model_records).sort_values('model')\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e66ce04",
   "metadata": {},
   "source": [
    "### Quick lookÂ â€“Â overall ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f49d168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_108be\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_108be_level0_col0\" class=\"col_heading level0 col0\" >model</th>\n",
       "      <th id=\"T_108be_level0_col1\" class=\"col_heading level0 col1\" >segment_pass_rate</th>\n",
       "      <th id=\"T_108be_level0_col2\" class=\"col_heading level0 col2\" >exam_pass_prob_first_try</th>\n",
       "      <th id=\"T_108be_level0_col3\" class=\"col_heading level0 col3\" >mean_strict_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_108be_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_108be_row0_col0\" class=\"data row0 col0\" >0-TEST-Control_evaluations</td>\n",
       "      <td id=\"T_108be_row0_col1\" class=\"data row0 col1\" >64.3%</td>\n",
       "      <td id=\"T_108be_row0_col2\" class=\"data row0 col2\" >26.3%</td>\n",
       "      <td id=\"T_108be_row0_col3\" class=\"data row0 col3\" >0.503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_108be_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_108be_row1_col0\" class=\"data row1 col0\" >1-TEST-BaseEvaluator_evaluations</td>\n",
       "      <td id=\"T_108be_row1_col1\" class=\"data row1 col1\" >64.3%</td>\n",
       "      <td id=\"T_108be_row1_col2\" class=\"data row1 col2\" >26.5%</td>\n",
       "      <td id=\"T_108be_row1_col3\" class=\"data row1 col3\" >0.658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_108be_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_108be_row2_col0\" class=\"data row2 col0\" >2-TEST-Medium-Rubric_evaluations</td>\n",
       "      <td id=\"T_108be_row2_col1\" class=\"data row2 col1\" >71.4%</td>\n",
       "      <td id=\"T_108be_row2_col2\" class=\"data row2 col2\" >35.8%</td>\n",
       "      <td id=\"T_108be_row2_col3\" class=\"data row2 col3\" >0.592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_108be_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_108be_row3_col0\" class=\"data row3 col0\" >3-TEST-High-Rubric_evaluations</td>\n",
       "      <td id=\"T_108be_row3_col1\" class=\"data row3 col1\" >83.3%</td>\n",
       "      <td id=\"T_108be_row3_col2\" class=\"data row3 col2\" >58.5%</td>\n",
       "      <td id=\"T_108be_row3_col3\" class=\"data row3 col3\" >0.517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_108be_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_108be_row4_col0\" class=\"data row4 col0\" >4-TEST-High-Rubric-Reasoning_evaluations</td>\n",
       "      <td id=\"T_108be_row4_col1\" class=\"data row4 col1\" >82.1%</td>\n",
       "      <td id=\"T_108be_row4_col2\" class=\"data row4 col2\" >56.5%</td>\n",
       "      <td id=\"T_108be_row4_col3\" class=\"data row4 col3\" >0.513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_108be_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_108be_row5_col0\" class=\"data row5 col0\" >5-TEST-High-Rubric-Timestamped_evaluations</td>\n",
       "      <td id=\"T_108be_row5_col1\" class=\"data row5 col1\" >81.0%</td>\n",
       "      <td id=\"T_108be_row5_col2\" class=\"data row5 col2\" >53.0%</td>\n",
       "      <td id=\"T_108be_row5_col3\" class=\"data row5 col3\" >0.520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x14d0cc050>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_cols = ['model', 'segment_pass_rate', 'exam_pass_prob_first_try', 'mean_strict_distance']\n",
    "summary_df[display_cols].style.format({\n",
    "    'segment_pass_rate': '{:.1%}',\n",
    "    'exam_pass_prob_first_try': '{:.1%}',\n",
    "    'mean_strict_distance': '{:.3f}',\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed5b71f",
   "metadata": {},
   "source": [
    "## 6Â Â Visualisations\n",
    "\n",
    "Below: (i) segment passÂ rate bar chart, (ii) improvement trend line, (iii) distribution of elementsâ€‘correct per segment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "af9fa30f",
   "metadata": {},
   "outputs": [
    {
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# (i)Â BarÂ â€“ segment pass rate --------------------------------------------------\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(summary_df['model'], summary_df['segment_pass_rate']*100)\n",
    "ax.set_ylabel('Segment pass rateÂ (%)')\n",
    "ax.set_title('FigureÂ 1Â â€“Â Perâ€‘segment TEACH reliability pass rate by model')\n",
    "for idx, val in enumerate(summary_df['segment_pass_rate']*100):\n",
    "    ax.text(idx, val + 1, f\"{val:.1f}%\", ha='center')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "61ffdbba",
   "metadata": {},
   "outputs": [
    {
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# (ii)Â LineÂ â€“ development trajectory -----------------------------------------\n",
    "ordered = summary_df.sort_values('model')\n",
    "plt.plot(ordered['model'], ordered['segment_pass_rate']*100, marker='o')\n",
    "plt.title('FigureÂ 2Â â€“Â Improvement trajectory across iterative model versions')\n",
    "plt.ylabel('Segment pass rateÂ (%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', ls='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "73281454",
   "metadata": {},
   "outputs": [
    {
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# (iii)Â Box plotÂ â€“ elements correct distribution ------------------------------\n",
    "fig, ax = plt.subplots()\n",
    "elements_correct = []\n",
    "labels = []\n",
    "for model, passes in per_segment_matrix.items():\n",
    "    # we only stored segmentâ€‘level pass/fail; recompute perâ€‘segment â€˜elements matchedâ€™ quickly\n",
    "    path = EVAL_DIR / f\"{model}.csv\" if not model.endswith('.csv') else EVAL_DIR / model\n",
    "    df_m = pd.read_csv(path)\n",
    "    merged = df_m.merge(human_df, on='School_Clip', suffixes=('_model', '_human'))\n",
    "    scores = []\n",
    "    for _, row in merged.iterrows():\n",
    "        row_m = row[[c + '_model' for c in ALL_COMPONENTS]].rename(lambda x: x[:-6])\n",
    "        row_h = row[[c + '_human' for c in ALL_COMPONENTS]].rename(lambda x: x[:-6])\n",
    "        n_pass = int(time_on_task_pass(row_m, row_h))\n",
    "        for col in DOMAIN_COLS:\n",
    "            n_pass += int(domain_pass(row_m[col], row_h[col]))\n",
    "        scores.append(n_pass)\n",
    "    elements_correct.append(scores)\n",
    "    labels.append(model)\n",
    "\n",
    "ax.boxplot(elements_correct, labels=labels, showfliers=False)\n",
    "ax.set_ylabel('Elements passed (0â€“10)')\n",
    "ax.set_title('FigureÂ 3Â â€“Â Distribution of perâ€‘segment elements matched by model')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378ddc51",
   "metadata": {},
   "source": [
    "## 7Â Â Domainâ€‘level error breakdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d9e1a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Supportive Learning Environment</th>\n",
       "      <th>Positive Behavioral Expectations</th>\n",
       "      <th>Lesson Facilitation</th>\n",
       "      <th>Checks for understanding</th>\n",
       "      <th>Feedback</th>\n",
       "      <th>Critical Thinking</th>\n",
       "      <th>Autonomy</th>\n",
       "      <th>Perseverance</th>\n",
       "      <th>Social &amp; Collaborative Skills</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0-TEST-Control_evaluations</th>\n",
       "      <td>1.036145</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>1.084337</td>\n",
       "      <td>1.277108</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.602410</td>\n",
       "      <td>0.819277</td>\n",
       "      <td>0.313253</td>\n",
       "      <td>0.759036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1-TEST-BaseEvaluator_evaluations</th>\n",
       "      <td>0.907895</td>\n",
       "      <td>1.151515</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.675676</td>\n",
       "      <td>0.743590</td>\n",
       "      <td>0.641026</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.742424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-TEST-Medium-Rubric_evaluations</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>0.597403</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.963636</td>\n",
       "      <td>0.743902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-TEST-High-Rubric_evaluations</th>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.773810</td>\n",
       "      <td>0.738095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-TEST-High-Rubric-Reasoning_evaluations</th>\n",
       "      <td>0.345238</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.559524</td>\n",
       "      <td>0.797619</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Supportive Learning Environment  \\\n",
       "model                                                                       \n",
       "0-TEST-Control_evaluations                                       1.036145   \n",
       "1-TEST-BaseEvaluator_evaluations                                 0.907895   \n",
       "2-TEST-Medium-Rubric_evaluations                                 0.444444   \n",
       "3-TEST-High-Rubric_evaluations                                   0.321429   \n",
       "4-TEST-High-Rubric-Reasoning_evaluations                         0.345238   \n",
       "\n",
       "                                          Positive Behavioral Expectations  \\\n",
       "model                                                                        \n",
       "0-TEST-Control_evaluations                                        0.891566   \n",
       "1-TEST-BaseEvaluator_evaluations                                  1.151515   \n",
       "2-TEST-Medium-Rubric_evaluations                                  0.805556   \n",
       "3-TEST-High-Rubric_evaluations                                    0.857143   \n",
       "4-TEST-High-Rubric-Reasoning_evaluations                          0.916667   \n",
       "\n",
       "                                          Lesson Facilitation  \\\n",
       "model                                                           \n",
       "0-TEST-Control_evaluations                           1.084337   \n",
       "1-TEST-BaseEvaluator_evaluations                     0.880000   \n",
       "2-TEST-Medium-Rubric_evaluations                     0.576923   \n",
       "3-TEST-High-Rubric_evaluations                       0.595238   \n",
       "4-TEST-High-Rubric-Reasoning_evaluations             0.595238   \n",
       "\n",
       "                                          Checks for understanding  Feedback  \\\n",
       "model                                                                          \n",
       "0-TEST-Control_evaluations                                1.277108  0.698795   \n",
       "1-TEST-BaseEvaluator_evaluations                          0.958333  0.675676   \n",
       "2-TEST-Medium-Rubric_evaluations                          0.815789  0.597403   \n",
       "3-TEST-High-Rubric_evaluations                            0.916667  0.571429   \n",
       "4-TEST-High-Rubric-Reasoning_evaluations                  0.916667  0.571429   \n",
       "\n",
       "                                          Critical Thinking  Autonomy  \\\n",
       "model                                                                   \n",
       "0-TEST-Control_evaluations                         0.602410  0.819277   \n",
       "1-TEST-BaseEvaluator_evaluations                   0.743590  0.641026   \n",
       "2-TEST-Medium-Rubric_evaluations                   0.750000  0.698795   \n",
       "3-TEST-High-Rubric_evaluations                     0.690476  0.583333   \n",
       "4-TEST-High-Rubric-Reasoning_evaluations           0.666667  0.559524   \n",
       "\n",
       "                                          Perseverance  \\\n",
       "model                                                    \n",
       "0-TEST-Control_evaluations                    0.313253   \n",
       "1-TEST-BaseEvaluator_evaluations              0.904762   \n",
       "2-TEST-Medium-Rubric_evaluations              0.963636   \n",
       "3-TEST-High-Rubric_evaluations                0.773810   \n",
       "4-TEST-High-Rubric-Reasoning_evaluations      0.797619   \n",
       "\n",
       "                                          Social & Collaborative Skills  \n",
       "model                                                                    \n",
       "0-TEST-Control_evaluations                                     0.759036  \n",
       "1-TEST-BaseEvaluator_evaluations                               0.742424  \n",
       "2-TEST-Medium-Rubric_evaluations                               0.743902  \n",
       "3-TEST-High-Rubric_evaluations                                 0.738095  \n",
       "4-TEST-High-Rubric-Reasoning_evaluations                       0.750000  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute mean strict distance per domain (not plotted for every subâ€‘item)\n",
    "domain_error = []\n",
    "for model in summary_df['model']:\n",
    "    path = EVAL_DIR / f\"{model}.csv\" if not model.endswith('.csv') else EVAL_DIR / model\n",
    "    df_m = pd.read_csv(path)\n",
    "    merged = df_m.merge(human_df, on='School_Clip', suffixes=('_model', '_human'))\n",
    "    errors = {'model': model}\n",
    "    for dom in DOMAIN_COLS:\n",
    "        diffs = []\n",
    "        for _, row in merged.iterrows():\n",
    "            mv = _as_numeric(row[f'{dom}_model'])\n",
    "            hv = _as_numeric(row[f'{dom}_human'])\n",
    "            if pd.isna(mv) or pd.isna(hv):\n",
    "                continue\n",
    "            diffs.append(abs(mv - hv))\n",
    "        errors[dom] = np.mean(diffs) if diffs else np.nan\n",
    "    domain_error.append(errors)\n",
    "\n",
    "domain_df = pd.DataFrame(domain_error).set_index('model').sort_index()\n",
    "domain_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "84245527",
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot two illustrative domains: Classroom Culture (SupportiveÂ +Â PBE) & TimeÂ onÂ Task\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=False)\n",
    "\n",
    "axes[0].bar(domain_df.index, domain_df['Supportive Learning Environment'])\n",
    "axes[0].set_title('Domain error â€“ SupportiveÂ LearningÂ Environment')\n",
    "axes[0].set_ylabel('Mean absÂ difference')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "tot_error = []\n",
    "for model in summary_df['model']:\n",
    "    path = EVAL_DIR / f\"{model}.csv\" if not model.endswith('.csv') else EVAL_DIR / model\n",
    "    df_m = pd.read_csv(path)\n",
    "    merged = df_m.merge(human_df, on='School_Clip', suffixes=('_model', '_human'))\n",
    "    diff_list = []\n",
    "    for _, row in merged.iterrows():\n",
    "        count_mismatch = 0\n",
    "        for t_col, s_col in zip(TOT_TEACHER_COLS, TOT_STUDENTS_COLS):\n",
    "            count_mismatch += int(row[f'{t_col}_model'] != row[f'{t_col}_human'])\n",
    "            count_mismatch += int(row[f'{s_col}_model'] != row[f'{s_col}_human'])\n",
    "        diff_list.append(count_mismatch / 6)  # 6 components in ToT\n",
    "    tot_error.append(np.mean(diff_list))\n",
    "axes[1].bar(summary_df['model'], tot_error)\n",
    "axes[1].set_title('Domain error â€“ TimeÂ onÂ Task')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e53afd7",
   "metadata": {},
   "source": [
    "## 8Â Â Statistical tests\n",
    "\n",
    "We apply:\n",
    "\n",
    "* **Cochranâ€™sÂ Q** â€“ omnibus test on repeatedâ€‘measures (same 40 clips across all models).  \n",
    "* **Pairwise McNemar** (Holmâ€‘corrected) on selected hypotheses.  \n",
    "* **FriedmanÂ & pairedÂ *t*Â / Wilcoxon** on the continuous strictâ€‘distance metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "15106e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cochranâ€™s Q statistic = 24.88  (p = 0.0001469)\n",
      "Pairwise McNemar (Holmâ€‘corrected):\n",
      "  1-TEST-BaseEvaluator_evaluationsÂ vsÂ 2-TEST-Medium-Rubric_evaluations: pÂ =Â 1\n",
      "  2-TEST-Medium-Rubric_evaluationsÂ vsÂ 3-TEST-High-Rubric_evaluations: pÂ =Â 0.1356\n",
      "  3-TEST-High-Rubric_evaluationsÂ vsÂ 4-TEST-High-Rubric-Reasoning_evaluations: pÂ =Â 1\n",
      "  4-TEST-High-Rubric-Reasoning_evaluationsÂ vsÂ 5-TEST-High-Rubric-Timestamped_evaluations: pÂ =Â 1\n",
      "  1-TEST-BaseEvaluator_evaluationsÂ vsÂ 5-TEST-High-Rubric-Timestamped_evaluations: pÂ =Â 0.0701\n",
      "  0-TEST-Control_evaluationsÂ vsÂ 5-TEST-High-Rubric-Timestamped_evaluations: pÂ =Â 0.04778\n",
      "Friedman Ï‡Â² = 94.97  (pÂ =Â 6.06e-19)\n",
      "Paired tâ€‘test (baseline vs best): tÂ =Â 7.96, pÂ =Â 7.645e-12\n"
     ]
    }
   ],
   "source": [
    "# Build boolean matrixÂ (n_clipsÂ Ã—Â n_models) for Cochran\n",
    "models_sorted = sorted(per_segment_matrix.keys())\n",
    "matrix_bool = np.column_stack([per_segment_matrix[m] for m in models_sorted])\n",
    "\n",
    "################################ Cochranâ€™sÂ Q ################################\n",
    "cq_res = cochrans_q(matrix_bool)\n",
    "q_stat = cq_res.statistic\n",
    "q_p    = cq_res.pvalue\n",
    "print(f\"Cochranâ€™s Q statistic = {q_stat:.2f}  (p = {q_p:.4g})\")\n",
    "\n",
    "# --------------------------- Pairwise McNemar ------------------------------\n",
    "def mcnemar_p(col1, col2):\n",
    "    tbl = sm.stats.Table2x2(pd.crosstab(col1, col2)).table\n",
    "    result = mcnemar(tbl, exact=False, correction=True)\n",
    "    return result.pvalue\n",
    "\n",
    "pairs_to_test = [\n",
    "    ('1-TEST-BaseEvaluator_evaluations', '2-TEST-Medium-Rubric_evaluations'),\n",
    "    ('2-TEST-Medium-Rubric_evaluations', '3-TEST-High-Rubric_evaluations'),\n",
    "    ('3-TEST-High-Rubric_evaluations', '4-TEST-High-Rubric-Reasoning_evaluations'),\n",
    "    ('4-TEST-High-Rubric-Reasoning_evaluations', '5-TEST-High-Rubric-Timestamped_evaluations'),\n",
    "    ('1-TEST-BaseEvaluator_evaluations', '5-TEST-High-Rubric-Timestamped_evaluations'),\n",
    "    ('0-TEST-Control_evaluations', '5-TEST-High-Rubric-Timestamped_evaluations'),\n",
    "]\n",
    "\n",
    "p_raw = []\n",
    "for a, b in pairs_to_test:\n",
    "    p = mcnemar_p(per_segment_matrix[a], per_segment_matrix[b])\n",
    "    p_raw.append(p)\n",
    "\n",
    "# Holm correction -----------------------------------------------------------\n",
    "p_order = np.argsort(p_raw)\n",
    "p_holm = np.empty_like(p_raw)\n",
    "m = len(p_raw)\n",
    "for i, idx in enumerate(p_order):\n",
    "    p_holm[idx] = min(1, (m - i) * p_raw[idx])\n",
    "\n",
    "print(\"Pairwise McNemar (Holmâ€‘corrected):\")\n",
    "for (a, b), p_corr in zip(pairs_to_test, p_holm):\n",
    "    print(f\"  {a}Â vsÂ {b}: pÂ =Â {p_corr:.4g}\")\n",
    "\n",
    "################################ Friedman test on distances #################\n",
    "# Build matrix of strict distances (n_clipsÂ Ã—Â n_models)\n",
    "distance_mat = []\n",
    "for model in models_sorted:\n",
    "    path = EVAL_DIR / f\"{model}.csv\" if not model.endswith('.csv') else EVAL_DIR / model\n",
    "    df_m = pd.read_csv(path)\n",
    "    merged = df_m.merge(human_df, on='School_Clip', suffixes=('_model', '_human'))\n",
    "    dists = [strict_distance(row[[c + '_model' for c in ALL_COMPONENTS]].rename(lambda x: x[:-6]),\n",
    "                             row[[c + '_human' for c in ALL_COMPONENTS]].rename(lambda x: x[:-6]))\n",
    "             for _, row in merged.iterrows()]\n",
    "    distance_mat.append(dists)\n",
    "distance_mat = np.column_stack(distance_mat)\n",
    "\n",
    "fried_stat, fried_p = stats.friedmanchisquare(*[distance_mat[:, i] for i in range(distance_mat.shape[1])])\n",
    "print(f\"Friedman Ï‡Â² = {fried_stat:.2f}  (pÂ =Â {fried_p:.4g})\")\n",
    "\n",
    "# Paired tâ€‘test baseline vs best -------------------------------------------\n",
    "base_idx = models_sorted.index('1-TEST-BaseEvaluator_evaluations')\n",
    "best_idx = models_sorted.index('5-TEST-High-Rubric-Timestamped_evaluations')\n",
    "t_stat, t_p = stats.ttest_rel(distance_mat[:, base_idx], distance_mat[:, best_idx])\n",
    "print(f\"Paired tâ€‘test (baseline vs best): tÂ =Â {t_stat:.2f}, pÂ =Â {t_p:.4g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9916e0b",
   "metadata": {},
   "source": [
    "### Interpretation (brief)\n",
    "\n",
    "Across all six evaluator versions, the omnibus Cochranâ€™s Q test indicates a highly significant difference in per-segment pass rates (Q = 24.88, p < 0.001), confirming that at least one model differs from the others in reliability.  Pairwise McNemar comparisons (Holm-corrected) show that the jump from the medium-rubric model (Model 2) to the high-rubric model (Model 3) trends toward improvement (p â‰ˆ 0.14), though it does not reach statistical significance at Î± = 0.05.  Subsequent tweaks for reasoning and timestamped transcripts (Models 4 and 5) do not yield statistically significant pairwise gains over the core high-rubric model, suggesting diminishing returns from those final refinements.\n",
    "\n",
    "On the continuous strict-distance metric, the Friedman test likewise rejects the null of equal performance (Ï‡Â² = 94.97, p < 1e-18), and a paired *t*-test between the baseline (Model 1) and the most advanced system (Model 5) confirms a large, highly significant reduction in average mismatch (t = 7.96, p â‰ˆ 7.6e-12).  \n",
    "\n",
    "Together, these results show that the biggest leap in reliability comes from moving to the high-rubric framework (Model 3), which boosts segment-level pass rate from ~64 % to ~83 % and drives first-try exam pass probability from ~26 % to ~59 %.  Further fine-tuning for reasoning prompts and timestamp alignment does not produce significant statistical gains, indicating that the core rubric upgrade captures most of the attainable improvement.  These findings validate the rubric refinement as the principal driver of performance, with later tweaks offering only marginal, non-significant enhancements.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf5823e",
   "metadata": {},
   "source": [
    "## 9Â Â Save summary for manuscript &Â reâ€‘use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "81c429ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‘Â Saved topline results â†’ model_comparison_summary.csv\n"
     ]
    }
   ],
   "source": [
    "out_path = ROOT_DIR / 'model_comparison_summary.csv'\n",
    "summary_df.to_csv(out_path, index=False)\n",
    "print(f\"ðŸ“‘Â Saved topline results â†’ {out_path.relative_to(ROOT_DIR)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10Â Â Conclusions\n",
    "\n",
    "# TODO: Add conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Â©Â 2025Â TheÂ WorldÂ BankÂ GroupÂ /Â Matt Krasnow â€“ Licensed under the MITÂ License.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Harvard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
