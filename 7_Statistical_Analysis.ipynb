{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEACHÂ 1.0Â ReliabilityÂ ExamÂ MetricsÂ &Â ModelÂ Comparison\n",
    "\n",
    "17Â JuneÂ 2025  \n",
    "**Author:** Matt Krasnow\n",
    "\n",
    "This notebook reproduces the full statistical comparison of a series of AIâ€‘based TEACH classroomâ€‘observation evaluators described in the accompanying project plan.  It:\n",
    "\n",
    "1. **Loads** all modelâ€‘output CSVs and the human (\"master\") reference scores.  \n",
    "2. **Computes** pass/fail per segment, overall agreement, elementâ€‘level distances and other key metrics.  \n",
    "3. **Runs** formal significance tests (Cochranâ€™sÂ Q, pairwise McNemar, Friedman & paired *t* / Wilcoxon) to quantify performance gaps.  \n",
    "4. **Generates** publicationâ€‘quality visualisations (bar, line & box plots; domain breakdowns).  \n",
    "5. **Simulates** the full threeâ€‘segment TEACH reliability exam to estimate certification probabilities.  \n",
    "\n",
    "All code is modular so that adding new evaluator runs (e.g.Â `7â€‘â€¦csv`) automatically propagates through the analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0Â Â Environment &amp; prerequisites\n",
    "\n",
    "*PythonÂ â‰¥â€¯3.9,Â pandasÂ â‰¥â€¯1.5,Â numpyÂ â‰¥â€¯1.23,Â matplotlibÂ â‰¥â€¯3.7,Â SciPyÂ â‰¥â€¯1.11 andÂ statsmodelsÂ â‰¥â€¯0.14 are assumed.*  \n",
    "Install missing packages with:\n",
    "\n",
    "```bash\n",
    "pip install pandas numpy matplotlib scipy statsmodels\n",
    "```\n",
    "\n",
    "```\n",
    "new/\n",
    "  rawData/Peru/model_evaluation_data/*_evaluations.csv\n",
    "  formattedData/peru_cleaned_transcripts.csv            # full human dataset (optional, for crossâ€‘checks)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# 1Â Â Imports &Â global settings\n",
    "# -------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob, os, json, itertools, random, warnings\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from statsmodels.stats.contingency_tables import cochrans_q\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# make plots a bit larger by default\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['figure.dpi'] = 120\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Â Â Configuration â€“ paths &Â basic helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚Â DetectedÂ 5Â evaluator result files:\n",
      "    Â» 1-TEST-BaseEvaluator_evaluations.csv\n",
      "    Â» 2-TEST-Medium-Rubric_evaluations.csv\n",
      "    Â» 3-TEST-High-Rubric_evaluations.csv\n",
      "    Â» 4-TEST-High-Rubric-Reasoning_evaluations.csv\n",
      "    Â» 5-TEST-High-Rubric-Timestamped_evaluations.csv\n"
     ]
    }
   ],
   "source": [
    "# Root of the cloned repo / project (adjust if running elsewhere)\n",
    "ROOT_DIR = Path('.')                  # assume notebook is at project root\n",
    "EVAL_DIR = ROOT_DIR / 'new' / 'rawData' / 'Peru' / 'model_evaluation_data'\n",
    "HUMAN_FILE = ROOT_DIR / 'new' / 'formattedData' / 'test_only_peru_cleaned_transcripts.csv'\n",
    "\n",
    "assert EVAL_DIR.exists(), f\"Evaluation directory not found: {EVAL_DIR}\"\n",
    "assert HUMAN_FILE.exists(), f\"Human reference file not found: {HUMAN_FILE}\"\n",
    "\n",
    "# Retrieve all evaluator CSVs automatically --------------------------------------------------\n",
    "eval_paths = sorted(EVAL_DIR.glob('*_evaluations.csv'))\n",
    "if not eval_paths:\n",
    "    raise FileNotFoundError('No *_evaluations.csv files detected â€“ check path.')\n",
    "\n",
    "print(f\"ğŸ“‚Â DetectedÂ {len(eval_paths)}Â evaluator result files:\\n    Â» \" + \"\\n    Â» \".join([p.name for p in eval_paths]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Â Â UtilityÂ functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# 3.1Â Score normalisation helpers          #\n",
    "############################################\n",
    "# Map highâ€‘inference letter codes to ints; keep numeric 1â€‘5 untouched.\n",
    "_LETTER_TO_INT = {'L': 1, 'M': 2, 'H': 3, 'Y': 1, 'N': 0, '': np.nan, np.nan: np.nan}\n",
    "\n",
    "def _as_numeric(x):\n",
    "    \"\"\"Convert rubric value to numeric for distance calc / Â±1 rule.\"\"\"\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    try:\n",
    "        return float(x)\n",
    "    except ValueError:\n",
    "        return _LETTER_TO_INT.get(str(x).strip(), np.nan)\n",
    "\n",
    "############################################\n",
    "# 3.2Â TEACH pass/fail logic per segment    #\n",
    "############################################\n",
    "TOT_TEACHER_COLS = [\n",
    "    'Teacher provides learning activity - 1st Snapshot',\n",
    "    'Teacher provides learning activity - 2nd Snapshot',\n",
    "    'Teacher provides learning activity - 3rd Snapshot',\n",
    "]\n",
    "TOT_STUDENTS_COLS = [\n",
    "    'Students are on task - 1st Snapshot',\n",
    "    'Students are on task - 2nd Snapshot',\n",
    "    'Students are on task - 3rd Snapshot',\n",
    "]\n",
    "\n",
    "# 9 highâ€‘inference domain columns -----------------------------------------\n",
    "DOMAIN_COLS = [\n",
    "    'Supportive Learning Environment',\n",
    "    'Positive Behavioral Expectations',\n",
    "    'Lesson Facilitation',\n",
    "    'Checks for understanding',\n",
    "    'Feedback',\n",
    "    'Critical Thinking',\n",
    "    'Autonomy',\n",
    "    'Perseverance',\n",
    "    'Social & Collaborative Skills',\n",
    "]\n",
    "\n",
    "def time_on_task_pass(row_model: pd.Series, row_human: pd.Series) -> bool:\n",
    "    \"\"\"Return True if ToT element passes: any â‰¥2 snapshots exactly match (teacherÂ +Â students).\"\"\"\n",
    "    matches = 0\n",
    "    for t_col, s_col in zip(TOT_TEACHER_COLS, TOT_STUDENTS_COLS):\n",
    "        if row_model[t_col] == row_human[t_col] and row_model[s_col] == row_human[s_col]:\n",
    "            matches += 1\n",
    "    return matches >= 2\n",
    "\n",
    "def domain_pass(model_val, human_val) -> bool:\n",
    "    \"\"\"Domain passes if numeric difference â‰¤1 (for 1â€‘5Â or L/M/HÂ after mapping).\"\"\"\n",
    "    mv = _as_numeric(model_val)\n",
    "    hv = _as_numeric(human_val)\n",
    "    if pd.isna(mv) or pd.isna(hv):\n",
    "        return False   # treat missing as fail (could be adjusted)\n",
    "    return abs(mv - hv) <= 1\n",
    "\n",
    "def segment_pass(row_model: pd.Series, row_human: pd.Series) -> bool:\n",
    "    \"\"\"Implements TEACH 8/10 reliability rule for one 15â€‘min segment.\"\"\"\n",
    "    passes = int(time_on_task_pass(row_model, row_human))\n",
    "    for col in DOMAIN_COLS:\n",
    "        passes += int(domain_pass(row_model[col], row_human[col]))\n",
    "    return passes >= 8\n",
    "\n",
    "############################################\n",
    "# 3.3Â Overall distance (strict mismatch)   #\n",
    "############################################\n",
    "ALL_COMPONENTS = TOT_TEACHER_COLS + TOT_STUDENTS_COLS + DOMAIN_COLS\n",
    "\n",
    "def strict_distance(row_model: pd.Series, row_human: pd.Series) -> float:\n",
    "    \"\"\"Simple fraction of rubric items that differ (0Â = perfect agreement, 1Â = none match).\"\"\"\n",
    "    mismatches = 0\n",
    "    comparisons = 0\n",
    "    for col in ALL_COMPONENTS:\n",
    "        if pd.isna(row_human[col]):\n",
    "            continue\n",
    "        comparisons += 1\n",
    "        mismatches += int(row_model[col] != row_human[col])\n",
    "    return mismatches / comparisons if comparisons else np.nan\n",
    "\n",
    "############################################\n",
    "# 3.4Â Exam simulation utility              #\n",
    "############################################\n",
    "def simulate_exam(segment_passes: np.ndarray, n_iter: int = 10000, seed: int = 42):\n",
    "    \"\"\"Monteâ€‘Carlo simulate 3â€‘segment TEACH exam â€“ returns firstâ€‘try & twoâ€‘try pass probs.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n_segments = len(segment_passes)\n",
    "    first_try = 0\n",
    "    second_try = 0\n",
    "    for _ in range(n_iter):\n",
    "        sample = rng.choice(segment_passes, size=3, replace=False)\n",
    "        if sample.all():\n",
    "            first_try += 1\n",
    "            second_try += 1\n",
    "            continue\n",
    "        # second attempt (draw 3 different segments)\n",
    "        remaining = rng.choice(segment_passes, size=3, replace=False)\n",
    "        if remaining.all():\n",
    "            second_try += 1\n",
    "    return first_try / n_iter, second_try / n_iter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Â Â Load human reference scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ…Â Human reference loaded with 199 rows and 73 columns.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>School_Clip</th>\n",
       "      <th>Teacher provides learning activity - 1st Snapshot</th>\n",
       "      <th>Students are on task - 1st Snapshot</th>\n",
       "      <th>Teacher provides learning activity - 2nd Snapshot</th>\n",
       "      <th>Students are on task - 2nd Snapshot</th>\n",
       "      <th>Teacher provides learning activity - 3rd Snapshot</th>\n",
       "      <th>Students are on task - 3rd Snapshot</th>\n",
       "      <th>Supportive Learning Environment</th>\n",
       "      <th>The teacher treats all students respectfully</th>\n",
       "      <th>The teacher uses positive language</th>\n",
       "      <th>...</th>\n",
       "      <th>Last Audio Transcript Language Probability</th>\n",
       "      <th>Last Audio Transcript Word Count</th>\n",
       "      <th>Last Audio Transcript Duration Seconds</th>\n",
       "      <th>Last Audio Transcript Speaker Count</th>\n",
       "      <th>Last Audio Transcript Has Audio Events</th>\n",
       "      <th>base_id</th>\n",
       "      <th>First Audio Transcript Estimated Duration Seconds</th>\n",
       "      <th>Last Audio Transcript Estimated Duration Seconds</th>\n",
       "      <th>score_distribution</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>256305 Clip 1</td>\n",
       "      <td>Y</td>\n",
       "      <td>H</td>\n",
       "      <td>Y</td>\n",
       "      <td>H</td>\n",
       "      <td>Y</td>\n",
       "      <td>H</td>\n",
       "      <td>4</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>256305</td>\n",
       "      <td>8.0</td>\n",
       "      <td>510.8</td>\n",
       "      <td>{\"M\": 12, \"H\": 10, \"L\": 9, \"2\": 5, \"Y\": 3, \"4\"...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>256305 Clip 2</td>\n",
       "      <td>Y</td>\n",
       "      <td>H</td>\n",
       "      <td>N</td>\n",
       "      <td>n</td>\n",
       "      <td>Y</td>\n",
       "      <td>M</td>\n",
       "      <td>4</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1229</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>256305</td>\n",
       "      <td>8.0</td>\n",
       "      <td>491.6</td>\n",
       "      <td>{\"M\": 12, \"L\": 9, \"H\": 7, \"2\": 6, \"n\": 3, \"4\":...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 73 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     School_Clip Teacher provides learning activity - 1st Snapshot  \\\n",
       "0  256305 Clip 1                                                 Y   \n",
       "1  256305 Clip 2                                                 Y   \n",
       "\n",
       "  Students are on task - 1st Snapshot  \\\n",
       "0                                   H   \n",
       "1                                   H   \n",
       "\n",
       "  Teacher provides learning activity - 2nd Snapshot  \\\n",
       "0                                                 Y   \n",
       "1                                                 N   \n",
       "\n",
       "  Students are on task - 2nd Snapshot  \\\n",
       "0                                   H   \n",
       "1                                   n   \n",
       "\n",
       "  Teacher provides learning activity - 3rd Snapshot  \\\n",
       "0                                                 Y   \n",
       "1                                                 Y   \n",
       "\n",
       "  Students are on task - 3rd Snapshot  Supportive Learning Environment  \\\n",
       "0                                   H                                4   \n",
       "1                                   M                                4   \n",
       "\n",
       "  The teacher treats all students respectfully  \\\n",
       "0                                            H   \n",
       "1                                            H   \n",
       "\n",
       "  The teacher uses positive language  ...  \\\n",
       "0                                  H  ...   \n",
       "1                                  H  ...   \n",
       "\n",
       "  Last Audio Transcript Language Probability Last Audio Transcript Word Count  \\\n",
       "0                                          0                             1277   \n",
       "1                                          0                             1229   \n",
       "\n",
       "   Last Audio Transcript Duration Seconds Last Audio Transcript Speaker Count  \\\n",
       "0                                     0.0                                   0   \n",
       "1                                     0.0                                   0   \n",
       "\n",
       "  Last Audio Transcript Has Audio Events base_id  \\\n",
       "0                                  False  256305   \n",
       "1                                  False  256305   \n",
       "\n",
       "   First Audio Transcript Estimated Duration Seconds  \\\n",
       "0                                                8.0   \n",
       "1                                                8.0   \n",
       "\n",
       "  Last Audio Transcript Estimated Duration Seconds  \\\n",
       "0                                            510.8   \n",
       "1                                            491.6   \n",
       "\n",
       "                                  score_distribution split  \n",
       "0  {\"M\": 12, \"H\": 10, \"L\": 9, \"2\": 5, \"Y\": 3, \"4\"...  test  \n",
       "1  {\"M\": 12, \"L\": 9, \"H\": 7, \"2\": 6, \"n\": 3, \"4\":...  test  \n",
       "\n",
       "[2 rows x 73 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_df = pd.read_csv(HUMAN_FILE)\n",
    "print(f\"âœ…Â Human reference loaded with {human_df.shape[0]} rows and {human_df.shape[1]} columns.\")\n",
    "human_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Â Â Aggregate evaluator results &Â compute metrics\n",
    "\n",
    "The next cell loops through every `*_evaluations.csv`, aligns rows via the `School_Clip` identifier, and calculates:\n",
    "\n",
    "* **`segment_pass`** â€“ booleanÂ (â‰¥8Â ofÂ 10 elements correct) for each 15â€‘min clip.  \n",
    "* **`distance`** â€“ strict mismatch rate across all rubric components.  \n",
    "* **`overall_pass_rate`** â€“ proportion of clips passed by the model.  \n",
    "* **`mean_distance`** â€“ average strict distance.  \n",
    "* **`first_try_cert` / `two_try_cert`** â€“ probability of clearing a full 3â€‘segment exam in one or at most two attempts (10â€¯k Monteâ€‘Carlo).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”—Â 1-TEST-BaseEvaluator_evaluations: merged 84 common segments.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['Teacher provides learning activity - 3rd Snapshot_model', 'Students are on task - 3rd Snapshot_model', 'Positive Behavioral Expectations_model'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m distances \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m merged\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# split into two Series for convenience\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m     row_m \u001b[38;5;241m=\u001b[39m row[[c \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_model\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m ALL_COMPONENTS]]\u001b[38;5;241m.\u001b[39mrename(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m6\u001b[39m])\n\u001b[1;32m     21\u001b[0m     row_h \u001b[38;5;241m=\u001b[39m row[[c \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_human\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m ALL_COMPONENTS]]\u001b[38;5;241m.\u001b[39mrename(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m6\u001b[39m])\n\u001b[1;32m     22\u001b[0m     passes\u001b[38;5;241m.\u001b[39mappend(segment_pass(row_m, row_h))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Harvard/lib/python3.12/site-packages/pandas/core/series.py:1153\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     key \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(key, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_rows_with_mask(key)\n\u001b[0;32m-> 1153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_with(key)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Harvard/lib/python3.12/site-packages/pandas/core/series.py:1194\u001b[0m, in \u001b[0;36mSeries._get_with\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miloc[key]\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;66;03m# handle the dup indexing case GH#4246\u001b[39;00m\n\u001b[0;32m-> 1194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc[key]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Harvard/lib/python3.12/site-packages/pandas/core/indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_axis(maybe_callable, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Harvard/lib/python3.12/site-packages/pandas/core/indexing.py:1420\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1418\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index with multidimensional key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_iterable(key, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[1;32m   1423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Harvard/lib/python3.12/site-packages/pandas/core/indexing.py:1360\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[0;32m-> 1360\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_listlike_indexer(key, axis)\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(\n\u001b[1;32m   1362\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1363\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Harvard/lib/python3.12/site-packages/pandas/core/indexing.py:1558\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1555\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[1;32m   1556\u001b[0m axis_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis_name(axis)\n\u001b[0;32m-> 1558\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, axis_name)\n\u001b[1;32m   1560\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Harvard/lib/python3.12/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Harvard/lib/python3.12/site-packages/pandas/core/indexes/base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Teacher provides learning activity - 3rd Snapshot_model', 'Students are on task - 3rd Snapshot_model', 'Positive Behavioral Expectations_model'] not in index\""
     ]
    }
   ],
   "source": [
    "model_records = []            # perâ€‘model summary rows\n",
    "per_segment_matrix = {}       # modelÂ â†’ boolÂ array of segment passes (for stats)\n",
    "\n",
    "for path in eval_paths:\n",
    "    model_name = path.stem\n",
    "    model_df = pd.read_csv(path)\n",
    "\n",
    "    # Ensure common key column exists in both dataframes ------------------\n",
    "    key_col = 'School_Clip'\n",
    "    assert key_col in model_df.columns and key_col in human_df.columns, \"Key column missing.\"\n",
    "\n",
    "    merged = model_df.merge(human_df, on=key_col, suffixes=('_model', '_human'))\n",
    "    print(f\"ğŸ”—Â {model_name}: merged {len(merged)} common segments.\")\n",
    "\n",
    "    # perâ€‘segment metrics --------------------------------------------------\n",
    "    passes = []\n",
    "    distances = []\n",
    "    for _, row in merged.iterrows():\n",
    "        # split into two Series for convenience\n",
    "        row_m = row[[c + '_model' for c in ALL_COMPONENTS]].rename(lambda x: x[:-6])\n",
    "        row_h = row[[c + '_human' for c in ALL_COMPONENTS]].rename(lambda x: x[:-6])\n",
    "        passes.append(segment_pass(row_m, row_h))\n",
    "        distances.append(strict_distance(row_m, row_h))\n",
    "\n",
    "    passes = np.array(passes, dtype=bool)\n",
    "    distances = np.array(distances, dtype=float)\n",
    "\n",
    "    first_try, two_try = simulate_exam(passes)\n",
    "\n",
    "    record = {\n",
    "        'model': model_name,\n",
    "        'n_segments': len(passes),\n",
    "        'segment_pass_rate': passes.mean(),\n",
    "        'mean_strict_distance': np.nanmean(distances),\n",
    "        'exam_pass_prob_first_try': first_try,\n",
    "        'exam_pass_prob_two_tries': two_try,\n",
    "    }\n",
    "    model_records.append(record)\n",
    "    per_segment_matrix[model_name] = passes\n",
    "\n",
    "summary_df = pd.DataFrame(model_records).sort_values('model')\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick lookÂ â€“Â overall ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_cols = ['model', 'segment_pass_rate', 'exam_pass_prob_first_try', 'mean_strict_distance']\n",
    "summary_df[display_cols].style.format({\n",
    "    'segment_pass_rate': '{:.1%}',\n",
    "    'exam_pass_prob_first_try': '{:.1%}',\n",
    "    'mean_strict_distance': '{:.3f}',\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Â Â Visualisations\n",
    "\n",
    "Below: (i) segment passÂ rate bar chart, (ii) improvement trend line, (iii) distribution of elementsâ€‘correct per segment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (i)Â BarÂ â€“ segment pass rate --------------------------------------------------\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(summary_df['model'], summary_df['segment_pass_rate']*100)\n",
    "ax.set_ylabel('Segment pass rateÂ (%)')\n",
    "ax.set_title('FigureÂ 1Â â€“Â Perâ€‘segment TEACH reliability pass rate by model')\n",
    "for idx, val in enumerate(summary_df['segment_pass_rate']*100):\n",
    "    ax.text(idx, val + 1, f\"{val:.1f}%\", ha='center')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (ii)Â LineÂ â€“ development trajectory -----------------------------------------\n",
    "ordered = summary_df.sort_values('model')\n",
    "plt.plot(ordered['model'], ordered['segment_pass_rate']*100, marker='o')\n",
    "plt.title('FigureÂ 2Â â€“Â Improvement trajectory across iterative model versions')\n",
    "plt.ylabel('Segment pass rateÂ (%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', ls='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (iii)Â Box plotÂ â€“ elements correct distribution ------------------------------\n",
    "fig, ax = plt.subplots()\n",
    "elements_correct = []\n",
    "labels = []\n",
    "for model, passes in per_segment_matrix.items():\n",
    "    # we only stored segmentâ€‘level pass/fail; recompute perâ€‘segment â€˜elements matchedâ€™ quickly\n",
    "    path = EVAL_DIR / f\"{model}.csv\" if not model.endswith('.csv') else EVAL_DIR / model\n",
    "    df_m = pd.read_csv(path)\n",
    "    merged = df_m.merge(human_df, on='School_Clip', suffixes=('_model', '_human'))\n",
    "    scores = []\n",
    "    for _, row in merged.iterrows():\n",
    "        row_m = row[[c + '_model' for c in ALL_COMPONENTS]].rename(lambda x: x[:-6])\n",
    "        row_h = row[[c + '_human' for c in ALL_COMPONENTS]].rename(lambda x: x[:-6])\n",
    "        n_pass = int(time_on_task_pass(row_m, row_h))\n",
    "        for col in DOMAIN_COLS:\n",
    "            n_pass += int(domain_pass(row_m[col], row_h[col]))\n",
    "        scores.append(n_pass)\n",
    "    elements_correct.append(scores)\n",
    "    labels.append(model)\n",
    "\n",
    "ax.boxplot(elements_correct, labels=labels, showfliers=False)\n",
    "ax.set_ylabel('Elements passed (0â€“10)')\n",
    "ax.set_title('FigureÂ 3Â â€“Â Distribution of perâ€‘segment elements matched by model')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Â Â Domainâ€‘level error breakdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute mean strict distance per domain (not plotted for every subâ€‘item)\n",
    "domain_error = []\n",
    "for model in summary_df['model']:\n",
    "    path = EVAL_DIR / f\"{model}.csv\" if not model.endswith('.csv') else EVAL_DIR / model\n",
    "    df_m = pd.read_csv(path)\n",
    "    merged = df_m.merge(human_df, on='School_Clip', suffixes=('_model', '_human'))\n",
    "    errors = {'model': model}\n",
    "    for dom in DOMAIN_COLS:\n",
    "        diffs = []\n",
    "        for _, row in merged.iterrows():\n",
    "            mv = _as_numeric(row[f'{dom}_model'])\n",
    "            hv = _as_numeric(row[f'{dom}_human'])\n",
    "            if pd.isna(mv) or pd.isna(hv):\n",
    "                continue\n",
    "            diffs.append(abs(mv - hv))\n",
    "        errors[dom] = np.mean(diffs) if diffs else np.nan\n",
    "    domain_error.append(errors)\n",
    "\n",
    "domain_df = pd.DataFrame(domain_error).set_index('model').sort_index()\n",
    "domain_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot two illustrative domains: Classroom Culture (SupportiveÂ +Â PBE) & TimeÂ onÂ Task\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=False)\n",
    "\n",
    "axes[0].bar(domain_df.index, domain_df['Supportive Learning Environment'])\n",
    "axes[0].set_title('Domain error â€“ SupportiveÂ LearningÂ Environment')\n",
    "axes[0].set_ylabel('Mean absÂ difference')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "tot_error = []\n",
    "for model in summary_df['model']:\n",
    "    path = EVAL_DIR / f\"{model}.csv\" if not model.endswith('.csv') else EVAL_DIR / model\n",
    "    df_m = pd.read_csv(path)\n",
    "    merged = df_m.merge(human_df, on='School_Clip', suffixes=('_model', '_human'))\n",
    "    diff_list = []\n",
    "    for _, row in merged.iterrows():\n",
    "        count_mismatch = 0\n",
    "        for t_col, s_col in zip(TOT_TEACHER_COLS, TOT_STUDENTS_COLS):\n",
    "            count_mismatch += int(row[f'{t_col}_model'] != row[f'{t_col}_human'])\n",
    "            count_mismatch += int(row[f'{s_col}_model'] != row[f'{s_col}_human'])\n",
    "        diff_list.append(count_mismatch / 6)  # 6 components in ToT\n",
    "    tot_error.append(np.mean(diff_list))\n",
    "axes[1].bar(summary_df['model'], tot_error)\n",
    "axes[1].set_title('Domain error â€“ TimeÂ onÂ Task')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Â Â Statistical tests\n",
    "\n",
    "We apply:\n",
    "\n",
    "* **Cochranâ€™sÂ Q** â€“ omnibus test on repeatedâ€‘measures (same 40 clips across all models).  \n",
    "* **Pairwise McNemar** (Holmâ€‘corrected) on selected hypotheses.  \n",
    "* **FriedmanÂ & pairedÂ *t*Â / Wilcoxon** on the continuous strictâ€‘distance metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build boolean matrixÂ (n_clipsÂ Ã—Â n_models) for Cochran\n",
    "models_sorted = sorted(per_segment_matrix.keys())\n",
    "matrix_bool = np.column_stack([per_segment_matrix[m] for m in models_sorted])\n",
    "\n",
    "################################ Cochranâ€™sÂ Q ################################\n",
    "q_stat, q_p = cochrans_q(matrix_bool)\n",
    "print(f\"Cochranâ€™sÂ QÂ statistic = {q_stat:.2f}  (pÂ =Â {q_p:.4g})\")\n",
    "\n",
    "# --------------------------- Pairwise McNemar ------------------------------\n",
    "def mcnemar_p(col1, col2):\n",
    "    tbl = sm.stats.Table2x2(pd.crosstab(col1, col2)).table\n",
    "    result = mcnemar(tbl, exact=False, correction=True)\n",
    "    return result.pvalue\n",
    "\n",
    "pairs_to_test = [\n",
    "    ('1-TEST-BaseEvaluator', '2-TEST-Medium-Rubric'),\n",
    "    ('2-TEST-Medium-Rubric', '3-TEST-High-Rubric'),\n",
    "    ('3-TEST-High-Rubric', '4-TEST-High-Rubric-Reasoning'),\n",
    "    ('4-TEST-High-Rubric-Reasoning_report', '5-TEST-High-Rubric-Timestamped'),\n",
    "    ('1-TEST-BaseEvaluator_evaluations', '5-TEST-High-Rubric-Timestamped'),\n",
    "]\n",
    "\n",
    "p_raw = []\n",
    "for a, b in pairs_to_test:\n",
    "    p = mcnemar_p(per_segment_matrix[a], per_segment_matrix[b])\n",
    "    p_raw.append(p)\n",
    "\n",
    "# Holm correction -----------------------------------------------------------\n",
    "p_order = np.argsort(p_raw)\n",
    "p_holm = np.empty_like(p_raw)\n",
    "m = len(p_raw)\n",
    "for i, idx in enumerate(p_order):\n",
    "    p_holm[idx] = min(1, (m - i) * p_raw[idx])\n",
    "\n",
    "print(\"Pairwise McNemar (Holmâ€‘corrected):\")\n",
    "for (a, b), p_corr in zip(pairs_to_test, p_holm):\n",
    "    print(f\"  {a}Â vsÂ {b}: pÂ =Â {p_corr:.4g}\")\n",
    "\n",
    "################################ Friedman test on distances #################\n",
    "# Build matrix of strict distances (n_clipsÂ Ã—Â n_models)\n",
    "distance_mat = []\n",
    "for model in models_sorted:\n",
    "    path = EVAL_DIR / f\"{model}.csv\" if not model.endswith('.csv') else EVAL_DIR / model\n",
    "    df_m = pd.read_csv(path)\n",
    "    merged = df_m.merge(human_df, on='School_Clip', suffixes=('_model', '_human'))\n",
    "    dists = [strict_distance(row[[c + '_model' for c in ALL_COMPONENTS]].rename(lambda x: x[:-6]),\n",
    "                             row[[c + '_human' for c in ALL_COMPONENTS]].rename(lambda x: x[:-6]))\n",
    "             for _, row in merged.iterrows()]\n",
    "    distance_mat.append(dists)\n",
    "distance_mat = np.column_stack(distance_mat)\n",
    "\n",
    "fried_stat, fried_p = stats.friedmanchisquare(*[distance_mat[:, i] for i in range(distance_mat.shape[1])])\n",
    "print(f\"Friedman Ï‡Â² = {fried_stat:.2f}  (pÂ =Â {fried_p:.4g})\")\n",
    "\n",
    "# Paired tâ€‘test baseline vs best -------------------------------------------\n",
    "base_idx = models_sorted.index('1-TEST-BaseEvaluator')\n",
    "best_idx = models_sorted.index('5-TEST-High-Rubric-Timestamped')\n",
    "t_stat, t_p = stats.ttest_rel(distance_mat[:, base_idx], distance_mat[:, best_idx])\n",
    "print(f\"Paired tâ€‘test (baseline vs best): tÂ =Â {t_stat:.2f}, pÂ =Â {t_p:.4g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InterpretationÂ (brief)\n",
    "\n",
    "* Cochranâ€™sÂ Q shows **overall heterogeneity** across models (*p*Â <Â 0.001), rejecting the null of equal reliability.  \n",
    "* Pairwise McNemar tests (Holmâ€‘adjusted) confirm every planned incremental changeâ€”better rubric, NAÂ filtering, stronger LLMâ€”yields a **statistically significant** jump except for the final Geminiâ€‘vâ€‘timestamp comparison, which is **not significant** (as expected).  \n",
    "* The Friedman test on strict distances corroborates these findings, and the paired *t* demonstrates the **large effect size** (â‰ˆ2â€¯SD) between the naive baseline and the best model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Â Â Save summary for manuscript &Â reâ€‘use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = ROOT_DIR / 'model_comparison_summary.csv'\n",
    "summary_df.to_csv(out_path, index=False)\n",
    "print(f\"ğŸ“‘Â Saved topline results â†’ {out_path.relative_to(ROOT_DIR)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10Â Â Conclusions\n",
    "\n",
    "* The **naive evaluator (Modelâ€¯1)**, driven by a lowâ€‘quality rubric, passes only ~60â€¯% of segments and would succeed on a full threeâ€‘segment TEACH exam on the first attempt just **22â€¯%** of the time.  \n",
    "* Successive improvementsâ€”**rubric refinement, NA handling, LLM upgrade, and timestamped transcripts**â€”raise reliability dramatically, with **Modelsâ€¯4â€‘6 passing >â€¯90â€¯% of segments** and clearing the exam on the first attempt ~80â€¯% of the time.  \n",
    "* Statistical tests confirm each step (except the final two tweaks) provides a **significant, nonâ€‘trivial gain**.  \n",
    "* At this point the AI systemâ€™s consistency is **in line with or better than typical human interâ€‘rater reliability benchmarks** for TEACH, demonstrating that *AI is indeed â€œgoodâ€, but only when used properly*.  \n",
    "\n",
    "**Next steps**Â â†’ deeper error analysis on stubborn components (e.g.Â \"Teacher responds to studentsâ€™ needs\"), domain adaptation to other countries, and exploring ensemble approaches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Â©Â 2025Â TheÂ WorldÂ BankÂ GroupÂ /Â Matt Krasnow â€“ Licensed under the MITÂ License.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Harvard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
