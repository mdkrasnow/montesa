\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{float}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{geometry}

\geometry{margin=1in}

\title{Practical 2025: Aligning AI-Generated Teacher Evaluations with Human Judgments}
\author{CS XXX: Advanced Machine Learning}
\date{Due 11:59 PM on Wednesday, May 31st, 2025}

\begin{document}

\maketitle

This assignment must be completed in groups of 2 to 3 students. You can seek partners via Ed, and course staff will release a team-finding spreadsheet. This document describes the fine-tuning project, including implementation details and submission logistics. In this same directory, you will find a file called \texttt{practical-template.tex}. Use it as a \LaTeX{} template for your writeup, and be sure to read it for grading details.

\textbf{Harvard College Honor Code:} Members of the Harvard College community commit themselves to producing academic work of integrity – that is, work that adheres to the scholarly and intellectual standards of accurate attribution of sources, appropriate collection and use of data, and transparent acknowledgement of the contribution of others to their ideas, discoveries, interpretations, and conclusions. As such, you may not share your code, or have it viewed by any other student outside your group.

\section{Implementation Guidelines}

\textbf{Important:} Throughout this project, you will be working with several external libraries and APIs. Always search for the most current documentation and implementation examples before coding. Libraries like Hugging Face's Transformers, PEFT, TRL, and various LLM APIs (OpenAI, Anthropic, Google) receive frequent updates. The correct implementation methods may change over time, so consulting official documentation is essential.

When implementing any aspect of this project:
\begin{itemize}
    \item Search for up-to-date tutorials, examples, and best practices for each library or API
    \item Review GitHub repositories with similar implementations to understand proper usage
    \item Pay special attention to data structures expected by these libraries
    \item Use JSON formatting for LLM outputs that need to be parsed
    \item Always constrain LLM generation using proper API syntax when expecting structured outputs
    \item Write well-documented, organized code with clear variable names and structure
    \item Include comments explaining complex operations and why certain decisions were made
\end{itemize}

\section{Logistics}

\subsection{Implementation Details}

Raw audio data and a starter framework have been included in the repo. You will be required to submit all of your implementation code to Gradescope. This assignment can be completed on either your own computer or Google Colab, if you are having issues locally.

\subsection{Submission Details}

The main deliverable of this practical is a 3-4 page typewritten document in PDF format to Gradescope. The document must follow the \texttt{practical-template.tex} file in this directory and follow all instructions outlined in Section 3. All relevant text—including your discussions of why you tried various things and why the results came out the way they did—must be included in the maximum of 4 pages. If you need more space than 4 pages for tables, plots, or figures, that is okay.

You should also submit your code as either a \texttt{.py} or \texttt{.ipynb} file on Gradescope. Make sure that the code is neatly organized to reflect which part is being answered. Before submitting, rerun the entire notebook and display all relevant results.

\subsection{Grading}

Our grading focus will be on your ability to clearly and concisely describe what you did, present the results, and most importantly discuss how and why different choices affected performance. Try to achieve at least 0.7 Cohen's Kappa on the validation set at the end, although you can still earn full credit without reaching this mark (provided that your methodology and writeup are sound).

The project will be graded holistically, with each major component evaluated on a check, check-minus, and minus basis. A check is provided for successfully and thoughtfully completing the section. A check-minus is provided for completing parts of the section and providing little interpretation. Lastly, a minus is provided for providing little to no work. Section 5 is an optional extra credit opportunity.

See \texttt{practical-template.tex} for our desired submission format and more tips for what a full-credit submission looks like. All team members will receive the same practical grade.

\section{Problem Background}

For this practical, you will fine-tune a large language model (LLM) to evaluate teacher performance based on classroom audio transcripts. Specifically, you will create a model that can analyze transcript text and produce evaluative feedback aligned with the World Bank's Teach framework.

The project is based on the SwiftScore-World Bank collaboration, which aims to develop a scalable teacher evaluation system that aligns closely with human expert judgments. The value of such a system lies in scalability and accessibility—AI can process far more classroom transcripts than human evaluators, providing timely feedback to inform teacher professional development.

Your model will focus on analyzing classroom audio transcripts (rather than videos) to emphasize accessibility for schools worldwide, recognizing that video recording infrastructure is often costly and impractical in many educational settings. Additionally, the audio-only approach helps preserve privacy by reducing concerns related to classroom recordings and sensitive interactions.

The primary goal is determining AI alignment with human judgment in education. Any AI-generated teacher evaluation must closely match what expert human evaluators would conclude based on the Teach framework. By aligning the model's evaluations with skilled human observers, we ensure the technology augments human-led evaluation processes rather than conflicting with them.

\section{Your Task and Deliverables}

Your task is to implement a complete pipeline for aligning AI-generated teacher evaluations with human judgments. This includes data processing, model fine-tuning, and evaluation. Below, we outline the key components of this project.

\subsection{Evaluation Metrics}

In this practical, you should focus on optimizing for inter-rater reliability between the AI model and human evaluators:

\begin{itemize}
    \item \textbf{Cohen's Kappa:} Measures agreement between AI and human evaluators across Teach framework dimensions, accounting for agreement by chance.
    \item \textbf{Percentage of Exact Agreement:} The percentage of cases where AI and human scores match exactly.
    \item \textbf{Mean Absolute Error (MAE):} Average absolute difference between AI and human scores.
    \item \textbf{Intraclass Correlation Coefficient (ICC):} For measuring consistency between human and AI ratings (if applicable to your implementation).
\end{itemize}

For each model you train, calculate these metrics on both the train and validation datasets and include these results in your write-up.

\textbf{Required Visualizations:} Your evaluation section should include the following visualizations:
\begin{itemize}
    \item Confusion matrices comparing human vs. AI scores for each Teach framework dimension
    \item Bar charts showing Cohen's Kappa values across different model variants (real data, synthetic data, mixed)
    \item Line graphs tracking evaluation metrics during training (to identify potential overfitting)
    \item Violin plots comparing score distributions between human evaluators and AI model outputs
    \item Heatmaps highlighting areas of highest disagreement between human and AI evaluations
\end{itemize}

\subsection{Data Processing and Exploration}

The first step in your project is to process the raw audio data and create structured training datasets:

\begin{enumerate}
    \item \textbf{Data Transcription:} You need to transcribe the raw audio recordings to text. Use a speech-to-text model like Open Whisper v3 (via Groq or another inference method). Ensure you include timestamps in the transcripts.
    
    \item \textbf{Exploratory Data Analysis (EDA):} 
    \begin{itemize}
        \item Understand how you will adapt the evaluation framework for creating the evaluation data
        \item Analyze the distribution of scores across different Teach framework dimensions
        \item Identify patterns in transcript characteristics that correlate with evaluation scores
    \end{itemize}
    
    \item \textbf{Data Cleaning and Preparation:}
    \begin{itemize}
        \item Create appropriate train-validation-test splits, based on the number of evaluations
        \item Split the data from whole evaluations into individual Teach components (this is likely an $\mathbb{R}^1 \to \mathbb{R}^4$ mapping)
        \item Format the data into proper structure with columns such as:
        \begin{itemize}
            \item Audio Transcript (with timestamps)
            \item Domain
            \item Score
            \item Summary
        \end{itemize}
        \item Ensure all data is well-organized and easily accessible for later stages
        \item Create clearly separated datasets for each training scenario (real, synthetic, mixed)
    \end{itemize}
    
    \item \textbf{Dataset Format:} Convert your processed data into a format suitable for fine-tuning. All data must be formatted in JSONL files or a Hugging Face dataset with the following structure:
    \begin{itemize}
        \item Input: Classroom transcript text
        \item Output: Formatted evaluation with scores and feedback in JSON format
    \end{itemize}
\end{enumerate}

\textbf{Important:} All model outputs (both in training data and evaluation) must be structured in JSON format as follows:

\begin{verbatim}
{
  "score": int,  // Integer score as per Teach framework
  "summary": "string"  // Text summary/feedback
}
\end{verbatim}

This standardized JSON format ensures consistency in data processing and evaluation. When creating your training dataset, ensure that human-labeled evaluations are also converted to this format. This approach facilitates easier parsing during evaluation and matches best practices for structured LLM outputs in production environments.

\subsection{Synthetic Data Generation}

Following the data processing, you should implement synthetic data generation to augment your training set:

\begin{enumerate}
    \item \textbf{Teacher Model Knowledge:} Before generating synthetic data, you must provide context to the teacher model about the Teach framework:
    \begin{itemize}
        \item Feed the model relevant text/PDF documents that explain the Teach framework
        \item Include detailed information about how to properly score classroom observations
        \item Provide examples of high-quality evaluations following the framework
        \item This context will help the model generate synthetic data that properly follows the scoring criteria, effectively acting like a human evaluator
    \end{itemize}
    
    \item \textbf{Generation Approach:} Use a high-capability LLM (e.g., Gemini 2.0 Flash) as your "teacher" model to generate synthetic transcript-evaluation pairs in JSON format. Create a variety of classroom transcript scenarios to ensure coverage of different teaching situations.
    
    \item \textbf{Quality Control:} Implement checks to ensure the synthetic evaluations are realistic and aligned with expected human outputs:
    \begin{itemize}
        \item Verify that all generated outputs follow the required JSON format
        \item Check that scores fall within valid ranges for each dimension
        \item Ensure summaries provide appropriate, constructive feedback
        \item Manually review a sample to validate overall quality
    \end{itemize}
    
    \item \textbf{Dataset Creation:} Your implementation must create and maintain three distinct datasets:
    \begin{itemize}
        \item \textbf{Real Dataset:} Contains only human-labeled evaluations
        \item \textbf{Synthetic Dataset:} Contains only synthetic evaluations (start with 100 examples)
        \item \textbf{Mixed Dataset:} Contains a balanced combination of real and synthetic data
    \end{itemize}
    
    \item \textbf{Flexible Implementation:} Design your code to be flexible and efficient:
    \begin{itemize}
        \item Store generated synthetic data so it can be reused
        \item Allow easy scaling of synthetic data generation (e.g., from 100 to 500 examples)
        \item Include parameters to control the ratio of real to synthetic data in the mixed dataset
        \item Implement caching to avoid regenerating data unnecessarily
    \end{itemize}
\end{enumerate}

\textbf{Note:} Start with a small synthetic dataset (around 100 examples) to verify your pipeline works correctly. Once you've successfully trained an initial model, you can scale up the synthetic data generation as needed to improve performance.

\subsection{Model Fine-Tuning}

For the main assignment, you will fine-tune Google's Gemma 3 27B model using QLoRA:

\begin{enumerate}
    \item \textbf{Model Setup:}
    \begin{itemize}
        \item Load the Gemma 3 27B model with appropriate quantization configuration
        \item Implement QLoRA (Quantized Low-Rank Adaptation) to efficiently fine-tune the model
        \item Configure the training pipeline using the TRL library's SFTTrainer
    \end{itemize}
    
    \item \textbf{Training Configuration:}
    \begin{itemize}
        \item Implement supervised fine-tuning with an appropriate learning rate, batch size, and training duration
        \item Use gradient checkpointing, mixed precision, and other efficiency techniques
        \item Configure QLoRA settings (rank, alpha, target modules)
    \end{itemize}
    
    \item \textbf{Experimentation:}
    \begin{itemize}
        \item Train multiple model variants:
        \begin{itemize}
            \item Model variant trained on real data only
            \item Model variant trained on synthetic data only
            \item Model variant trained on mixed real+synthetic data
        \end{itemize}
    \end{itemize}
\end{enumerate}

\subsection{Model Evaluation}

After training your models, you should implement comprehensive evaluation:

\begin{enumerate}
    \item \textbf{Inter-rater Reliability:} Implement the reliability metrics described in section 3.1.
    
    \item \textbf{LLM-as-Judge Evaluation:} Implement an LLM-as-a-Judge framework for preference evaluation, substituting for human preferences. This involves:
    
    \begin{itemize}
        \item \textbf{Judge Selection:} Use a strong LLM (e.g., GPT-4, Claude, or Gemini) as your judge model. This model should be different from the one you're evaluating to avoid preference leakage and bias.
        
        \item \textbf{Prompt Engineering:} Create well-structured prompts that:
        \begin{itemize}
            \item Clearly define evaluation criteria based on the Teach framework
            \item Include Chain-of-Thought (CoT) reasoning to improve judgment quality
            \item Break down evaluation into specific aspects (e.g., completeness, relevance, accuracy)
            \item Provide few-shot examples with reasoning for different quality levels
        \end{itemize}
        
        \item \textbf{Evaluation Formats:} Implement at least two of these evaluation types:
        \begin{itemize}
            \item \textbf{Pairwise Comparison:} Have the judge choose between two model outputs (e.g., your model vs. reference human evaluation)
            \item \textbf{Direct Scoring:} Have the judge assign scores (1-5) to model outputs based on specified criteria
            \item \textbf{Batch Ranking:} Have the judge rank multiple outputs for the same transcript
        \end{itemize}
        
        \item \textbf{Validation:} Validate your LLM judge by calculating its agreement with human evaluations on a subset of data where both are available.
    \end{itemize}
    
    \textbf{Implementation Resources:} You should research and utilize existing implementations of LLM-as-a-Judge. Useful resources include:
    \begin{itemize}
        \item Hugging Face's evaluation libraries (especially their LLM judge templates)
        \item GitHub repositories like JudgeLM and DeepEval
        \item Academic papers such as "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena"
    \end{itemize}
    
    \item \textbf{Comparative Analysis:} Compare your fine-tuned model(s) with:
    \begin{itemize}
        \item Base model (Gemma 3 27B without fine-tuning)
        \item GPT-4o (through the OpenAI API)
        \item Gemini 2.0 Flash
        \item Any other models that you train
    \end{itemize}
    
    \item \textbf{Error Analysis:} Perform detailed error analysis focusing on:
    \begin{itemize}
        \item Discrepancies between human and AI scores/summaries
        \item Patterns in cases where AI-human disagreement is highest
        \item Categorizing the types of classroom interactions or teaching behaviors where alignment is weakest
        \item Feature extraction to identify what aspects of the transcripts lead to different evaluations
    \end{itemize}
    
    \item \textbf{Key Comparisons:} Analyze and visualize these important comparisons:
    \begin{itemize}
        \item Whether the fine-tuned model (distilled from the teacher model) outperforms the teacher model on test data
        \item Performance differences between models trained on real data only vs. synthetic data vs. mixed data
        \item How different fine-tuning methods affect evaluation quality
    \end{itemize}
\end{enumerate}

\section{Final Write-up and Reflections}

To wrap up your exploratory research into applying parameter-efficient fine-tuning to teacher evaluation, document and summarize the steps you took in the ML research pipeline, and reflect on the choices that you made.

For each of the components below, include a paragraph (2-4 sentence) explanation and reflection of what you did for each step. Think through any trade-offs, domain-specific input, real-world considerations, and ethical decisions that you made along the way, in addition to any considerations made from a purely machine learning perspective.

\textbf{Key Components:}

\begin{enumerate}
    \item \textbf{Data Processing}: How did you transcribe and format the audio-evaluation pairs? What challenges did you encounter in preparing the data for fine-tuning?

    \item \textbf{Synthetic Data Generation}: How did you approach synthetic data generation? What quality control measures did you implement?

    \item \textbf{Model Selection}: What considerations went into using Gemma 3 27B? What trade-offs did you make between model size, capability, and computational requirements?

    \item \textbf{Fine-Tuning Method}: How did your QLoRA implementation perform? What were the key advantages and disadvantages of using parameter-efficient fine-tuning in this context?

    \item \textbf{Hyperparameter Optimization}: What approach did you take to tuning hyperparameters? Which parameters had the most significant impact on performance?

    \item \textbf{Evaluation}: How well did your AI evaluations align with human judgments across different dimensions of the Teach framework? Were there particular aspects of teaching that the model struggled to evaluate accurately?

    \item \textbf{Domain-specific Insights}: Are there any insights about classroom evaluation that you gained through this project? How might educational experts view the use of AI for teacher evaluation?

    \item \textbf{Deployment Considerations}: What would be required to deploy this model in real educational settings? What privacy, ethical, and practical considerations would need to be addressed?
\end{enumerate}

\section{Optional: Extra Credit and Extensions}

\subsection{Competition: Extra Credit}

We will be awarding 5 percentage points of extra credit on this assignment to the 5 teams with the best models. This will be measured by performance on a test set of classroom transcripts, which can be found in the file \texttt{test.csv}. Note that you only have the raw audio for these clips—the evaluation scores have not been provided. 

For this extra credit opportunity, you should:

\begin{enumerate}
    \item Train an alternative LLM model (e.g., Llama 3.3, Llama 4, or another state-of-the-art model)
    \item Use the validation set as additional training data (this is only allowed for the extra credit component)
    \item Make predictions on the test set and submit those predictions to our Kaggle competition (click here for the link)
\end{enumerate}

An example of the submission format can be found in the file \texttt{sample\_submission.csv}.

Please separate your code for this component of the practical from your code for the main assignment, and ensure that we can replicate your predictions through running your code.

There are a few things you should keep in mind, should you choose to enter the competition. First of all, please do not cheat, e.g., by using external models or datasets. We will be verifying the top 5 teams by running their code files and confirming that the trained models do generate the submitted predictions. Second, you will get to see your model's performance on a subset of the test data through the public leaderboard. However, your ultimate ranking will be determined by your model's performance on the remainder of the test data, and this will be reflected in the private leaderboard, which will be released at 12:00am ET on June 3rd. As such, be careful not to "overfit" to the public leaderboard.

\subsection{Direct Preference Optimization (Optional)}

For students interested in exploring more advanced fine-tuning approaches, consider implementing Direct Preference Optimization (DPO):

\begin{enumerate}
    \item \textbf{Dataset Creation for DPO:} DPO requires a preference dataset consisting of (prompt, chosen, rejected) triplets:
    \begin{itemize}
        \item \textbf{Prompt:} The classroom transcript
        \item \textbf{Chosen (Winner):} The human evaluation or high-quality synthetic evaluation
        \item \textbf{Rejected (Loser):} A flawed evaluation that is clearly inappropriate
    \end{itemize}
    
    \item \textbf{Creating Flawed Evaluations:} Generate deliberately imperfect evaluations that:
    \begin{itemize}
        \item Miss important aspects of the teaching performance
        \item Provide overly vague or generic feedback
        \item Give scores inconsistent with the transcript content
        \item Contain factual errors about what happened in the classroom
        \item Use inappropriate or unhelpful language
    \end{itemize}
    
    \item \textbf{Preference Dataset Structure:} Each item in your preference dataset should contain:
    \begin{itemize}
        \item The original transcript (input prompt)
        \item The high-quality evaluation (winner)
        \item The flawed evaluation (loser)
    \end{itemize}
    
    \item \textbf{Implementation:} Use Hugging Face's TRL library which provides a \texttt{DPOTrainer}:
    \begin{itemize}
        \item Start with a model that has already undergone supervised fine-tuning (SFT)
        \item Configure DPO parameters like beta (typically 0.1-0.5) to control regularization
        \item Balance the training to avoid overfitting to the preference dataset
        \item Consider implementing conservative DPO with label smoothing for better stability
    \end{itemize}
    
    \item \textbf{Evaluation:} Compare DPO-tuned models against your SFT models to assess whether:
    \begin{itemize}
        \item DPO improves alignment with human evaluations
        \item The model better avoids common evaluation pitfalls
        \item The feedback quality improves in subjective assessments
    \end{itemize}
\end{enumerate}

\subsection{Contrastive Fine-Tuning (Optional)}

Another advanced approach is Contrastive Fine-Tuning, which explicitly teaches the model what not to do:

\begin{enumerate}
    \item \textbf{Approach:} Create contrasting pairs of good vs. bad evaluations for the same transcripts:
    \begin{itemize}
        \item Generate multiple types of "bad" evaluations with different flaws
        \item Pair each with the corresponding "good" evaluation
        \item Fine-tune the model to increase probability of good outputs and decrease probability of bad ones
    \end{itemize}
    
    \item \textbf{Implementation Methods:}
    \begin{itemize}
        \item Use a pairwise contrastive loss similar to DPO implementation
        \item Create a "negative persona" model to generate flawed evaluations
        \item Train with pairs showing both good and bad examples for the same prompt
    \end{itemize}
    
    \item \textbf{Analysis:} Study how contrastive tuning affects:
    \begin{itemize}
        \item The model's ability to avoid common pitfalls in evaluations
        \item Overall alignment with human judgments
        \item The specificity and helpfulness of feedback
    \end{itemize}
\end{enumerate}

\section{Other FAQs}

\textbf{What language should I code in?} As you will be submitting your code, you should code in Python.

\textbf{Can I use \{transformers | trl | peft | other ML library\}?} You can use these tools, but not blindly. You are expected to show a deep understanding of the methods we study in the course, and your writeup will be where you demonstrate this.

\textbf{What do I submit?} You will submit both your write-up on Gradescope and all of your practical code to a supplemental assignment on Gradescope.

\textbf{Can I have an extension?} Yes, your writeup can be turned in late according to the standard homework late day policy. You can use at most two late days on the practical.

\textbf{Where can I find resources on fine-tuning?} Start by consulting Hugging Face's documentation for PEFT, TRL, and BitsAndBytes libraries. Look for existing examples of QLoRA implementation, especially those that involve training on structured outputs like evaluations or ratings.

\textbf{How should I handle data for fine-tuning?} For best results with LLM fine-tuning, your data should be formatted as JSON or JSONL files. Each example should contain both the input (transcript) and the expected output (evaluation in JSON format). This standardized approach ensures that your model learns to produce consistently structured outputs that are easy to parse and evaluate.

\end{document}