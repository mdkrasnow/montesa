{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c64c5fa7",
      "metadata": {},
      "source": [
        "# Validation Evaluation Pipeline Notebook\n",
        "\n",
        "This notebook implements the plan for building a validation and evaluation pipeline for comparing AI-based evaluators against human evaluations. It follows the detailed plan to load data, define distance metrics, compute inter-rater reliability, incorporate multiple AI evaluators, and produce comprehensive visualizations. \n",
        "\n",
        "**Notebook Structure**:\n",
        "1. Imports and Setup\n",
        "2. Load Framework and Human Evaluations\n",
        "3. Helper Functions: Mapping and Distance Computation\n",
        "4. Model Calling Helper\n",
        "5. Compute Inter-Rater Reliability (Cohen's Kappa)\n",
        "6. Widgets for Model and Dataset Selection\n",
        "7. Evaluation Execution Function\n",
        "8. Visualizations\n",
        "9. Manual Controls for Test Set Evaluation\n",
        "10. Save Intermediate Results\n",
        "\n",
        "*Note: This notebook assumes that the evaluation framework JSON (`framework.json`) and the cleaned transcripts CSV (`new/formattedData/peru_cleaned_transcripts.csv`) are available in the working directory. It also assumes that model evaluator classes (e.g., `SimpleModelEvaluator`, `MultiModalModelEvaluator`, `AdvancedModelEvaluatorX`) are importable.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56b94454",
      "metadata": {},
      "source": [
        "## 1. Imports and Setup\n",
        "\n",
        "Import all necessary libraries and set up global configuration. We will use `pandas`, `numpy`, `json`, `scikit-learn` for metrics, `matplotlib` for plotting, and `ipywidgets` for interactive controls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "15cc0cd4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully.\n"
          ]
        }
      ],
      "source": [
        "# Standard data manipulation libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Metrics and statistical functions\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Interactive widgets\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# Progress bars\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Concurrency for model calls\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# Typing\n",
        "from typing import Dict, Any, Optional, List, Tuple\n",
        "\n",
        "# Configure matplotlib for inline display\n",
        "%matplotlib inline\n",
        "\n",
        "# Set random seed for reproducibility where needed\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34fc5d68",
      "metadata": {},
      "source": [
        "## 2. Load Framework and Human Evaluations\n",
        "\n",
        "Define a function to load the cleaned transcripts CSV and extract human evaluation columns. The actual evaluation columns are already properly named in the CSV file, so we'll identify and extract them directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b36bb600",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_human_evaluations(cleaned_csv_path: str, framework_json_path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load cleaned transcripts and human evaluation columns.\n",
        "    Returns a DataFrame with columns ['base_id','clip_number',<each human component name>].\n",
        "    \"\"\"\n",
        "    # Read the cleaned transcripts CSV\n",
        "    df = pd.read_csv(cleaned_csv_path, dtype=str)\n",
        "    \n",
        "    # The actual evaluation columns from the CSV\n",
        "    evaluation_columns = [\n",
        "        'Teacher provides learning activity - 1st Snapshot',\n",
        "        'Students are on task - 1st Snapshot',\n",
        "        'Teacher provides learning activity - 2nd Snapshot',\n",
        "        'Students are on task - 2nd Snapshot',\n",
        "        'Teacher provides learning activity - 3rd Snapshot',\n",
        "        'Students are on task - 3rd Snapshot',\n",
        "        'Supportive Learning Environment',\n",
        "        'The teacher treats all students respectfully',\n",
        "        'The teacher uses positive language',\n",
        "        'The teacher responds to students needs',\n",
        "        'The teacher does not exhibit gender bias',\n",
        "        'Positive Behavioral Expectations',\n",
        "        'The teacher sets clear behavioral expectations',\n",
        "        'The teacher acknowledges positive student behavior',\n",
        "        'The teacher redirects misbehavior',\n",
        "        'Lesson Facilitation',\n",
        "        'The teacher explicitly articulates learning objectives',\n",
        "        'The teacher\\'s explanation of content is clear',\n",
        "        'The teacher makes connections in the lesson',\n",
        "        'The teacher models by enacting or thinking aloud',\n",
        "        'Checks for understanding',\n",
        "        'The teacher uses questions',\n",
        "        'The teacher uses prompts',\n",
        "        'The teacher monitors most students',\n",
        "        'The teacher adjusts teaching to the level of students',\n",
        "        'Feedback',\n",
        "        'The teacher provides specific comments for misunderstandings',\n",
        "        'The teacher provides specific comments for successes',\n",
        "        'Critical Thinking',\n",
        "        'The teacher asks open-ended questions',\n",
        "        'The teacher provides thinking tasks',\n",
        "        'Students ask open-ended questions or perform thinking tasks',\n",
        "        'Autonomy',\n",
        "        'The teacher provides students with choices',\n",
        "        'The teacher provides students with opportunities to take meaningful roles',\n",
        "        'Students volunteer to participate in the classroom',\n",
        "        'Perseverance',\n",
        "        'The teacher acknowledges students\\' efforts',\n",
        "        'The teacher has a positive attitude towards students\\' challenges',\n",
        "        'The teacher encourages goal-setting',\n",
        "        'Social & Collaborative Skills',\n",
        "        'The teacher promotes students\\' collaboration',\n",
        "        'The teacher promotes students\\' interpersonal skills'\n",
        "    ]\n",
        "\n",
        "    # Extract base_id and clip_number from School_Clip\n",
        "    clip_info = df['School_Clip'].str.extract(r'(?P<base_id>\\d{6,7})\\s*Clip\\s*(?P<clip_num>[12])')\n",
        "    df['base_id'] = clip_info['base_id']\n",
        "    df['clip_number'] = clip_info['clip_num'].map({'1': 'first', '2': 'last'})\n",
        "\n",
        "    # Identify which evaluation columns actually exist\n",
        "    present = [col for col in evaluation_columns if col in df.columns]\n",
        "    missing = [col for col in evaluation_columns if col not in df.columns]\n",
        "    if missing:\n",
        "        print(f\"⚠️ Missing human evaluation columns: {missing}\")\n",
        "    \n",
        "    print(f\"✅ Found {len(present)} evaluation columns in the dataset\")\n",
        "\n",
        "    # Return only base_id, clip_number, and the human-scored columns\n",
        "    return df[['base_id', 'clip_number'] + present]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ea4c292",
      "metadata": {},
      "source": [
        "## 3. Helper Functions: Mapping and Distance Computation\n",
        "\n",
        "We need functions to:\n",
        "1. Convert various score types (Y/N, L/M/H, 1-5, N/A) into numeric representations.\n",
        "2. Compute per-component distance (normalized between 0 and 1) based on the TEACH 1 framework rules:\n",
        "   - If both human and AI scores are `N/A`, distance = 0.\n",
        "   - If exactly one is `N/A`, distance = 1.0.\n",
        "   - Otherwise, distance = |numeric_human − numeric_AI| / d_max, where d_max depends on the type:\n",
        "     - d_max = 1 for Y/N\n",
        "     - d_max = 2 for L/M/H\n",
        "     - d_max = 4 for 1–5\n",
        "3. Aggregate component-level distances to domain-level and overall distances using weights specified in `framework.json`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7b550a21",
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def alpha_to_numeric(x: Any) -> Optional[float]:\n",
        "    \"\"\"\n",
        "    Convert string labels to numeric:\n",
        "    - 'Y'/'y'/'Yes'/'1' -> 1.0\n",
        "    - 'N'/'n'/'No'/'0'  -> 0.0\n",
        "    - 'N/A' or empty  -> np.nan\n",
        "    - 'L' -> 1.0, 'M' -> 2.0, 'H' -> 3.0\n",
        "    - Numeric strings convertible to float -> float(x)\n",
        "    - Otherwise -> np.nan\n",
        "    \"\"\"\n",
        "    if x is None:\n",
        "        return np.nan\n",
        "    x_str = str(x).strip()\n",
        "    if x_str in {\"Y\", \"y\", \"Yes\", \"1\"}:\n",
        "        return 1.0\n",
        "    if x_str in {\"N\", \"n\", \"No\", \"0\"}:\n",
        "        return 0.0\n",
        "    if x_str in {\"N/A\", \"\", \"NA\", \"na\", \"nan\"}:\n",
        "        return np.nan\n",
        "    if x_str in {\"L\", \"M\", \"H\"}:\n",
        "        return {\"L\": 1.0, \"M\": 2.0, \"H\": 3.0}[x_str]\n",
        "    # Try converting directly to float for numeric (1-5 scale)\n",
        "    try:\n",
        "        return float(x_str)\n",
        "    except ValueError:\n",
        "        return np.nan\n",
        "\n",
        "def component_distance(human_score: Any, ai_score: Any, score_type: str) -> float:\n",
        "    \"\"\"\n",
        "    Compute normalized distance between human_score and ai_score based on score_type.\n",
        "    score_type is one of: 'YN', 'LMH', 'NUM' (for 1-5), where 'N/A' handling is builtin.\n",
        "    Returns a float in [0,1].\n",
        "    \"\"\"\n",
        "    # Convert both to numeric (np.nan if N/A)\n",
        "    human_num = alpha_to_numeric(human_score)\n",
        "    ai_num = alpha_to_numeric(ai_score)\n",
        "    \n",
        "    # Both N/A => perfect agreement (distance = 0)\n",
        "    if math.isnan(human_num) and math.isnan(ai_num):\n",
        "        return 0.0\n",
        "    # Exactly one is N/A => maximum penalty\n",
        "    if math.isnan(human_num) ^ math.isnan(ai_num):\n",
        "        return 1.0\n",
        "    # Neither is N/A => compute absolute difference / d_max\n",
        "    if score_type == 'YN':\n",
        "        d_max = 1.0\n",
        "    elif score_type == 'LMH':\n",
        "        d_max = 2.0  # L->1, H->3, max diff = 2\n",
        "    elif score_type == 'NUM':\n",
        "        d_max = 4.0  # 1-5 scale, max diff = 4\n",
        "    else:\n",
        "        d_max = 1.0\n",
        "    \n",
        "    raw_diff = abs(human_num - ai_num)\n",
        "    normalized_diff = raw_diff / d_max\n",
        "    return min(max(normalized_diff, 0.0), 1.0)\n",
        "\n",
        "def compute_distances_for_item(human_row: pd.Series,\n",
        "                               ai_row: pd.Series,\n",
        "                               framework: Dict[str, Any]) -> Tuple[Dict[str, float], float]:\n",
        "    \"\"\"\n",
        "    For a single item (identified by base_id), compute per-domain normalized distances and overall distance.\n",
        "    Returns domain_distances dict and overall_distance float.\n",
        "    \"\"\"\n",
        "    domain_distances: Dict[str, float] = {}\n",
        "    domain_weights_sum = 0.0\n",
        "    overall_numerator = 0.0\n",
        "    \n",
        "    for domain in framework['structure']['domains']:\n",
        "        domain_id = str(domain['id'])\n",
        "        domain_weight = float(domain.get('weight', 1.0))\n",
        "        domain_numerator = 0.0\n",
        "        domain_weight_sum = 0.0\n",
        "        \n",
        "        for comp in domain['components']:\n",
        "            comp_id = str(comp['id'])\n",
        "            comp_name = comp['name']\n",
        "            comp_weight = float(comp.get('weight', 1.0))\n",
        "            score_list = comp.get('scoreList', [])\n",
        "            score_type = 'NUM'\n",
        "            if set(score_list) <= {\"Y\",\"N\",\"N/A\"}:\n",
        "                score_type = 'YN'\n",
        "            elif set(score_list) <= {\"L\",\"M\",\"H\",\"N/A\"}:\n",
        "                score_type = 'LMH'\n",
        "            human_score = human_row.get(comp_name, None)\n",
        "            ai_score = ai_row.get(comp_name, None)\n",
        "            d_c = component_distance(human_score, ai_score, score_type)\n",
        "            domain_numerator += comp_weight * d_c\n",
        "            domain_weight_sum += comp_weight\n",
        "        \n",
        "        D_domain = (domain_numerator / domain_weight_sum) if domain_weight_sum > 0 else 0.0\n",
        "        domain_distances[domain_id] = D_domain\n",
        "        overall_numerator += domain_weight * D_domain\n",
        "        domain_weights_sum += domain_weight\n",
        "    \n",
        "    overall_distance = (overall_numerator / domain_weights_sum) if domain_weights_sum > 0 else 0.0\n",
        "    return domain_distances, overall_distance"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "652d112f",
      "metadata": {},
      "source": [
        "## 4. Model Calling Helper\n",
        "\n",
        "Define a helper function `call_model` to:\n",
        "1. Instantiate an evaluator class with the `framework`.\n",
        "2. Call its `.evaluate()` method, passing `transcript_text` and optionally `audio_file_path`.\n",
        "3. Ensure a standardized return: the `evaluation` dictionary containing domains, component scores, summary, etc.\n",
        "\n",
        "This helper encapsulates differences between simple text-based evaluators and the `MultiModalModelEvaluator` which requires an audio file path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f5097c93",
      "metadata": {},
      "outputs": [],
      "source": [
        "import inspect\n",
        "\n",
        "def call_model(evaluator_cls: type,\n",
        "               framework: Dict[str, Any],\n",
        "               transcript_text: str,\n",
        "               audio_file_path: Optional[str] = None\n",
        "              ) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Instantiate the evaluator_cls with the framework, call .evaluate(),\n",
        "    normalize and return the evaluation dict. Supports evaluators that\n",
        "    return either a wrapper dict with 'success' and 'evaluation' keys\n",
        "    or a direct evaluation dict with 'domains', etc.\n",
        "    \"\"\"\n",
        "    evaluator = evaluator_cls(framework)\n",
        "    try:\n",
        "        sig = inspect.signature(evaluator.evaluate)\n",
        "        # Prefer named param 'audio_file_path'\n",
        "        if 'audio_file_path' in sig.parameters and audio_file_path is not None:\n",
        "            result = evaluator.evaluate(transcript_text, audio_file_path=audio_file_path)\n",
        "        # Fallback to param named 'audio_file'\n",
        "        elif 'audio_file' in sig.parameters and audio_file_path is not None:\n",
        "            result = evaluator.evaluate(transcript_text, audio_file=audio_file_path)\n",
        "        else:\n",
        "            result = evaluator.evaluate(transcript_text)\n",
        "    except Exception as e:\n",
        "        return {'domains': {}, 'summary': '', 'error': str(e)}\n",
        "\n",
        "    # Normalize result to the evaluation dict\n",
        "    if isinstance(result, dict):\n",
        "        # If wrapper dict with success flag\n",
        "        if 'success' in result:\n",
        "            if not result['success']:\n",
        "                return {'domains': {}, 'summary': '', 'error': result.get('error', 'Unknown error')}\n",
        "            eval_result = result.get('evaluation', {})\n",
        "        else:\n",
        "            # Direct evaluation dict\n",
        "            eval_result = result\n",
        "    else:\n",
        "        # Unexpected result type\n",
        "        return {'domains': {}, 'summary': '', 'error': 'Unexpected evaluator output type'}\n",
        "\n",
        "    # Ensure at least empty domains key\n",
        "    if 'domains' not in eval_result:\n",
        "        eval_result['domains'] = {}\n",
        "    return eval_result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77e7fed9",
      "metadata": {},
      "source": [
        "## 5. Compute Inter-Rater Reliability (Cohen's Kappa)\n",
        "\n",
        "Define functions to compute Cohen's kappa (unweighted or weighted) between human and AI scores for each component or domain. We will treat the AI as a second rater and compute:\n",
        "- Exact match agreement (δ = 0)\n",
        "- Near-match agreement (δ = 1) for ordinal scales (1-5 or L/M/H)\n",
        "\n",
        "This section provides functions to:\n",
        "1. Encode categorical labels into integers.\n",
        "2. Compute Cohen's kappa for a single component across all items.\n",
        "3. Compute κ for each component and possibly domain-level aggregated scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "42904cc8",
      "metadata": {},
      "outputs": [],
      "source": [
        "def encode_for_kappa(series: pd.Series, score_list: List[Any]) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Map series values to integer codes based on score_list ordering.\n",
        "    \"\"\"\n",
        "    mapping: Dict[Any, int] = {}\n",
        "    for idx, label in enumerate(score_list):\n",
        "        mapping[label] = idx\n",
        "    return series.map(lambda x: mapping.get(x, mapping.get('N/A', len(score_list)-1)))\n",
        "\n",
        "def compute_component_kappa(human_series: pd.Series,\n",
        "                            ai_series: pd.Series,\n",
        "                            score_list: List[Any],\n",
        "                            weight: Optional[str] = None\n",
        "                           ) -> float:\n",
        "    \"\"\"\n",
        "    Compute Cohen's kappa between human_series and ai_series using the provided score_list for mapping.\n",
        "    \"\"\"\n",
        "    human_encoded = encode_for_kappa(human_series, score_list)\n",
        "    ai_encoded = encode_for_kappa(ai_series, score_list)\n",
        "    kappa = cohen_kappa_score(human_encoded, ai_encoded, weights=weight)\n",
        "    return kappa"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f5c5aee",
      "metadata": {},
      "source": [
        "## 6. Widgets for Model and Dataset Selection\n",
        "\n",
        "Use `ipywidgets` to allow manual selection of which AI models to evaluate and which dataset split (Train/Validation or Test). We will create:\n",
        "1. `model_selector`: A `SelectMultiple` widget listing available model evaluator class names.\n",
        "2. `set_selector`: A `Dropdown` widget to choose between \"Train/Validation\" and \"Test\" splits.\n",
        "3. `test_confirm`: A `Checkbox` that the user must check to confirm evaluation on the Test set.\n",
        "4. `run_button`: A `Button` to trigger the evaluation process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "08a73ff3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -q groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a42bda23",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-08 13:41:10,037 - new.models.AI - WARNING - GROQ_API_KEY environment variable not set\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "095734ad267344839a83969eae29b8c9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "SelectMultiple(description='Select Models', index=(0,), options=('BaseEvaluator',), value=('BaseEvaluator',))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ec377752dacc4a668f65e769400a97da",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Dropdown(description='Dataset Split', options=('Train/Validation', 'Test'), value='Train/Validation')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5f71213b9ec5472bb8557f9d89b740d8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Checkbox(value=False, description='Confirm Test Set Evaluation')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dcea652a305e47a0a3e238c0c7fa0fb1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Button(button_style='primary', description='Run Evaluation', style=ButtonStyle())"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0f4cfa42dde843b69c285529e0653362",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating models:   0%|          | 0/118 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Define available model evaluator classes here (import statements should be adjusted as needed).\n",
        "import os\n",
        "import sys\n",
        "\n",
        "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"../../../\")))\n",
        "\n",
        "from new.models.base.BaseModelEvaluator import BaseModelEvaluator\n",
        "# from new.models.chain.ChainModelEvaluator import ChainModelEvaluator\n",
        "# from new.models.CoT.CoTModelEvaluator import CoTModelEvaluator\n",
        "\n",
        "# This dictionary maps display names to the actual classes\n",
        "available_models = {\n",
        "    'BaseEvaluator': BaseModelEvaluator,\n",
        "    # 'ChainEvaluator': ChainModelEvaluator,\n",
        "    # 'CoTEvaluator': CoTModelEvaluator\n",
        "}\n",
        "\n",
        "# Model selector widget with default selection of all models\n",
        "model_selector = widgets.SelectMultiple(\n",
        "    options=list(available_models.keys()),\n",
        "    value=list(available_models.keys()),  # default to selecting all available models\n",
        "    description='Select Models',\n",
        "    disabled=False\n",
        ")\n",
        "display(model_selector)\n",
        "\n",
        "# Dataset split selector\n",
        "set_selector = widgets.Dropdown(\n",
        "    options=['Train/Validation', 'Test'],\n",
        "    value='Train/Validation',\n",
        "    description='Dataset Split',\n",
        "    disabled=False\n",
        ")\n",
        "display(set_selector)\n",
        "\n",
        "# Test set confirmation checkbox\n",
        "test_confirm = widgets.Checkbox(\n",
        "    value=False,\n",
        "    description='Confirm Test Set Evaluation',\n",
        "    disabled=False\n",
        ")\n",
        "display(test_confirm)\n",
        "\n",
        "# Run evaluation button\n",
        "run_button = widgets.Button(\n",
        "    description='Run Evaluation',\n",
        "    button_style='primary'\n",
        ")\n",
        "display(run_button)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ac1e06e",
      "metadata": {},
      "source": [
        "## 7. Evaluation Execution Function\n",
        "\n",
        "Define the `run_evaluation` function to orchestrate the entire evaluation pipeline when the user clicks the \"Run Evaluation\" button. This function will:\n",
        "1. Check if Test set evaluation is allowed (if `set_selector` is \"Test\").\n",
        "2. Load human evaluations and transcripts for the selected split.\n",
        "3. Loop through each selected model, call it for every item, and collect AI evaluations.\n",
        "4. Compute per-component distances and domain/overall distances.\n",
        "5. Compute inter-rater reliability (Cohen's kappa) for each component.\n",
        "6. (Optional) Compute LLM-as-Judge metric.\n",
        "7. Store results in DataFrames: `ai_eval_df`, `distances_df`, `kappa_df`.\n",
        "8. Trigger visualizations to display results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fb42cbd",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_evaluation(_):\n",
        "    \"\"\"\n",
        "    Main function to run evaluations on each clip using selected models.\n",
        "    Supports resuming, shows a tqdm progress bar, saves per-model CSVs,\n",
        "    and parallelizes model calls.\n",
        "    \"\"\"\n",
        "    global GLOBAL_AI_EVAL_DF, GLOBAL_DISTANCES_DF, GLOBAL_KAPPA_DF, GLOBAL_HUMAN_EVAL_DF\n",
        "    \n",
        "    # Prevent accidental test-set runs\n",
        "    if set_selector.value == 'Test' and not test_confirm.value:\n",
        "        print('⚠️ You must confirm test-set evaluation before running.')\n",
        "        return\n",
        "\n",
        "    # Load framework and raw transcripts\n",
        "    framework = json.load(open('/Users/mkrasnow/Desktop/montesa/new/models/_context/Teach_1.json'))\n",
        "    df_all = pd.read_csv('/Users/mkrasnow/Desktop/montesa/new/formattedData/peru_cleaned_transcripts.csv', dtype=str)\n",
        "\n",
        "    # Extract base_id and clip_number as Series\n",
        "    df_all['base_id'] = df_all['School_Clip'].str.extract(r'(\\d{6,7})', expand=False)\n",
        "    df_all['clip_number'] = (\n",
        "        df_all['School_Clip']\n",
        "        .str.extract(r'Clip\\s*([12])', expand=False)\n",
        "        .astype(int)\n",
        "        .map({1: 'first', 2: 'last'})\n",
        "    )\n",
        "\n",
        "    # Apply split filter\n",
        "    if set_selector.value == 'Test':\n",
        "        df_split = df_all[df_all['split'] == 'test'].copy()\n",
        "    else:\n",
        "        df_split = df_all[df_all['split'].isin(['train', 'val'])].copy()\n",
        "\n",
        "    # Get human evaluations directly from the dataframe\n",
        "    evaluation_columns = [\n",
        "        'Teacher provides learning activity - 1st Snapshot',\n",
        "        'Students are on task - 1st Snapshot',\n",
        "        'Teacher provides learning activity - 2nd Snapshot',\n",
        "        'Students are on task - 2nd Snapshot',\n",
        "        'Teacher provides learning activity - 3rd Snapshot',\n",
        "        'Students are on task - 3rd Snapshot',\n",
        "        'Supportive Learning Environment',\n",
        "        'The teacher treats all students respectfully',\n",
        "        'The teacher uses positive language',\n",
        "        'The teacher responds to students needs',\n",
        "        'The teacher does not exhibit gender bias',\n",
        "        'Positive Behavioral Expectations',\n",
        "        'The teacher sets clear behavioral expectations',\n",
        "        'The teacher acknowledges positive student behavior',\n",
        "        'The teacher redirects misbehavior',\n",
        "        'Lesson Facilitation',\n",
        "        'The teacher explicitly articulates learning objectives',\n",
        "        'The teacher\\'s explanation of content is clear',\n",
        "        'The teacher makes connections in the lesson',\n",
        "        'The teacher models by enacting or thinking aloud',\n",
        "        'Checks for understanding',\n",
        "        'The teacher uses questions',\n",
        "        'The teacher uses prompts',\n",
        "        'The teacher monitors most students',\n",
        "        'The teacher adjusts teaching to the level of students',\n",
        "        'Feedback',\n",
        "        'The teacher provides specific comments for misunderstandings',\n",
        "        'The teacher provides specific comments for successes',\n",
        "        'Critical Thinking',\n",
        "        'The teacher asks open-ended questions',\n",
        "        'The teacher provides thinking tasks',\n",
        "        'Students ask open-ended questions or perform thinking tasks',\n",
        "        'Autonomy',\n",
        "        'The teacher provides students with choices',\n",
        "        'The teacher provides students with opportunities to take meaningful roles',\n",
        "        'Students volunteer to participate in the classroom',\n",
        "        'Perseverance',\n",
        "        'The teacher acknowledges students\\' efforts',\n",
        "        'The teacher has a positive attitude towards students\\' challenges',\n",
        "        'The teacher encourages goal-setting',\n",
        "        'Social & Collaborative Skills',\n",
        "        'The teacher promotes students\\' collaboration',\n",
        "        'The teacher promotes students\\' interpersonal skills'\n",
        "    ]\n",
        "    \n",
        "    # Filter to existing evaluation columns\n",
        "    present_eval_cols = [col for col in evaluation_columns if col in df_split.columns]\n",
        "    print(f\"✅ Found {len(present_eval_cols)} evaluation columns in the dataset\")\n",
        "    \n",
        "    # Store human evaluations for later use\n",
        "    GLOBAL_HUMAN_EVAL_DF = df_split[['base_id', 'clip_number'] + present_eval_cols].copy()\n",
        "\n",
        "    # Prepare output directory\n",
        "    outdir = 'model_evaluation_data'\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "\n",
        "    # Determine already evaluated items to skip for each selected model\n",
        "    skip_records_by_model = {}\n",
        "    for model_name in model_selector.value:\n",
        "        skip_records_by_model[model_name] = set()\n",
        "        fname = f\"{outdir}/{model_name}_evaluations.csv\"\n",
        "        if os.path.exists(fname):\n",
        "            df_prev = pd.read_csv(fname)\n",
        "            # Identify evaluation columns that require AI output (exclude manual components)\n",
        "            manual_components = [\n",
        "                c['name'] \n",
        "                for domain in framework['structure']['domains'] \n",
        "                for c in domain['components'] \n",
        "                if c.get('isManuallyScored', False)\n",
        "            ]\n",
        "            eval_cols = [col for col in df_prev.columns \n",
        "                         if col not in ['base_id', 'clip_number', 'model_name']]\n",
        "            required_cols = [col for col in eval_cols if col not in manual_components]\n",
        "            # Mask rows that have all required columns filled (no NaN in required cols)\n",
        "            if required_cols:\n",
        "                completed_mask = df_prev[required_cols].notna().all(axis=1)\n",
        "            else:\n",
        "                completed_mask = pd.Series([True] * len(df_prev))\n",
        "            completed_rows = df_prev[completed_mask]\n",
        "            # Build a set of (base_id, clip_number) for already evaluated rows\n",
        "            skip_records_by_model[model_name] = set(\n",
        "                zip(completed_rows['base_id'].astype(str), \n",
        "                    completed_rows['clip_number'].astype(str))\n",
        "            )\n",
        "\n",
        "    # Build list of tasks (row, model_name), skipping those already evaluated\n",
        "    tasks = []\n",
        "    for model_name in model_selector.value:\n",
        "        for _, row in df_split.iterrows():\n",
        "            base_id = str(row['base_id'])\n",
        "            clip_num = str(row['clip_number'])\n",
        "            if (base_id, clip_num) in skip_records_by_model.get(model_name, set()):\n",
        "                print(f\"⚡️ Skipping already evaluated base_id {base_id}, clip {clip_num} for model {model_name}\")\n",
        "                continue\n",
        "            tasks.append((row, model_name))\n",
        "    total_tasks = len(tasks)\n",
        "    if total_tasks == 0:\n",
        "        print(\"✅ No evaluations to run.\")\n",
        "        return\n",
        "\n",
        "    # Container for all results\n",
        "    all_records: List[Dict[str, Any]] = []\n",
        "\n",
        "    # Run tasks in parallel with a progress bar\n",
        "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "        futures = {}\n",
        "        for row, model_name in tasks:\n",
        "            # Determine transcript text and audio path for the clip\n",
        "            if row['clip_number'] == 'first':\n",
        "                transcript_text = row.get('First Audio Transcript Text', '')\n",
        "                audio_file_path = row.get('First Audio Clip', '')\n",
        "            else:\n",
        "                transcript_text = row.get('Last Audio Transcript Text', '')\n",
        "                audio_file_path = row.get('Last Audio Clip', '')\n",
        "            # Skip and warn if no transcript is available for this clip\n",
        "            if not transcript_text or str(transcript_text).strip() == '':\n",
        "                print(f\"⚠️ No transcript available for base_id {row['base_id']} clip {row['clip_number']}. Skipping.\")\n",
        "                continue\n",
        "            # Submit the evaluation task to the thread pool\n",
        "            future = executor.submit(\n",
        "                call_model,\n",
        "                available_models[model_name],\n",
        "                framework,\n",
        "                transcript_text,\n",
        "                audio_file_path=audio_file_path if audio_file_path else None\n",
        "            )\n",
        "            futures[future] = (row, model_name)\n",
        "\n",
        "        completed_count = 0\n",
        "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Evaluating models\"):\n",
        "            completed_count += 1\n",
        "            row, model_name = futures[future]\n",
        "            # Prepare a short snippet of the transcript for display (not the whole text)\n",
        "            snippet_text = (row.get('First Audio Transcript Text', '') \n",
        "                            if row['clip_number'] == 'first' \n",
        "                            else row.get('Last Audio Transcript Text', ''))\n",
        "            snippet_clean = str(snippet_text).strip().replace('\\n', ' ')\n",
        "            snippet = snippet_clean[:100] + \"...\" if len(snippet_clean) > 100 else snippet_clean\n",
        "            # Print progress update for the completed evaluation\n",
        "            print(f\"✅ Completed {completed_count}/{len(futures)}: base_id {row['base_id']} | clip {row['clip_number']} | model {model_name}. Transcript snippet: {snippet}\")\n",
        "            try:\n",
        "                eval_output = future.result()\n",
        "            except Exception as e:\n",
        "                eval_output = {'domains': {}, 'summary': '', 'error': str(e)}\n",
        "            # Flatten domain/component scores into a record\n",
        "            flat = {\n",
        "                'base_id': row['base_id'],\n",
        "                'clip_number': row['clip_number'],\n",
        "                'model_name': model_name\n",
        "            }\n",
        "            for domain in eval_output.get('domains', {}).values():\n",
        "                for comp_id, comp_data in domain.get('components', {}).items():\n",
        "                    # Map comp_id to comp_name via framework and add score\n",
        "                    for d in framework['structure']['domains']:\n",
        "                        for c in d['components']:\n",
        "                            if str(c['id']) == str(comp_id):\n",
        "                                flat[c['name']] = comp_data.get('score')\n",
        "                                break\n",
        "            all_records.append(flat)\n",
        "\n",
        "    # Build DataFrame of all evaluations\n",
        "    ai_eval_df = pd.DataFrame(all_records)\n",
        "    GLOBAL_AI_EVAL_DF = ai_eval_df\n",
        "\n",
        "    # Save a **separate CSV per model**, named \"{model_name}_evaluations.csv\"\n",
        "    for model_name, group_df in ai_eval_df.groupby('model_name'):\n",
        "        fname = f\"{outdir}/{model_name}_evaluations.csv\"\n",
        "        group_df.to_csv(fname, index=False)\n",
        "        print(f\"✅ Saved evaluations for {model_name} → {fname}\")\n",
        "\n",
        "    # Compute and save distances & kappas\n",
        "    # -- distances\n",
        "    distance_records = []\n",
        "    for _, row in ai_eval_df.iterrows():\n",
        "        # Find matching human evaluation row\n",
        "        h_match = GLOBAL_HUMAN_EVAL_DF[\n",
        "            (GLOBAL_HUMAN_EVAL_DF['base_id'] == row['base_id']) &\n",
        "            (GLOBAL_HUMAN_EVAL_DF['clip_number'] == row['clip_number'])\n",
        "        ]\n",
        "        \n",
        "        if len(h_match) > 0:\n",
        "            h = h_match.iloc[0]\n",
        "            domains, overall = compute_distances_for_item(h, row, framework)\n",
        "            rec = {\n",
        "                'base_id': row['base_id'],\n",
        "                'clip_number': row['clip_number'],\n",
        "                'model_name': row['model_name'],\n",
        "                **{f\"domain_{did}_dist\": d for did, d in domains.items()},\n",
        "                'overall_distance': overall\n",
        "            }\n",
        "            distance_records.append(rec)\n",
        "    \n",
        "    distances_df = pd.DataFrame(distance_records).set_index(['base_id', 'clip_number', 'model_name'])\n",
        "    GLOBAL_DISTANCES_DF = distances_df\n",
        "    distances_df.to_csv(f\"{outdir}/distance_metrics.csv\")\n",
        "\n",
        "    # -- kappas\n",
        "    kappa_records = []\n",
        "    for domain in framework['structure']['domains']:\n",
        "        for comp in domain['components']:\n",
        "            comp_id, comp_name, score_list = str(comp['id']), comp['name'], comp.get('scoreList', ['Y', 'N', 'N/A'])\n",
        "            \n",
        "            # Skip if component not in evaluation columns\n",
        "            if comp_name not in present_eval_cols:\n",
        "                continue\n",
        "                \n",
        "            for model_name in ai_eval_df['model_name'].unique():\n",
        "                # Get human scores for this component\n",
        "                human_ser = GLOBAL_HUMAN_EVAL_DF[comp_name]\n",
        "                \n",
        "                # Get AI scores for this component and model\n",
        "                ai_model_df = ai_eval_df[ai_eval_df['model_name'] == model_name]\n",
        "                if comp_name in ai_model_df.columns:\n",
        "                    ai_ser = ai_model_df[comp_name]\n",
        "                    \n",
        "                    # Align the series by index if needed\n",
        "                    common_idx = human_ser.index.intersection(ai_ser.index)\n",
        "                    if len(common_idx) > 1:\n",
        "                        human_aligned = human_ser.loc[common_idx]\n",
        "                        ai_aligned = ai_ser.loc[common_idx]\n",
        "                        \n",
        "                        weight = 'quadratic' if any(lbl in ['L', 'M', 'H'] for lbl in score_list) else None\n",
        "                        try:\n",
        "                            val = compute_component_kappa(human_aligned, ai_aligned, score_list, weight)\n",
        "                        except:\n",
        "                            val = np.nan\n",
        "                    else:\n",
        "                        val = np.nan\n",
        "                else:\n",
        "                    val = np.nan\n",
        "                    \n",
        "                kappa_records.append({\n",
        "                    'component_id': comp_id,\n",
        "                    'component_name': comp_name,\n",
        "                    'model_name': model_name,\n",
        "                    'kappa': val\n",
        "                })\n",
        "    \n",
        "    kappa_df = pd.DataFrame(kappa_records)\n",
        "    GLOBAL_KAPPA_DF = kappa_df\n",
        "    kappa_df.to_csv(f\"{outdir}/kappa_results.csv\", index=False)\n",
        "\n",
        "    print(f\"✅ Distance and kappa metrics saved under `{outdir}/`\")\n",
        "\n",
        "    # Show final plots\n",
        "    visualize_results()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "08b55cb6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Link the run_button click to run_evaluation\n",
        "run_button.on_click(run_evaluation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81c3624c",
      "metadata": {},
      "source": [
        "## 8. Visualizations\n",
        "\n",
        "Define the `visualize_results` function to produce plots for:\n",
        "1. Boxplots of component-level distances for each model.\n",
        "2. Histograms of overall distances per model.\n",
        "3. Scatter plot: transcript length vs. overall distance.\n",
        "4. Bar chart of Cohen's kappa values per model under exact match vs. ±1 thresholds (for ordinal scales).\n",
        "5. (Optional) Bar chart for LLM-as-Judge win rates.\n",
        "\n",
        "These functions will use the global variables populated by `run_evaluation`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "33172c16",
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_results():\n",
        "    \"\"\"\n",
        "    Produce comprehensive visualizations based on GLOBAL_DISTANCES_DF and GLOBAL_KAPPA_DF.\n",
        "    \"\"\"\n",
        "    # Check if global variables exist\n",
        "    if 'GLOBAL_DISTANCES_DF' not in globals() or 'GLOBAL_KAPPA_DF' not in globals():\n",
        "        print(\"❌ No evaluation results found. Please run evaluation first.\")\n",
        "        return\n",
        "        \n",
        "    # Retrieve global DataFrames\n",
        "    distances_df = GLOBAL_DISTANCES_DF.reset_index()\n",
        "    kappa_df = GLOBAL_KAPPA_DF.copy()\n",
        "    human_df = GLOBAL_HUMAN_EVAL_DF.copy()\n",
        "    \n",
        "    # 1. Boxplot of overall distances per model\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    model_names = distances_df['model_name'].unique()\n",
        "    model_distances = [distances_df[distances_df['model_name'] == model]['overall_distance'].dropna() \n",
        "                      for model in model_names]\n",
        "    \n",
        "    plt.boxplot(model_distances, labels=model_names)\n",
        "    plt.title('Boxplot of Overall Distances per Model')\n",
        "    plt.ylabel('Overall Normalized Distance')\n",
        "    plt.xlabel('Model')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # 2. Histogram of overall distances per model\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for model_name in distances_df['model_name'].unique():\n",
        "        model_distances = distances_df[distances_df['model_name'] == model_name]['overall_distance']\n",
        "        plt.hist(model_distances.dropna(), alpha=0.5, bins=20, label=model_name)\n",
        "    plt.title('Histogram of Overall Distances')\n",
        "    plt.xlabel('Overall Normalized Distance')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "    # 3. Scatter: transcript length vs. overall distance\n",
        "    try:\n",
        "        df_all = pd.read_csv('/Users/mkrasnow/Desktop/montesa/new/formattedData/peru_cleaned_transcripts.csv', dtype=str)\n",
        "        df_all['base_id'] = df_all['School_Clip'].str.extract(r\"(\\d{6,7})\")\n",
        "        df_all = df_all.drop_duplicates(subset=['base_id']).set_index('base_id')\n",
        "        \n",
        "        word_counts = df_all['Last Audio Transcript Word Count'].astype(float)\n",
        "        merged = distances_df.merge(word_counts.rename('word_count'), left_on='base_id', right_index=True, how='left')\n",
        "        \n",
        "        plt.figure(figsize=(8, 6))\n",
        "        for model_name in merged['model_name'].unique():\n",
        "            subset = merged[merged['model_name'] == model_name]\n",
        "            subset_clean = subset.dropna(subset=['word_count', 'overall_distance'])\n",
        "            if len(subset_clean) > 0:\n",
        "                plt.scatter(subset_clean['word_count'], subset_clean['overall_distance'], \n",
        "                           alpha=0.3, label=model_name)\n",
        "        plt.xlabel('Transcript Word Count')\n",
        "        plt.ylabel('Overall Normalized Distance')\n",
        "        plt.title('Distance vs. Transcript Length')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.show()\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Could not create scatter plot: {e}\")\n",
        "    \n",
        "    # 4. Bar chart of average Cohen's kappa per model\n",
        "    kappa_summary = kappa_df.groupby('model_name')['kappa'].mean().reset_index()\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.bar(kappa_summary['model_name'], kappa_summary['kappa'])\n",
        "    plt.title(\"Average Cohen's Kappa per Model (All Components)\")\n",
        "    plt.ylabel('Average Kappa')\n",
        "    plt.xlabel('Model')\n",
        "    plt.ylim(-1, 1)\n",
        "    plt.grid(True, axis='y', alpha=0.3)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # 5. Component-wise kappa heatmap\n",
        "    if len(kappa_df) > 0:\n",
        "        kappa_pivot = kappa_df.pivot(index='component_name', columns='model_name', values='kappa')\n",
        "        \n",
        "        plt.figure(figsize=(12, 10))\n",
        "        import seaborn as sns\n",
        "        sns.heatmap(kappa_pivot, annot=True, cmap='RdYlBu_r', center=0, \n",
        "                   fmt='.2f', cbar_kws={'label': 'Cohen\\'s Kappa'})\n",
        "        plt.title('Component-wise Cohen\\'s Kappa by Model')\n",
        "        plt.xlabel('Model')\n",
        "        plt.ylabel('Component')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.yticks(rotation=0)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    # Print summary statistics\n",
        "    print(\"\\n📊 EVALUATION SUMMARY:\")\n",
        "    print(f\"Total evaluations performed: {len(GLOBAL_AI_EVAL_DF)}\")\n",
        "    print(f\"Models evaluated: {list(GLOBAL_AI_EVAL_DF['model_name'].unique())}\")\n",
        "    \n",
        "    # Distance summary\n",
        "    print(\"\\n📏 DISTANCE METRICS:\")\n",
        "    for model_name in distances_df['model_name'].unique():\n",
        "        model_dists = distances_df[distances_df['model_name'] == model_name]['overall_distance']\n",
        "        print(f\"{model_name}: mean={model_dists.mean():.3f}, std={model_dists.std():.3f}\")\n",
        "    \n",
        "    # Kappa summary\n",
        "    print(\"\\n🎯 INTER-RATER RELIABILITY (Kappa):\")\n",
        "    for model_name in kappa_summary['model_name']:\n",
        "        avg_kappa = kappa_summary[kappa_summary['model_name'] == model_name]['kappa'].iloc[0]\n",
        "        print(f\"{model_name}: average κ = {avg_kappa:.3f}\")\n",
        "    \n",
        "    print('\\n✅ Visualizations complete.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a32f06d4",
      "metadata": {},
      "source": [
        "## 9. Manual Controls for Test Set Evaluation\n",
        "\n",
        "We have already set up `test_confirm` as a checkbox and linked it in `run_evaluation`. If the user selects \"Test\" in `set_selector` and does not check `test_confirm`, the evaluation will not proceed. This ensures we do not accidentally run expensive model calls on the test set without explicit confirmation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90be3ef0",
      "metadata": {},
      "source": [
        "## 10. Save Intermediate Results\n",
        "\n",
        "Optionally save the distances DataFrame and kappa results to disk so that we do not need to rerun all model calls during iterative notebook development.\n",
        "The following code saves `GLOBAL_DISTANCES_DF` and `GLOBAL_KAPPA_DF` to CSV files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9b515a0b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_results():\n",
        "    \"\"\"\n",
        "    Save GLOBAL_DISTANCES_DF and GLOBAL_KAPPA_DF to CSV files.\n",
        "    \"\"\"\n",
        "    if 'GLOBAL_DISTANCES_DF' in globals() and 'GLOBAL_KAPPA_DF' in globals():\n",
        "        GLOBAL_DISTANCES_DF.reset_index().to_csv('evaluation_distances.csv', index=False)\n",
        "        GLOBAL_KAPPA_DF.to_csv('evaluation_kappa.csv', index=False)\n",
        "        print('Results saved to evaluation_distances.csv and evaluation_kappa.csv')\n",
        "    else:\n",
        "        print('No results to save. Run evaluation first.')\n",
        "\n",
        "# Call save_results() when needed\n",
        "# save_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19f7a69a",
      "metadata": {},
      "source": [
        "## Notes and Next Steps\n",
        "\n",
        "- **Data Splits**: The notebook now properly uses the `split` column from the cleaned CSV to filter train/validation vs test data.\n",
        "- **Model Imports**: Updated the import paths for evaluator classes to match your project structure.\n",
        "- **Audio File Paths**: Fixed to use the actual audio file paths stored in `First Audio Clip` and `Last Audio Clip` columns.\n",
        "- **Human Evaluation Columns**: Now directly uses the evaluation columns that exist in the CSV rather than trying to load from framework JSON.\n",
        "- **Component Mapping**: Fixed the mapping between framework components and actual CSV column names.\n",
        "- **Error Handling**: Enhanced error handling for missing data and failed model calls.\n",
        "- **Parallelization**: Uses ThreadPoolExecutor for efficient parallel model evaluation.\n",
        "- **Visualizations**: Added comprehensive visualizations including heatmaps and summary statistics.\n",
        "\n",
        "This notebook now properly handles the actual data format and should work with your Peru transcript dataset. The key fixes include:\n",
        "\n",
        "1. **Correct evaluation column extraction** from the actual CSV columns\n",
        "2. **Proper audio file path usage** from the `First Audio Clip` and `Last Audio Clip` columns\n",
        "3. **Fixed transcript text retrieval** using the correct column names\n",
        "4. **Improved data alignment** between human and AI evaluations\n",
        "5. **Enhanced error handling** for missing or malformed data\n",
        "\n",
        "The notebook is now ready to run comprehensive validation and evaluation of your AI models against human evaluations."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Harvard",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
