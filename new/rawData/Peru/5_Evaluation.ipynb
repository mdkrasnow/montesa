{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8357fab8",
      "metadata": {},
      "source": [
        "# Validation Evaluation Pipeline Notebook\n",
        "\n",
        "This notebook implements the plan for building a validation and evaluation pipeline for comparing AI-based evaluators against human evaluations. It follows the detailed plan to load data, define distance metrics, compute inter-rater reliability, incorporate multiple AI evaluators, and produce comprehensive visualizations.\n",
        "\n",
        "**Notebook Structure**:\n",
        "1. Imports and Setup\n",
        "2. Load Framework and Human Evaluations\n",
        "3. Helper Functions: Mapping and Distance Computation\n",
        "4. Model Calling Helper\n",
        "5. Compute Inter-Rater Reliability (Cohen's Kappa)\n",
        "6. Manual Configuration: Models & Dataset Selection\n",
        "7. Prepare Tasks for Debugging (skip completed tasks)\n",
        "8. Evaluation Execution (progressive saving every 5 evaluations)\n",
        "9. Visualizations\n",
        "10. Save Intermediate Results\n",
        "\n",
        "*Note: This notebook assumes that the evaluation framework JSON (`Teach_1.json`) and the cleaned transcripts CSV (`peru_cleaned_transcripts.csv`) are available in the working directory. Intermediate results will be saved progressively to the `model_evaluation_data` directory.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd2da427",
      "metadata": {},
      "source": [
        "## Root Cause Analysis\n",
        "\n",
        "Using `ipywidgets` and registering `run_evaluation` via `run_button.on_click` executes the evaluation in an asynchronous callback context. In Jupyter, output from widget callbacks does not appear in the original code cell output area, causing debugging logs and `print` statements to be hidden. To fix this, we remove widgets entirely and invoke the evaluation functions directly within cells, ensuring all logs and prints are displayed inline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3342a0f6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Imports and Setup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "import logging\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from tqdm.notebook import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import math\n",
        "import inspect\n",
        "from typing import Dict, Any, Optional, List, Tuple\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Detect Google Colab environment\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "print(f\"Libraries imported successfully.  IN_COLAB = {IN_COLAB}\")\n",
        "\n",
        "# Utility: download file from Google Drive if running in Colab\n",
        "if IN_COLAB:\n",
        "    try:\n",
        "        import gdown\n",
        "    except ImportError:\n",
        "        # Install gdown if missing\n",
        "        print(\"ðŸ”„ Installing gdown for Drive downloads...\")\n",
        "        !pip install --quiet gdown\n",
        "        import gdown\n",
        "\n",
        "    def download_from_gdrive(url: str) -> str:\n",
        "        \"\"\"\n",
        "        Download a file from a Google Drive link to the local environment.\n",
        "        Returns the local file path.\n",
        "        \"\"\"\n",
        "        print(f\"ðŸ” Downloading audio file from Google Drive: {url}\")\n",
        "        # Let gdown handle the link parsing\n",
        "        local_filename = os.path.basename(url.split('?')[0])\n",
        "        if not local_filename:\n",
        "            local_filename = 'audio_file'\n",
        "        try:\n",
        "            gdown.download(url, local_filename, quiet=False)\n",
        "            print(f\"âœ… Downloaded audio to {local_filename}\")\n",
        "            return local_filename\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error downloading audio: {e}\")\n",
        "            raise\n",
        "else:\n",
        "    def download_from_gdrive(url: str) -> str:\n",
        "        \"\"\"\n",
        "        Dummy stub when not in Colab to prevent accidental downloads.\n",
        "        \"\"\"\n",
        "        raise RuntimeError(\n",
        "            \"Multimodal evaluation requires Google Colab environment for audio download.\"\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e0674e4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Load Framework and Human Evaluations\n",
        "def load_human_evaluations(cleaned_csv_path: str, framework_json_path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load cleaned transcripts and human evaluation columns.\n",
        "    Returns a DataFrame with columns ['base_id','clip_number',<each human component name>].\n",
        "    \"\"\"\n",
        "    # Read the cleaned transcripts CSV\n",
        "    df = pd.read_csv(cleaned_csv_path, dtype=str)\n",
        "    \n",
        "    # The actual evaluation columns from the CSV\n",
        "    evaluation_columns = [\n",
        "        'Teacher provides learning activity - 1st Snapshot',\n",
        "        'Students are on task - 1st Snapshot',\n",
        "        'Teacher provides learning activity - 2nd Snapshot',\n",
        "        'Students are on task - 2nd Snapshot',\n",
        "        'Teacher provides learning activity - 3rd Snapshot',\n",
        "        'Students are on task - 3rd Snapshot',\n",
        "        'Supportive Learning Environment',\n",
        "        'The teacher treats all students respectfully',\n",
        "        'The teacher uses positive language',\n",
        "        'The teacher responds to students needs',\n",
        "        'The teacher does not exhibit gender bias',\n",
        "        'Positive Behavioral Expectations',\n",
        "        'The teacher sets clear behavioral expectations',\n",
        "        'The teacher acknowledges positive student behavior',\n",
        "        'The teacher redirects misbehavior',\n",
        "        'Lesson Facilitation',\n",
        "        'The teacher explicitly articulates learning objectives',\n",
        "        'The teacher\\'s explanation of content is clear',\n",
        "        'The teacher makes connections in the lesson',\n",
        "        'The teacher models by enacting or thinking aloud',\n",
        "        'Checks for understanding',\n",
        "        'The teacher uses questions',\n",
        "        'The teacher uses prompts',\n",
        "        'The teacher monitors most students',\n",
        "        'The teacher adjusts teaching to the level of students',\n",
        "        'Feedback',\n",
        "        'The teacher provides specific comments for misunderstandings',\n",
        "        'The teacher provides specific comments for successes',\n",
        "        'Critical Thinking',\n",
        "        'The teacher asks open-ended questions',\n",
        "        'The teacher provides thinking tasks',\n",
        "        'Students ask open-ended questions or perform thinking tasks',\n",
        "        'Autonomy',\n",
        "        'The teacher provides students with choices',\n",
        "        'The teacher provides students with opportunities to take meaningful roles',\n",
        "        'Students volunteer to participate in the classroom',\n",
        "        'Perseverance',\n",
        "        'The teacher acknowledges students\\' efforts',\n",
        "        'The teacher has a positive attitude towards students\\' challenges',\n",
        "        'The teacher encourages goal-setting',\n",
        "        'Social & Collaborative Skills',\n",
        "        'The teacher promotes students\\' collaboration',\n",
        "        'The teacher promotes students\\' interpersonal skills',\n",
        "    ]\n",
        "\n",
        "    # Extract base_id and clip_number from School_Clip\n",
        "    clip_info = df['School_Clip'].str.extract(r'(?P<base_id>\\d{6,7})\\s*Clip\\s*(?P<clip_num>[12])')\n",
        "    df['base_id'] = clip_info['base_id']\n",
        "    df['clip_number'] = clip_info['clip_num'].map({'1': 'first', '2': 'last'})\n",
        "\n",
        "    # Identify which evaluation columns actually exist\n",
        "    present = [col for col in evaluation_columns if col in df.columns]\n",
        "    missing = [col for col in evaluation_columns if col not in df.columns]\n",
        "    if missing:\n",
        "        print(f\"âš ï¸ Missing human evaluation columns: {missing}\")\n",
        "    \n",
        "    print(f\"âœ… Found {len(present)} evaluation columns in the dataset\")\n",
        "\n",
        "    # Return only base_id, clip_number, and the human-scored columns\n",
        "    return df[['base_id', 'clip_number'] + present]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da0b0a2f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Helper Functions: Mapping and Distance Computation\n",
        "def alpha_to_numeric(x: Any) -> Optional[float]:\n",
        "    \"\"\"\n",
        "    Convert string labels to numeric:\n",
        "    - 'Y'/'y'/'Yes'/'1' -> 1.0\n",
        "    - 'N'/'n'/'No'/'0'  -> 0.0\n",
        "    - 'N/A' or empty  -> np.nan\n",
        "    - 'L' -> 1.0, 'M' -> 2.0, 'H' -> 3.0\n",
        "    - Numeric strings convertible to float -> float(x)\n",
        "    - Otherwise -> np.nan\n",
        "    \"\"\"\n",
        "    if x is None:\n",
        "        return np.nan\n",
        "    x_str = str(x).strip()\n",
        "    if x_str in {\"Y\", \"y\", \"Yes\", \"1\"}:\n",
        "        return 1.0\n",
        "    if x_str in {\"N\", \"n\", \"No\", \"0\"}:\n",
        "        return 0.0\n",
        "    if x_str in {\"N/A\", \"\", \"NA\", \"na\", \"nan\"}:\n",
        "        return np.nan\n",
        "    if x_str in {\"L\", \"M\", \"H\"}:\n",
        "        return {\"L\": 1.0, \"M\": 2.0, \"H\": 3.0}[x_str]\n",
        "    # Try converting directly to float for numeric (1-5 scale)\n",
        "    try:\n",
        "        return float(x_str)\n",
        "    except ValueError:\n",
        "        return np.nan\n",
        "\n",
        "def component_distance(human_score: Any, ai_score: Any, score_type: str) -> float:\n",
        "    \"\"\"\n",
        "    Compute normalized distance between human_score and ai_score based on score_type.\n",
        "    score_type is one of: 'YN', 'LMH', 'NUM' (for 1-5), where 'N/A' handling is builtin.\n",
        "    Returns a float in [0,1].\n",
        "    \"\"\"\n",
        "    human_num = alpha_to_numeric(human_score)\n",
        "    ai_num = alpha_to_numeric(ai_score)\n",
        "    \n",
        "    if math.isnan(human_num) and math.isnan(ai_num):\n",
        "        return 0.0\n",
        "    if math.isnan(human_num) ^ math.isnan(ai_num):\n",
        "        return 1.0\n",
        "    if score_type == 'YN':\n",
        "        d_max = 1.0\n",
        "    elif score_type == 'LMH':\n",
        "        d_max = 2.0\n",
        "    elif score_type == 'NUM':\n",
        "        d_max = 4.0\n",
        "    else:\n",
        "        d_max = 1.0\n",
        "    raw_diff = abs(human_num - ai_num)\n",
        "    normalized_diff = raw_diff / d_max\n",
        "    return min(max(normalized_diff, 0.0), 1.0)\n",
        "\n",
        "def compute_distances_for_item(human_row: pd.Series,\n",
        "                               ai_row: pd.Series,\n",
        "                               framework: Dict[str, Any]) -> Tuple[Dict[str, float], float]:\n",
        "    domain_distances: Dict[str, float] = {}\n",
        "    domain_weights_sum = 0.0\n",
        "    overall_numerator = 0.0\n",
        "    for domain in framework['structure']['domains']:\n",
        "        domain_id = str(domain['id'])\n",
        "        domain_weight = float(domain.get('weight', 1.0))\n",
        "        domain_numerator = 0.0\n",
        "        domain_weight_sum = 0.0\n",
        "        for comp in domain['components']:\n",
        "            comp_name = comp['name']\n",
        "            comp_weight = float(comp.get('weight', 1.0))\n",
        "            score_list = comp.get('scoreList', [])\n",
        "            score_type = 'NUM'\n",
        "            if set(score_list) <= {\"Y\",\"N\",\"N/A\"}:\n",
        "                score_type = 'YN'\n",
        "            elif set(score_list) <= {\"L\",\"M\",\"H\",\"N/A\"}:\n",
        "                score_type = 'LMH'\n",
        "            human_score = human_row.get(comp_name, None)\n",
        "            ai_score = ai_row.get(comp_name, None)\n",
        "            d_c = component_distance(human_score, ai_score, score_type)\n",
        "            domain_numerator += comp_weight * d_c\n",
        "            domain_weight_sum += comp_weight\n",
        "        D_domain = (domain_numerator / domain_weight_sum) if domain_weight_sum > 0 else 0.0\n",
        "        domain_distances[domain_id] = D_domain\n",
        "        overall_numerator += domain_weight * D_domain\n",
        "        domain_weights_sum += domain_weight\n",
        "    overall_distance = (overall_numerator / domain_weights_sum) if domain_weights_sum > 0 else 0.0\n",
        "    return domain_distances, overall_distance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cf8e1b0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Model Calling Helper\n",
        "def call_model(\n",
        "    evaluator_cls: type,\n",
        "    framework: Dict[str, Any],\n",
        "    transcript_text: str,\n",
        "    audio_file_path: Optional[str] = None\n",
        ") -> Dict[str, Any]:\n",
        "    evaluator = evaluator_cls(framework)\n",
        "\n",
        "    local_audio_path = None\n",
        "    # Step 1: Download (if needed)\n",
        "    if audio_file_path:\n",
        "        if not IN_COLAB:\n",
        "            error_msg = \"Multimodal evaluation requires Google Colab environment. Skipping evaluation.\"\n",
        "            print(f\"âŒ {error_msg}\")\n",
        "            return {'domains': {}, 'summary': '', 'error': error_msg}\n",
        "        try:\n",
        "            local_audio_path = download_from_gdrive(audio_file_path)\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Audio download failed: {e}\"\n",
        "            print(f\"âŒ {error_msg}\")\n",
        "            return {'domains': {}, 'summary': '', 'error': error_msg}\n",
        "\n",
        "    try:\n",
        "        # Step 2: Invoke evaluator\n",
        "        sig = inspect.signature(evaluator.evaluate)\n",
        "        if 'audio_file_path' in sig.parameters and local_audio_path:\n",
        "            result = evaluator.evaluate(transcript_text, audio_file_path=local_audio_path)\n",
        "        elif 'audio_file' in sig.parameters and local_audio_path:\n",
        "            result = evaluator.evaluate(transcript_text, audio_file=local_audio_path)\n",
        "        else:\n",
        "            result = evaluator.evaluate(transcript_text)\n",
        "\n",
        "        # Normalize returned structure\n",
        "        if isinstance(result, dict):\n",
        "            if 'success' in result:\n",
        "                if not result['success']:\n",
        "                    print(f\"âŒ Error in call_model: {result.get('error', 'Unknown error')}\")\n",
        "                    return {'domains': {}, 'summary': '', 'error': result.get('error', 'Unknown error')}\n",
        "                eval_result = result.get('evaluation', {})\n",
        "            else:\n",
        "                eval_result = result\n",
        "        else:\n",
        "            print(\"âŒ Error in call_model: Unexpected evaluator output type\")\n",
        "            return {'domains': {}, 'summary': '', 'error': 'Unexpected evaluator output type'}\n",
        "\n",
        "        if 'domains' not in eval_result:\n",
        "            print(\"âŒ Error in call_model: Missing 'domains' in evaluator output\")\n",
        "            eval_result['domains'] = {}\n",
        "\n",
        "        return eval_result\n",
        "\n",
        "    finally:\n",
        "        # Step 3: Cleanup downloaded audio\n",
        "        if local_audio_path and os.path.exists(local_audio_path):\n",
        "            try:\n",
        "                os.remove(local_audio_path)\n",
        "                print(f\"ðŸ§¹ Removed temporary audio file: {local_audio_path}\")\n",
        "            except Exception as cleanup_err:\n",
        "                print(f\"âš ï¸ Failed to remove temporary audio file {local_audio_path}: {cleanup_err}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b649f55c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Compute Inter-Rater Reliability (Cohen's Kappa)\n",
        "def encode_for_kappa(series: pd.Series, score_list: List[Any]) -> pd.Series:\n",
        "    mapping = {str(lbl): idx for idx, lbl in enumerate(score_list)}\n",
        "    na_idx = mapping.get('N/A')\n",
        "    def map_val(x):\n",
        "        x_str = str(x)\n",
        "        if x_str in mapping:\n",
        "            return mapping[x_str]\n",
        "        return na_idx if na_idx is not None else np.nan\n",
        "    return series.map(map_val)\n",
        "\n",
        "def compute_component_kappa(human_series: pd.Series,\n",
        "                            ai_series: pd.Series,\n",
        "                            score_list: List[Any],\n",
        "                            weight: Optional[str] = None\n",
        "                           ) -> float:\n",
        "    h_enc = encode_for_kappa(human_series, score_list)\n",
        "    a_enc = encode_for_kappa(ai_series, score_list)\n",
        "    mask = ~(h_enc.isna() | a_enc.isna())\n",
        "    if mask.sum() == 0:\n",
        "        return np.nan\n",
        "    if weight is None:\n",
        "        numeric_labels = all(str(lbl).isdigit() for lbl in score_list) and len(score_list) > 2\n",
        "        if any(lbl in ['L','M','H'] for lbl in score_list) or numeric_labels:\n",
        "            weight = 'quadratic'\n",
        "    return cohen_kappa_score(h_enc[mask], a_enc[mask], weights=weight)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3beedd74",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. Manual Configuration: Models & Dataset Selection\n",
        "import sys\n",
        "sys.path.append('/Users/mkrasnow/Desktop/montesa')\n",
        "\n",
        "from new.models.base.BaseModelEvaluator import BaseModelEvaluator\n",
        "# Add other evaluator imports as needed\n",
        "available_models = {\n",
        "    '5-BaseEvaluator-Validate-no-NA-data-g-25-flash': BaseModelEvaluator,\n",
        "    # 'ChainEvaluator': ChainModelEvaluator,\n",
        "    # 'CoTEvaluator': CoTModelEvaluator\n",
        "}\n",
        "\n",
        "# Manually configure which models to run\n",
        "MODEL_LIST = ['5-BaseEvaluator-Validate-no-NA-data-g-25-flash']  # e.g., ['BaseEvaluator', 'ChainEvaluator']\n",
        "\n",
        "# Manually configure dataset split: 'Train/Validation' or 'Test'\n",
        "SPLIT = 'Train/Validation'\n",
        "\n",
        "# If using Test split, set TEST_CONFIRM to True to allow evaluation\n",
        "TEST_CONFIRM = False\n",
        "\n",
        "print(f\"Configured models: {MODEL_LIST}\")\n",
        "print(f\"Configured split: {SPLIT} (Test confirm: {TEST_CONFIRM})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22a4aa8b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7. Prepare Tasks for Debugging (skip already completed)\n",
        "# Load framework and transcripts\n",
        "FRAMEWORK_PATH = '/Users/mkrasnow/Desktop/montesa/new/models/_context/Teach_1.json'\n",
        "TRANSCRIPTS_PATH = '/Users/mkrasnow/Desktop/montesa/new/formattedData/FINAL_peru_cleaned_transcripts.csv'\n",
        "OUTDIR = 'model_evaluation_data'\n",
        "os.makedirs(OUTDIR, exist_ok=True)\n",
        "\n",
        "framework = json.load(open(FRAMEWORK_PATH))\n",
        "df_all = pd.read_csv(TRANSCRIPTS_PATH, dtype=str)\n",
        "df_all['base_id'] = df_all['School_Clip'].str.extract(r\"(\\d{6,7})\", expand=False)\n",
        "df_all['clip_number'] = (\n",
        "    df_all['School_Clip']\n",
        "    .str.extract(r'Clip\\s*([12])', expand=False)\n",
        "    .map({'1': 'first', '2': 'last'})\n",
        ")\n",
        "\n",
        "# Determine completed tasks by reading existing evaluation files\n",
        "completed = set()\n",
        "for model_name in MODEL_LIST:\n",
        "    eval_file = f\"{OUTDIR}/{model_name}_evaluations.csv\"\n",
        "    if os.path.exists(eval_file):\n",
        "        df_done = pd.read_csv(eval_file, dtype=str)\n",
        "        for _, r in df_done.iterrows():\n",
        "            completed.add((r.get('base_id'), r.get('clip_number'), model_name))\n",
        "\n",
        "# Filter based on manual split configuration\n",
        "if SPLIT == 'Test' and not TEST_CONFIRM:\n",
        "    print('âš ï¸ Cannot prepare tasks: Test set evaluation not confirmed.')\n",
        "    PREPARED_TASKS = []\n",
        "else:\n",
        "    if SPLIT == 'Test':\n",
        "        df_split = df_all[df_all['split'] == 'test'].copy()\n",
        "    else:\n",
        "        df_split = df_all[df_all['split'].isin(['train', 'val'])].copy()\n",
        "\n",
        "    # Build tasks list (row, model_name) skipping completed\n",
        "    PREPARED_TASKS = []\n",
        "    for model_name in MODEL_LIST:\n",
        "        for _, row in df_split.iterrows():\n",
        "            key = (row['base_id'], row['clip_number'], model_name)\n",
        "            if key not in completed:\n",
        "                PREPARED_TASKS.append((row, model_name))\n",
        "\n",
        "    print(f\"ðŸ”§ Prepared {len(PREPARED_TASKS)} new tasks for evaluation.\")\n",
        "    for i, (row, model_name) in enumerate(PREPARED_TASKS[:5], start=1):\n",
        "        print(f\"  Task {i}: base_id={row['base_id']}, clip={row['clip_number']}, model={model_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c52085bd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 9. Visualizations\n",
        "import os\n",
        "\n",
        "def visualize_results(outdir='model_evaluation_data'):\n",
        "    # Load distance metrics CSV\n",
        "    distances_path = os.path.join(outdir, 'distance_metrics.csv')\n",
        "    if not os.path.exists(distances_path):\n",
        "        print(f\"âŒ Distance metrics file not found at {distances_path}. Please run evaluation first.\")\n",
        "        return\n",
        "    distances_df = pd.read_csv(distances_path)\n",
        "    \n",
        "    # Load kappa results CSV\n",
        "    kappa_path = os.path.join(outdir, 'kappa_results.csv')\n",
        "    if not os.path.exists(kappa_path):\n",
        "        print(f\"âŒ Kappa results file not found at {kappa_path}. Please run evaluation first.\")\n",
        "        return\n",
        "    kappa_df = pd.read_csv(kappa_path)\n",
        "\n",
        "    # Ensure required columns\n",
        "    if 'model_name' not in distances_df.columns or 'overall_distance' not in distances_df.columns:\n",
        "        print(\"âŒ Required columns missing in distances_df.\")\n",
        "        return\n",
        "\n",
        "    names = distances_df['model_name'].unique().tolist()\n",
        "\n",
        "    # 1. Boxplot of overall distances per model\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    data = [\n",
        "        distances_df.loc[distances_df['model_name'] == name, 'overall_distance'].dropna()\n",
        "        for name in names\n",
        "    ]\n",
        "    plt.boxplot(data, labels=names)\n",
        "    plt.title('Overall Distance per Model')\n",
        "    plt.ylabel('Normalized Distance')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "    # 2. Histograms of overall distances\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for name in names:\n",
        "        subset = distances_df.loc[distances_df['model_name'] == name, 'overall_distance'].dropna()\n",
        "        plt.hist(subset, bins=20, alpha=0.5, label=name)\n",
        "    plt.legend()\n",
        "    plt.title('Histogram of Overall Distances')\n",
        "    plt.xlabel('Normalized Distance')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.show()\n",
        "\n",
        "    # 3. Bar chart of average kappa per model\n",
        "    if 'model_name' in kappa_df.columns and 'kappa' in kappa_df.columns:\n",
        "        summary = kappa_df.groupby('model_name')['kappa'].mean().reset_index()\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.bar(summary['model_name'], summary['kappa'])\n",
        "        plt.title(\"Average Cohen's Kappa per Model\")\n",
        "        plt.ylim(-1, 1)\n",
        "        plt.xlabel('Model Name')\n",
        "        plt.ylabel(\"Cohen's Kappa\")\n",
        "        plt.grid(True, axis='y', alpha=0.3)\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"âŒ Required columns missing in kappa_df.\")\n",
        "\n",
        "    print(\"âœ… Visualizations complete.\")\n",
        "\n",
        "# 10. Save Intermediate Results\n",
        "def save_results():\n",
        "    if 'GLOBAL_DISTANCES_DF' in globals() and 'GLOBAL_KAPPA_DF' in globals():\n",
        "        GLOBAL_DISTANCES_DF.reset_index().to_csv('evaluation_distances.csv', index=False)\n",
        "        GLOBAL_KAPPA_DF.to_csv('evaluation_kappa.csv', index=False)\n",
        "        print('Results saved to evaluation_distances.csv and evaluation_kappa.csv')\n",
        "    else:\n",
        "        print('No results to save. Run evaluation first.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8329ca7e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 8. Evaluation Execution (progressive saving every 5)\n",
        "def run_evaluation():\n",
        "    \"\"\"\n",
        "    Run through the previously-prepared tasks in PREPARED_TASKS, saving after every 5.\n",
        "    \"\"\"\n",
        "    global GLOBAL_AI_EVAL_DF, GLOBAL_DISTANCES_DF, GLOBAL_KAPPA_DF, GLOBAL_HUMAN_EVAL_DF\n",
        "\n",
        "    tasks = PREPARED_TASKS\n",
        "    if not tasks:\n",
        "        print(\"âŒ No tasks to run. Please prepare tasks first.\")\n",
        "        return\n",
        "\n",
        "    # Load human evaluations for distance and kappa\n",
        "    GLOBAL_HUMAN_EVAL_DF = load_human_evaluations(\n",
        "        TRANSCRIPTS_PATH,\n",
        "        FRAMEWORK_PATH\n",
        "    )\n",
        "\n",
        "    all_records = []\n",
        "    outdir = OUTDIR\n",
        "\n",
        "    # Parallel execution with progress\n",
        "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "        futures = {}\n",
        "        for row, model_name in PREPARED_TASKS:\n",
        "            # Select the transcript text based on clip_number\n",
        "            transcript_text = (\n",
        "                row.get('First Audio Transcript Text', '')\n",
        "                if row['clip_number'] == 'first'\n",
        "                else row.get('Last Audio Transcript Text', '')\n",
        "            )\n",
        "\n",
        "            # Only supply an audio link for multimodal evaluators\n",
        "            if 'MultiModal' in model_name:\n",
        "                audio_file_path = (\n",
        "                    row.get('First Audio Clip', '')\n",
        "                    if row['clip_number'] == 'first'\n",
        "                    else row.get('Last Audio Clip', '')\n",
        "                ) or None\n",
        "            else:\n",
        "                audio_file_path = None\n",
        "\n",
        "            # Schedule the evaluation call\n",
        "            future = executor.submit(\n",
        "                call_model,\n",
        "                available_models[model_name],\n",
        "                framework,\n",
        "                transcript_text,\n",
        "                audio_file_path\n",
        "            )\n",
        "            futures[future] = (row, model_name)\n",
        "\n",
        "        for i, future in enumerate(\n",
        "            tqdm(as_completed(futures), total=len(futures), desc=\"Evaluating models\"),\n",
        "            start=1\n",
        "        ):\n",
        "            row, model_name = futures[future]\n",
        "            print(f\"âœ… Task {i}/{len(futures)} â†’ School_Clip={row['School_Clip']}, \"\n",
        "                  f\"base_id={row['base_id']}, clip={row['clip_number']}, model={model_name}\")\n",
        "            try:\n",
        "                eval_output = future.result()\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ Error in task {i}/{len(futures)} â†’ School_Clip={row['School_Clip']}, \"\n",
        "                      f\"base_id={row['base_id']}, clip={row['clip_number']}, model={model_name}\")\n",
        "                eval_output = {'domains': {}, 'summary': '', 'error': str(e)}\n",
        "\n",
        "            # Flatten evaluator output\n",
        "            flat = {\n",
        "                'School_Clip': row['School_Clip'],\n",
        "                'base_id': row['base_id'],\n",
        "                'clip_number': row['clip_number'],\n",
        "                'model_name': model_name\n",
        "            }\n",
        "            for domain in eval_output.get('domains', {}).values():\n",
        "                for comp_id, comp_data in domain.get('components', {}).items():\n",
        "                    for d in framework['structure']['domains']:\n",
        "                        for c in d['components']:\n",
        "                            if str(c['id']) == str(comp_id):\n",
        "                                flat[c['name']] = comp_data.get('score')\n",
        "            all_records.append(flat)\n",
        "\n",
        "            # Progressive save every 5 tasks\n",
        "            if i % 5 == 0 or i == len(futures):\n",
        "                temp_df = pd.DataFrame(all_records)\n",
        "                for m, grp in temp_df.groupby('model_name'):\n",
        "                    fname = f\"{outdir}/{m}_evaluations.csv\"\n",
        "                    grp.to_csv(fname, index=False)\n",
        "                    print(f\"âœ… Intermediate save after {i} tasks for {m} â†’ {fname}\")\n",
        "\n",
        "    # Final AI evaluations DataFrame\n",
        "    GLOBAL_AI_EVAL_DF = pd.DataFrame(all_records)\n",
        "\n",
        "    # Distances & Kappa computations\n",
        "    distance_records = []\n",
        "    kappa_records = []\n",
        "    for _, row in GLOBAL_AI_EVAL_DF.iterrows():\n",
        "        h_match = GLOBAL_HUMAN_EVAL_DF[\n",
        "            (GLOBAL_HUMAN_EVAL_DF['base_id'] == row['base_id']) &\n",
        "            (GLOBAL_HUMAN_EVAL_DF['clip_number'] == row['clip_number'])\n",
        "        ]\n",
        "        if not h_match.empty:\n",
        "            domains, overall = compute_distances_for_item(h_match.iloc[0], row, framework)\n",
        "            rec = {\n",
        "                'base_id': row['base_id'],\n",
        "                'clip_number': row['clip_number'],\n",
        "                'model_name': row['model_name'],\n",
        "                **{f\"domain_{did}_dist\": d for did, d in domains.items()},\n",
        "                'overall_distance': overall\n",
        "            }\n",
        "            distance_records.append(rec)\n",
        "\n",
        "    GLOBAL_DISTANCES_DF = pd.DataFrame(distance_records).set_index(['base_id', 'clip_number', 'model_name'])\n",
        "    GLOBAL_DISTANCES_DF.to_csv(f\"{outdir}/distance_metrics.csv\")\n",
        "\n",
        "    for domain in framework['structure']['domains']:\n",
        "        for comp in domain['components']:\n",
        "            comp_id = str(comp['id'])\n",
        "            comp_name = comp['name']\n",
        "            score_list = comp.get('scoreList', ['Y', 'N', 'N/A'])\n",
        "            for model_name in GLOBAL_AI_EVAL_DF['model_name'].unique():\n",
        "                ai_ser = (\n",
        "                    GLOBAL_AI_EVAL_DF[GLOBAL_AI_EVAL_DF['model_name'] == model_name][comp_name]\n",
        "                    if comp_name in GLOBAL_AI_EVAL_DF else pd.Series()\n",
        "                )\n",
        "                human_ser = GLOBAL_HUMAN_EVAL_DF[comp_name]\n",
        "                if len(human_ser) > 1 and not ai_ser.empty:\n",
        "                    numeric_labels = all(str(lbl).isdigit() for lbl in score_list) and len(score_list) > 2\n",
        "                    weight = 'quadratic' if any(lbl in ['L', 'M', 'H'] for lbl in score_list) or numeric_labels else None\n",
        "                    try:\n",
        "                        val = compute_component_kappa(human_ser, ai_ser, score_list, weight)\n",
        "                    except Exception:\n",
        "                        val = np.nan\n",
        "                else:\n",
        "                    val = np.nan\n",
        "                kappa_records.append({\n",
        "                    'component_id': comp_id,\n",
        "                    'component_name': comp_name,\n",
        "                    'model_name': model_name,\n",
        "                    'kappa': val\n",
        "                })\n",
        "\n",
        "    GLOBAL_KAPPA_DF = pd.DataFrame(kappa_records)\n",
        "    GLOBAL_KAPPA_DF.to_csv(f\"{outdir}/kappa_results.csv\", index=False)\n",
        "    print(f\"âœ… Distance and kappa metrics saved under `{outdir}/`\")\n",
        "\n",
        "    # Visualize results by reading CSVs\n",
        "    visualize_results()\n",
        "\n",
        "# Call run_evaluation directly to see logs\n",
        "print(\"Starting evaluation...\")\n",
        "run_evaluation()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Harvard",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
