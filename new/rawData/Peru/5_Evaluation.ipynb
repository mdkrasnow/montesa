{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8357fab8",
      "metadata": {},
      "source": [
        "# Validation Evaluation Pipeline Notebook\n",
        "\n",
        "This notebook implements the plan for building a validation and evaluation pipeline for comparing AI-based evaluators against human evaluations. It follows the detailed plan to load data, define distance metrics, compute inter-rater reliability, incorporate multiple AI evaluators, and produce comprehensive visualizations.\n",
        "\n",
        "**Notebook Structure**:\n",
        "1. Imports and Setup\n",
        "2. Load Framework and Human Evaluations\n",
        "3. Helper Functions: Mapping and Distance Computation\n",
        "4. Model Calling Helper\n",
        "5. Compute Inter-Rater Reliability (Cohen's Kappa)\n",
        "6. Manual Configuration: Models & Dataset Selection\n",
        "7. Prepare Tasks for Debugging (skip completed tasks)\n",
        "8. Evaluation Execution (progressive saving every 5 evaluations)\n",
        "9. Visualizations\n",
        "10. Save Intermediate Results\n",
        "\n",
        "*Note: This notebook assumes that the evaluation framework JSON (`Teach_1.json`) and the cleaned transcripts CSV (`peru_cleaned_transcripts.csv`) are available in the working directory. Intermediate results will be saved progressively to the `model_evaluation_data` directory.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd2da427",
      "metadata": {},
      "source": [
        "## Root Cause Analysis\n",
        "\n",
        "Using `ipywidgets` and registering `run_evaluation` via `run_button.on_click` executes the evaluation in an asynchronous callback context. In Jupyter, output from widget callbacks does not appear in the original code cell output area, causing debugging logs and `print` statements to be hidden. To fix this, we remove widgets entirely and invoke the evaluation functions directly within cells, ensuring all logs and prints are displayed inline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3342a0f6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully.\n"
          ]
        }
      ],
      "source": [
        "# 1. Imports and Setup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from tqdm.notebook import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import math\n",
        "import inspect\n",
        "from typing import Dict, Any, Optional, List, Tuple\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8e0674e4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Load Framework and Human Evaluations\n",
        "def load_human_evaluations(cleaned_csv_path: str, framework_json_path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load cleaned transcripts and human evaluation columns.\n",
        "    Returns a DataFrame with columns ['base_id','clip_number',<each human component name>].\n",
        "    \"\"\"\n",
        "    # Read the cleaned transcripts CSV\n",
        "    df = pd.read_csv(cleaned_csv_path, dtype=str)\n",
        "    \n",
        "    # The actual evaluation columns from the CSV\n",
        "    evaluation_columns = [\n",
        "        'Teacher provides learning activity - 1st Snapshot',\n",
        "        'Students are on task - 1st Snapshot',\n",
        "        'Teacher provides learning activity - 2nd Snapshot',\n",
        "        'Students are on task - 2nd Snapshot',\n",
        "        'Teacher provides learning activity - 3rd Snapshot',\n",
        "        'Students are on task - 3rd Snapshot',\n",
        "        'Supportive Learning Environment',\n",
        "        'The teacher treats all students respectfully',\n",
        "        'The teacher uses positive language',\n",
        "        'The teacher responds to students needs',\n",
        "        'The teacher does not exhibit gender bias',\n",
        "        'Positive Behavioral Expectations',\n",
        "        'The teacher sets clear behavioral expectations',\n",
        "        'The teacher acknowledges positive student behavior',\n",
        "        'The teacher redirects misbehavior',\n",
        "        'Lesson Facilitation',\n",
        "        'The teacher explicitly articulates learning objectives',\n",
        "        'The teacher\\'s explanation of content is clear',\n",
        "        'The teacher makes connections in the lesson',\n",
        "        'The teacher models by enacting or thinking aloud',\n",
        "        'Checks for understanding',\n",
        "        'The teacher uses questions',\n",
        "        'The teacher uses prompts',\n",
        "        'The teacher monitors most students',\n",
        "        'The teacher adjusts teaching to the level of students',\n",
        "        'Feedback',\n",
        "        'The teacher provides specific comments for misunderstandings',\n",
        "        'The teacher provides specific comments for successes',\n",
        "        'Critical Thinking',\n",
        "        'The teacher asks open-ended questions',\n",
        "        'The teacher provides thinking tasks',\n",
        "        'Students ask open-ended questions or perform thinking tasks',\n",
        "        'Autonomy',\n",
        "        'The teacher provides students with choices',\n",
        "        'The teacher provides students with opportunities to take meaningful roles',\n",
        "        'Students volunteer to participate in the classroom',\n",
        "        'Perseverance',\n",
        "        'The teacher acknowledges students\\' efforts',\n",
        "        'The teacher has a positive attitude towards students\\' challenges',\n",
        "        'The teacher encourages goal-setting',\n",
        "        'Social & Collaborative Skills',\n",
        "        'The teacher promotes students\\' collaboration',\n",
        "        'The teacher promotes students\\' interpersonal skills',\n",
        "    ]\n",
        "\n",
        "    # Extract base_id and clip_number from School_Clip\n",
        "    clip_info = df['School_Clip'].str.extract(r'(?P<base_id>\\d{6,7})\\s*Clip\\s*(?P<clip_num>[12])')\n",
        "    df['base_id'] = clip_info['base_id']\n",
        "    df['clip_number'] = clip_info['clip_num'].map({'1': 'first', '2': 'last'})\n",
        "\n",
        "    # Identify which evaluation columns actually exist\n",
        "    present = [col for col in evaluation_columns if col in df.columns]\n",
        "    missing = [col for col in evaluation_columns if col not in df.columns]\n",
        "    if missing:\n",
        "        print(f\"⚠️ Missing human evaluation columns: {missing}\")\n",
        "    \n",
        "    print(f\"✅ Found {len(present)} evaluation columns in the dataset\")\n",
        "\n",
        "    # Return only base_id, clip_number, and the human-scored columns\n",
        "    return df[['base_id', 'clip_number'] + present]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "da0b0a2f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Helper Functions: Mapping and Distance Computation\n",
        "def alpha_to_numeric(x: Any) -> Optional[float]:\n",
        "    \"\"\"\n",
        "    Convert string labels to numeric:\n",
        "    - 'Y'/'y'/'Yes'/'1' -> 1.0\n",
        "    - 'N'/'n'/'No'/'0'  -> 0.0\n",
        "    - 'N/A' or empty  -> np.nan\n",
        "    - 'L' -> 1.0, 'M' -> 2.0, 'H' -> 3.0\n",
        "    - Numeric strings convertible to float -> float(x)\n",
        "    - Otherwise -> np.nan\n",
        "    \"\"\"\n",
        "    if x is None:\n",
        "        return np.nan\n",
        "    x_str = str(x).strip()\n",
        "    if x_str in {\"Y\", \"y\", \"Yes\", \"1\"}:\n",
        "        return 1.0\n",
        "    if x_str in {\"N\", \"n\", \"No\", \"0\"}:\n",
        "        return 0.0\n",
        "    if x_str in {\"N/A\", \"\", \"NA\", \"na\", \"nan\"}:\n",
        "        return np.nan\n",
        "    if x_str in {\"L\", \"M\", \"H\"}:\n",
        "        return {\"L\": 1.0, \"M\": 2.0, \"H\": 3.0}[x_str]\n",
        "    # Try converting directly to float for numeric (1-5 scale)\n",
        "    try:\n",
        "        return float(x_str)\n",
        "    except ValueError:\n",
        "        return np.nan\n",
        "\n",
        "def component_distance(human_score: Any, ai_score: Any, score_type: str) -> float:\n",
        "    \"\"\"\n",
        "    Compute normalized distance between human_score and ai_score based on score_type.\n",
        "    score_type is one of: 'YN', 'LMH', 'NUM' (for 1-5), where 'N/A' handling is builtin.\n",
        "    Returns a float in [0,1].\n",
        "    \"\"\"\n",
        "    human_num = alpha_to_numeric(human_score)\n",
        "    ai_num = alpha_to_numeric(ai_score)\n",
        "    \n",
        "    if math.isnan(human_num) and math.isnan(ai_num):\n",
        "        return 0.0\n",
        "    if math.isnan(human_num) ^ math.isnan(ai_num):\n",
        "        return 1.0\n",
        "    if score_type == 'YN':\n",
        "        d_max = 1.0\n",
        "    elif score_type == 'LMH':\n",
        "        d_max = 2.0\n",
        "    elif score_type == 'NUM':\n",
        "        d_max = 4.0\n",
        "    else:\n",
        "        d_max = 1.0\n",
        "    raw_diff = abs(human_num - ai_num)\n",
        "    normalized_diff = raw_diff / d_max\n",
        "    return min(max(normalized_diff, 0.0), 1.0)\n",
        "\n",
        "def compute_distances_for_item(human_row: pd.Series,\n",
        "                               ai_row: pd.Series,\n",
        "                               framework: Dict[str, Any]) -> Tuple[Dict[str, float], float]:\n",
        "    domain_distances: Dict[str, float] = {}\n",
        "    domain_weights_sum = 0.0\n",
        "    overall_numerator = 0.0\n",
        "    for domain in framework['structure']['domains']:\n",
        "        domain_id = str(domain['id'])\n",
        "        domain_weight = float(domain.get('weight', 1.0))\n",
        "        domain_numerator = 0.0\n",
        "        domain_weight_sum = 0.0\n",
        "        for comp in domain['components']:\n",
        "            comp_name = comp['name']\n",
        "            comp_weight = float(comp.get('weight', 1.0))\n",
        "            score_list = comp.get('scoreList', [])\n",
        "            score_type = 'NUM'\n",
        "            if set(score_list) <= {\"Y\",\"N\",\"N/A\"}:\n",
        "                score_type = 'YN'\n",
        "            elif set(score_list) <= {\"L\",\"M\",\"H\",\"N/A\"}:\n",
        "                score_type = 'LMH'\n",
        "            human_score = human_row.get(comp_name, None)\n",
        "            ai_score = ai_row.get(comp_name, None)\n",
        "            d_c = component_distance(human_score, ai_score, score_type)\n",
        "            domain_numerator += comp_weight * d_c\n",
        "            domain_weight_sum += comp_weight\n",
        "        D_domain = (domain_numerator / domain_weight_sum) if domain_weight_sum > 0 else 0.0\n",
        "        domain_distances[domain_id] = D_domain\n",
        "        overall_numerator += domain_weight * D_domain\n",
        "        domain_weights_sum += domain_weight\n",
        "    overall_distance = (overall_numerator / domain_weights_sum) if domain_weights_sum > 0 else 0.0\n",
        "    return domain_distances, overall_distance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9cf8e1b0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Model Calling Helper\n",
        "def call_model(evaluator_cls: type,\n",
        "               framework: Dict[str, Any],\n",
        "               transcript_text: str,\n",
        "               audio_file_path: Optional[str] = None\n",
        "              ) -> Dict[str, Any]:\n",
        "    evaluator = evaluator_cls(framework)\n",
        "    try:\n",
        "        sig = inspect.signature(evaluator.evaluate)\n",
        "        if 'audio_file_path' in sig.parameters and audio_file_path is not None:\n",
        "            result = evaluator.evaluate(transcript_text, audio_file_path=audio_file_path)\n",
        "        elif 'audio_file' in sig.parameters and audio_file_path is not None:\n",
        "            result = evaluator.evaluate(transcript_text, audio_file=audio_file_path)\n",
        "        else:\n",
        "            result = evaluator.evaluate(transcript_text)\n",
        "    except Exception as e:\n",
        "        return {'domains': {}, 'summary': '', 'error': str(e)}\n",
        "    if isinstance(result, dict):\n",
        "        if 'success' in result:\n",
        "            if not result['success']:\n",
        "                return {'domains': {}, 'summary': '', 'error': result.get('error', 'Unknown error')}\n",
        "            eval_result = result.get('evaluation', {})\n",
        "        else:\n",
        "            eval_result = result\n",
        "    else:\n",
        "        return {'domains': {}, 'summary': '', 'error': 'Unexpected evaluator output type'}\n",
        "    if 'domains' not in eval_result:\n",
        "        eval_result['domains'] = {}\n",
        "    return eval_result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b649f55c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Compute Inter-Rater Reliability (Cohen's Kappa)\n",
        "def encode_for_kappa(series: pd.Series, score_list: List[Any]) -> pd.Series:\n",
        "    mapping: Dict[Any, int] = {}\n",
        "    for idx, label in enumerate(score_list):\n",
        "        mapping[label] = idx\n",
        "    return series.map(lambda x: mapping.get(x, mapping.get('N/A', len(score_list)-1)))\n",
        "\n",
        "def compute_component_kappa(human_series: pd.Series,\n",
        "                            ai_series: pd.Series,\n",
        "                            score_list: List[Any],\n",
        "                            weight: Optional[str] = None\n",
        "                           ) -> float:\n",
        "    human_encoded = encode_for_kappa(human_series, score_list)\n",
        "    ai_encoded = encode_for_kappa(ai_series, score_list)\n",
        "    return cohen_kappa_score(human_encoded, ai_encoded, weights=weight)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3beedd74",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configured models: ['BaseEvaluator-Validate']\n",
            "Configured split: Train/Validation (Test confirm: False)\n"
          ]
        }
      ],
      "source": [
        "# 6. Manual Configuration: Models & Dataset Selection\n",
        "import sys\n",
        "sys.path.append('/Users/mkrasnow/Desktop/montesa')\n",
        "\n",
        "from new.models.base.BaseModelEvaluator import BaseModelEvaluator\n",
        "# Add other evaluator imports as needed\n",
        "available_models = {\n",
        "    'BaseEvaluator-Validate': BaseModelEvaluator,\n",
        "    # 'ChainEvaluator': ChainModelEvaluator,\n",
        "    # 'CoTEvaluator': CoTModelEvaluator\n",
        "}\n",
        "\n",
        "# Manually configure which models to run\n",
        "MODEL_LIST = ['BaseEvaluator-Validate']  # e.g., ['BaseEvaluator', 'ChainEvaluator']\n",
        "\n",
        "# Manually configure dataset split: 'Train/Validation' or 'Test'\n",
        "SPLIT = 'Train/Validation'\n",
        "\n",
        "# If using Test split, set TEST_CONFIRM to True to allow evaluation\n",
        "TEST_CONFIRM = False\n",
        "\n",
        "print(f\"Configured models: {MODEL_LIST}\")\n",
        "print(f\"Configured split: {SPLIT} (Test confirm: {TEST_CONFIRM})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "22a4aa8b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔧 Prepared 118 new tasks for evaluation.\n",
            "  Task 1: base_id=1104488, clip=first, model=BaseEvaluator-Validate\n",
            "  Task 2: base_id=1104488, clip=last, model=BaseEvaluator-Validate\n",
            "  Task 3: base_id=256776, clip=first, model=BaseEvaluator-Validate\n",
            "  Task 4: base_id=259358, clip=first, model=BaseEvaluator-Validate\n",
            "  Task 5: base_id=259358, clip=last, model=BaseEvaluator-Validate\n"
          ]
        }
      ],
      "source": [
        "# 7. Prepare Tasks for Debugging (skip already completed)\n",
        "# Load framework and transcripts\n",
        "FRAMEWORK_PATH = '/Users/mkrasnow/Desktop/montesa/new/models/_context/Teach_1.json'\n",
        "TRANSCRIPTS_PATH = '/Users/mkrasnow/Desktop/montesa/new/formattedData/peru_cleaned_transcripts.csv'\n",
        "OUTDIR = 'model_evaluation_data'\n",
        "os.makedirs(OUTDIR, exist_ok=True)\n",
        "\n",
        "framework = json.load(open(FRAMEWORK_PATH))\n",
        "df_all = pd.read_csv(TRANSCRIPTS_PATH, dtype=str)\n",
        "df_all['base_id'] = df_all['School_Clip'].str.extract(r\"(\\d{6,7})\", expand=False)\n",
        "df_all['clip_number'] = (\n",
        "    df_all['School_Clip']\n",
        "    .str.extract(r'Clip\\s*([12])', expand=False)\n",
        "    .map({'1': 'first', '2': 'last'})\n",
        ")\n",
        "\n",
        "# Determine completed tasks by reading existing evaluation files\n",
        "completed = set()\n",
        "for model_name in MODEL_LIST:\n",
        "    eval_file = f\"{OUTDIR}/{model_name}_evaluations.csv\"\n",
        "    if os.path.exists(eval_file):\n",
        "        df_done = pd.read_csv(eval_file, dtype=str)\n",
        "        for _, r in df_done.iterrows():\n",
        "            completed.add((r.get('base_id'), r.get('clip_number'), model_name))\n",
        "\n",
        "# Filter based on manual split configuration\n",
        "if SPLIT == 'Test' and not TEST_CONFIRM:\n",
        "    print('⚠️ Cannot prepare tasks: Test set evaluation not confirmed.')\n",
        "    PREPARED_TASKS = []\n",
        "else:\n",
        "    if SPLIT == 'Test':\n",
        "        df_split = df_all[df_all['split'] == 'test'].copy()\n",
        "    else:\n",
        "        df_split = df_all[df_all['split'].isin(['train', 'val'])].copy()\n",
        "\n",
        "    # Build tasks list (row, model_name) skipping completed\n",
        "    PREPARED_TASKS = []\n",
        "    for model_name in MODEL_LIST:\n",
        "        for _, row in df_split.iterrows():\n",
        "            key = (row['base_id'], row['clip_number'], model_name)\n",
        "            if key not in completed:\n",
        "                PREPARED_TASKS.append((row, model_name))\n",
        "\n",
        "    print(f\"🔧 Prepared {len(PREPARED_TASKS)} new tasks for evaluation.\")\n",
        "    for i, (row, model_name) in enumerate(PREPARED_TASKS[:5], start=1):\n",
        "        print(f\"  Task {i}: base_id={row['base_id']}, clip={row['clip_number']}, model={model_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8329ca7e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting evaluation...\n",
            "✅ Found 43 evaluation columns in the dataset\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1749502359.314534 3866389 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "20f816dcf74e4f81b2e4701e0d53eef8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating models:   0%|          | 0/118 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Task 1/118 → School_Clip=1104488 Clip 2, base_id=1104488, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 2/118 → School_Clip=259358 Clip 1, base_id=259358, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Task 3/118 → School_Clip=1104488 Clip 1, base_id=1104488, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Task 4/118 → School_Clip=256776 Clip 1, base_id=256776, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Task 5/118 → School_Clip=259358 Clip 2, base_id=259358, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Intermediate save after 5 tasks for BaseEvaluator-Validate → model_evaluation_data/BaseEvaluator-Validate_evaluations.csv\n",
            "✅ Task 6/118 → School_Clip=492108 Clip 1, base_id=492108, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Task 7/118 → School_Clip=492108 Clip 2, base_id=492108, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 8/118 → School_Clip=358317 Clip 2, base_id=358317, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 9/118 → School_Clip=393322 Clip 2, base_id=393322, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 10/118 → School_Clip=361337 Clip 2, base_id=361337, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Intermediate save after 10 tasks for BaseEvaluator-Validate → model_evaluation_data/BaseEvaluator-Validate_evaluations.csv\n",
            "✅ Task 11/118 → School_Clip=361337 Clip 1, base_id=361337, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Task 12/118 → School_Clip=415992 Clip 1, base_id=415992, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Task 13/118 → School_Clip=237990 Clip 2, base_id=237990, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 14/118 → School_Clip=416594 Clip 2, base_id=416594, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 15/118 → School_Clip=237990 Clip 1, base_id=237990, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Intermediate save after 15 tasks for BaseEvaluator-Validate → model_evaluation_data/BaseEvaluator-Validate_evaluations.csv\n",
            "✅ Task 16/118 → School_Clip=238089 Clip 2, base_id=238089, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 17/118 → School_Clip=283416 Clip 1, base_id=283416, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Task 18/118 → School_Clip=283416 Clip 2, base_id=283416, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 19/118 → School_Clip=283598 Clip 1, base_id=283598, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Task 20/118 → School_Clip=283598 Clip 2, base_id=283598, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Intermediate save after 20 tasks for BaseEvaluator-Validate → model_evaluation_data/BaseEvaluator-Validate_evaluations.csv\n",
            "✅ Task 21/118 → School_Clip=411355 Clip 1, base_id=411355, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Task 22/118 → School_Clip=411355 Clip 2, base_id=411355, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 23/118 → School_Clip=226480 Clip 2, base_id=226480, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 24/118 → School_Clip=226480 Clip 1, base_id=226480, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Task 25/118 → School_Clip=306753 Clip 1, base_id=306753, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Intermediate save after 25 tasks for BaseEvaluator-Validate → model_evaluation_data/BaseEvaluator-Validate_evaluations.csv\n",
            "✅ Task 26/118 → School_Clip=306753 Clip 2, base_id=306753, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 27/118 → School_Clip=312793 Clip 1, base_id=312793, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Task 28/118 → School_Clip=312793 Clip 2, base_id=312793, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 29/118 → School_Clip=320978 Clip 1, base_id=320978, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Task 30/118 → School_Clip=321356 Clip 1, base_id=321356, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Intermediate save after 30 tasks for BaseEvaluator-Validate → model_evaluation_data/BaseEvaluator-Validate_evaluations.csv\n",
            "✅ Task 31/118 → School_Clip=723288 Clip 1, base_id=723288, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Task 32/118 → School_Clip=723288 Clip 2, base_id=723288, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 33/118 → School_Clip=1126747 Clip 1, base_id=1126747, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Task 34/118 → School_Clip=1422534 Clip 2, base_id=1422534, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 35/118 → School_Clip=1597251 Clip 1, base_id=1597251, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Intermediate save after 35 tasks for BaseEvaluator-Validate → model_evaluation_data/BaseEvaluator-Validate_evaluations.csv\n",
            "✅ Task 36/118 → School_Clip=1597251 Clip 2, base_id=1597251, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 37/118 → School_Clip=383760 Clip 1, base_id=383760, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Task 38/118 → School_Clip=383760 Clip 2, base_id=383760, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 39/118 → School_Clip=389817 Clip 2, base_id=389817, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 40/118 → School_Clip=389163 Clip 1, base_id=389163, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Intermediate save after 40 tasks for BaseEvaluator-Validate → model_evaluation_data/BaseEvaluator-Validate_evaluations.csv\n",
            "✅ Task 41/118 → School_Clip=444869 Clip 2, base_id=444869, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 42/118 → School_Clip=447268 Clip 1, base_id=447268, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Task 43/118 → School_Clip=447789 Clip 1, base_id=447789, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Task 44/118 → School_Clip=556704 Clip 1, base_id=556704, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Task 45/118 → School_Clip=556704 Clip 2, base_id=556704, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Intermediate save after 45 tasks for BaseEvaluator-Validate → model_evaluation_data/BaseEvaluator-Validate_evaluations.csv\n",
            "✅ Task 46/118 → School_Clip=588111 Clip 1, base_id=588111, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Task 47/118 → School_Clip=652768 Clip 2, base_id=652768, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 48/118 → School_Clip=588111 Clip 2, base_id=588111, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 49/118 → School_Clip=784199 Clip 1, base_id=784199, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Task 50/118 → School_Clip=1109818 Clip 1, base_id=1109818, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Intermediate save after 50 tasks for BaseEvaluator-Validate → model_evaluation_data/BaseEvaluator-Validate_evaluations.csv\n",
            "✅ Task 51/118 → School_Clip=1109818 Clip 2, base_id=1109818, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 52/118 → School_Clip=207258 Clip 1, base_id=207258, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Task 53/118 → School_Clip=408856 Clip 2, base_id=408856, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 54/118 → School_Clip=232512 Clip 1, base_id=232512, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Task 55/118 → School_Clip=233692 Clip 1, base_id=233692, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Intermediate save after 55 tasks for BaseEvaluator-Validate → model_evaluation_data/BaseEvaluator-Validate_evaluations.csv\n",
            "✅ Task 56/118 → School_Clip=410712 Clip 1, base_id=410712, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Task 57/118 → School_Clip=486621 Clip 2, base_id=486621, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 58/118 → School_Clip=410712 Clip 2, base_id=410712, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 59/118 → School_Clip=487124 Clip 1, base_id=487124, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Task 60/118 → School_Clip=587147 Clip 1, base_id=587147, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Intermediate save after 60 tasks for BaseEvaluator-Validate → model_evaluation_data/BaseEvaluator-Validate_evaluations.csv\n",
            "✅ Task 61/118 → School_Clip=688242 Clip 2, base_id=688242, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 62/118 → School_Clip=587147 Clip 2, base_id=587147, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 63/118 → School_Clip=818856 Clip 1, base_id=818856, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Task 64/118 → School_Clip=292714 Clip 1, base_id=292714, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Task 65/118 → School_Clip=292730 Clip 1, base_id=292730, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Intermediate save after 65 tasks for BaseEvaluator-Validate → model_evaluation_data/BaseEvaluator-Validate_evaluations.csv\n",
            "✅ Task 66/118 → School_Clip=292714 Clip 2, base_id=292714, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 67/118 → School_Clip=425983 Clip 2, base_id=425983, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 68/118 → School_Clip=515916 Clip 1, base_id=515916, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Task 69/118 → School_Clip=541433 Clip 1, base_id=541433, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Task 70/118 → School_Clip=596775 Clip 1, base_id=596775, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Intermediate save after 70 tasks for BaseEvaluator-Validate → model_evaluation_data/BaseEvaluator-Validate_evaluations.csv\n",
            "✅ Task 71/118 → School_Clip=624247 Clip 1, base_id=624247, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Task 72/118 → School_Clip=596775 Clip 2, base_id=596775, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 73/118 → School_Clip=1176106 Clip 2, base_id=1176106, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 74/118 → School_Clip=1176106 Clip 1, base_id=1176106, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Task 75/118 → School_Clip=1551092 Clip 2, base_id=1551092, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Intermediate save after 75 tasks for BaseEvaluator-Validate → model_evaluation_data/BaseEvaluator-Validate_evaluations.csv\n",
            "✅ Task 76/118 → School_Clip=1551092 Clip 1, base_id=1551092, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Task 77/118 → School_Clip=712950 Clip 1, base_id=712950, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Task 78/118 → School_Clip=379859 Clip 1, base_id=379859, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Task 79/118 → School_Clip=381855 Clip 1, base_id=381855, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Task 80/118 → School_Clip=382242 Clip 1, base_id=382242, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Intermediate save after 80 tasks for BaseEvaluator-Validate → model_evaluation_data/BaseEvaluator-Validate_evaluations.csv\n",
            "✅ Task 81/118 → School_Clip=382242 Clip 2, base_id=382242, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 82/118 → School_Clip=420703 Clip 2, base_id=420703, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 83/118 → School_Clip=428953 Clip 1, base_id=428953, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Task 84/118 → School_Clip=428953 Clip 2, base_id=428953, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 85/118 → School_Clip=808147 Clip 1, base_id=808147, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Intermediate save after 85 tasks for BaseEvaluator-Validate → model_evaluation_data/BaseEvaluator-Validate_evaluations.csv\n",
            "✅ Task 86/118 → School_Clip=829457 Clip 2, base_id=829457, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 87/118 → School_Clip=829457 Clip 1, base_id=829457, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Task 88/118 → School_Clip=808147 Clip 2, base_id=808147, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 89/118 → School_Clip=268367 Clip 2, base_id=268367, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 90/118 → School_Clip=268367 Clip 1, base_id=268367, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Intermediate save after 90 tasks for BaseEvaluator-Validate → model_evaluation_data/BaseEvaluator-Validate_evaluations.csv\n",
            "✅ Task 91/118 → School_Clip=268516 Clip 2, base_id=268516, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 92/118 → School_Clip=690867 Clip 1, base_id=690867, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Task 93/118 → School_Clip=269779 Clip 2, base_id=269779, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 94/118 → School_Clip=269779 Clip 1, base_id=269779, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Task 95/118 → School_Clip=269860 Clip 2, base_id=269860, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Intermediate save after 95 tasks for BaseEvaluator-Validate → model_evaluation_data/BaseEvaluator-Validate_evaluations.csv\n",
            "✅ Task 96/118 → School_Clip=269910 Clip 1, base_id=269910, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Task 97/118 → School_Clip=269910 Clip 2, base_id=269910, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 98/118 → School_Clip=351775 Clip 1, base_id=351775, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Task 99/118 → School_Clip=351775 Clip 2, base_id=351775, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 100/118 → School_Clip=357848 Clip 2, base_id=357848, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Intermediate save after 100 tasks for BaseEvaluator-Validate → model_evaluation_data/BaseEvaluator-Validate_evaluations.csv\n",
            "✅ Task 101/118 → School_Clip=746602 Clip 2, base_id=746602, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 102/118 → School_Clip=512160 Clip 2, base_id=512160, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 103/118 → School_Clip=838565 Clip 2, base_id=838565, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 104/118 → School_Clip=208751 Clip 2, base_id=208751, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 105/118 → School_Clip=1164664 Clip 2, base_id=1164664, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Intermediate save after 105 tasks for BaseEvaluator-Validate → model_evaluation_data/BaseEvaluator-Validate_evaluations.csv\n",
            "✅ Task 106/118 → School_Clip=1166503 Clip 1, base_id=1166503, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Task 107/118 → School_Clip=1164664 Clip 1, base_id=1164664, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Task 108/118 → School_Clip=758128 Clip 1, base_id=758128, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Task 109/118 → School_Clip=263673 Clip 2, base_id=263673, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 110/118 → School_Clip=263442 Clip 1, base_id=263442, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Intermediate save after 110 tasks for BaseEvaluator-Validate → model_evaluation_data/BaseEvaluator-Validate_evaluations.csv\n",
            "✅ Task 111/118 → School_Clip=758128 Clip 2, base_id=758128, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 112/118 → School_Clip=455840 Clip 2, base_id=455840, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 113/118 → School_Clip=515650 Clip 2, base_id=515650, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 114/118 → School_Clip=515650 Clip 1, base_id=515650, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Task 115/118 → School_Clip=860783 Clip 2, base_id=860783, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Intermediate save after 115 tasks for BaseEvaluator-Validate → model_evaluation_data/BaseEvaluator-Validate_evaluations.csv\n",
            "✅ Task 116/118 → School_Clip=661033 Clip 2, base_id=661033, clip=last, model=BaseEvaluator-Validate\n",
            "✅ Task 117/118 → School_Clip=661033 Clip 1, base_id=661033, clip=first, model=BaseEvaluator-Validate\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Invalid JSON in component evaluation: {\n",
            "  \"analysis\": {\n",
            "    \"L\": \"The teacher does assign roles to students, so this is not the correct an...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Task 118/118 → School_Clip=263772 Clip 1, base_id=263772, clip=first, model=BaseEvaluator-Validate\n",
            "✅ Intermediate save after 118 tasks for BaseEvaluator-Validate → model_evaluation_data/BaseEvaluator-Validate_evaluations.csv\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'The teacher sets behavioral expectations'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[0;32m/opt/anaconda3/envs/Harvard/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'The teacher sets behavioral expectations'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 122\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Call run_evaluation directly to see logs\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting evaluation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 122\u001b[0m run_evaluation()\n",
            "Cell \u001b[0;32mIn[8], line 97\u001b[0m, in \u001b[0;36mrun_evaluation\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m GLOBAL_AI_EVAL_DF[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique():\n\u001b[1;32m     96\u001b[0m     ai_ser \u001b[38;5;241m=\u001b[39m GLOBAL_AI_EVAL_DF[GLOBAL_AI_EVAL_DF[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m model_name][comp_name] \u001b[38;5;28;01mif\u001b[39;00m comp_name \u001b[38;5;129;01min\u001b[39;00m GLOBAL_AI_EVAL_DF \u001b[38;5;28;01melse\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mSeries()\n\u001b[0;32m---> 97\u001b[0m     human_ser \u001b[38;5;241m=\u001b[39m GLOBAL_HUMAN_EVAL_DF[comp_name]\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(human_ser) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ai_ser\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m     99\u001b[0m         weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquadratic\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(lbl \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m lbl \u001b[38;5;129;01min\u001b[39;00m score_list) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/Harvard/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
            "File \u001b[0;32m/opt/anaconda3/envs/Harvard/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
            "\u001b[0;31mKeyError\u001b[0m: 'The teacher sets behavioral expectations'"
          ]
        }
      ],
      "source": [
        "# 8. Evaluation Execution (progressive saving every 5)\n",
        "def run_evaluation():\n",
        "    \"\"\"\n",
        "    Run through the previously-prepared tasks in PREPARED_TASKS, saving after every 5.\n",
        "    \"\"\"\n",
        "    global GLOBAL_AI_EVAL_DF, GLOBAL_DISTANCES_DF, GLOBAL_KAPPA_DF, GLOBAL_HUMAN_EVAL_DF\n",
        "\n",
        "    tasks = PREPARED_TASKS\n",
        "    if not tasks:\n",
        "        print(\"❌ No tasks to run. Please prepare tasks first.\")\n",
        "        return\n",
        "\n",
        "    # Load human evaluations for distance and kappa\n",
        "    GLOBAL_HUMAN_EVAL_DF = load_human_evaluations(\n",
        "        TRANSCRIPTS_PATH,\n",
        "        FRAMEWORK_PATH\n",
        "    )\n",
        "\n",
        "    all_records = []\n",
        "    outdir = OUTDIR\n",
        "\n",
        "    # Parallel execution with progress\n",
        "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "        futures = {executor.submit(\n",
        "            call_model,\n",
        "            available_models[model_name],\n",
        "            framework,\n",
        "            (row.get('First Audio Transcript Text', '') if row['clip_number'] == 'first'\n",
        "             else row.get('Last Audio Transcript Text', '')),\n",
        "            (row.get('First Audio Clip', '') if row['clip_number'] == 'first'\n",
        "             else row.get('Last Audio Clip', '')) or None\n",
        "        ): (row, model_name) for row, model_name in PREPARED_TASKS}\n",
        "\n",
        "        for i, future in enumerate(tqdm(as_completed(futures), total=len(futures), desc=\"Evaluating models\"), start=1):\n",
        "            row, model_name = futures[future]\n",
        "            print(f\"✅ Task {i}/{len(futures)} → School_Clip={row['School_Clip']}, base_id={row['base_id']}, clip={row['clip_number']}, model={model_name}\")\n",
        "            try:\n",
        "                eval_output = future.result()\n",
        "            except Exception as e:\n",
        "                eval_output = {'domains': {}, 'summary': '', 'error': str(e)}\n",
        "\n",
        "            # Include School_Clip in the output\n",
        "            flat = {\n",
        "                'School_Clip': row['School_Clip'],\n",
        "                'base_id': row['base_id'],\n",
        "                'clip_number': row['clip_number'],\n",
        "                'model_name': model_name\n",
        "            }\n",
        "            for domain in eval_output.get('domains', {}).values():\n",
        "                for comp_id, comp_data in domain.get('components', {}).items():\n",
        "                    for d in framework['structure']['domains']:\n",
        "                        for c in d['components']:\n",
        "                            if str(c['id']) == str(comp_id):\n",
        "                                flat[c['name']] = comp_data.get('score')\n",
        "            all_records.append(flat)\n",
        "\n",
        "            # Progressive save every 5 tasks\n",
        "            if i % 5 == 0 or i == len(futures):\n",
        "                temp_df = pd.DataFrame(all_records)\n",
        "                for m, grp in temp_df.groupby('model_name'):\n",
        "                    fname = f\"{outdir}/{m}_evaluations.csv\"\n",
        "                    grp.to_csv(fname, index=False)\n",
        "                    print(f\"✅ Intermediate save after {i} tasks for {m} → {fname}\")\n",
        "\n",
        "    # Final AI evaluations DF\n",
        "    GLOBAL_AI_EVAL_DF = pd.DataFrame(all_records)\n",
        "\n",
        "    # Distances & Kappa computations\n",
        "    distance_records = []\n",
        "    kappa_records = []\n",
        "    for _, row in GLOBAL_AI_EVAL_DF.iterrows():\n",
        "        h_match = GLOBAL_HUMAN_EVAL_DF[\n",
        "            (GLOBAL_HUMAN_EVAL_DF['base_id'] == row['base_id']) &\n",
        "            (GLOBAL_HUMAN_EVAL_DF['clip_number'] == row['clip_number'])\n",
        "        ]\n",
        "        if not h_match.empty:\n",
        "            domains, overall = compute_distances_for_item(h_match.iloc[0], row, framework)\n",
        "            rec = {\n",
        "                'base_id': row['base_id'],\n",
        "                'clip_number': row['clip_number'],\n",
        "                'model_name': row['model_name'],\n",
        "                **{f\"domain_{did}_dist\": d for did, d in domains.items()},\n",
        "                'overall_distance': overall\n",
        "            }\n",
        "            distance_records.append(rec)\n",
        "\n",
        "    GLOBAL_DISTANCES_DF = pd.DataFrame(distance_records).set_index(['base_id', 'clip_number', 'model_name'])\n",
        "    GLOBAL_DISTANCES_DF.to_csv(f\"{outdir}/distance_metrics.csv\")\n",
        "\n",
        "    for domain in framework['structure']['domains']:\n",
        "        for comp in domain['components']:\n",
        "            comp_id = str(comp['id'])\n",
        "            comp_name = comp['name']\n",
        "            score_list = comp.get('scoreList', ['Y', 'N', 'N/A'])\n",
        "            for model_name in GLOBAL_AI_EVAL_DF['model_name'].unique():\n",
        "                ai_ser = GLOBAL_AI_EVAL_DF[GLOBAL_AI_EVAL_DF['model_name'] == model_name][comp_name] if comp_name in GLOBAL_AI_EVAL_DF else pd.Series()\n",
        "                human_ser = GLOBAL_HUMAN_EVAL_DF[comp_name]\n",
        "                if len(human_ser) > 1 and not ai_ser.empty:\n",
        "                    weight = 'quadratic' if any(lbl in ['L', 'M', 'H'] for lbl in score_list) else None\n",
        "                    try:\n",
        "                        val = compute_component_kappa(human_ser, ai_ser, score_list, weight)\n",
        "                    except Exception:\n",
        "                        val = np.nan\n",
        "                else:\n",
        "                    val = np.nan\n",
        "                kappa_records.append({\n",
        "                    'component_id': comp_id,\n",
        "                    'component_name': comp_name,\n",
        "                    'model_name': model_name,\n",
        "                    'kappa': val\n",
        "                })\n",
        "\n",
        "    GLOBAL_KAPPA_DF = pd.DataFrame(kappa_records)\n",
        "    GLOBAL_KAPPA_DF.to_csv(f\"{outdir}/kappa_results.csv\", index=False)\n",
        "    print(f\"✅ Distance and kappa metrics saved under `{outdir}/`\")\n",
        "\n",
        "    # Visualize results by reading CSVs\n",
        "    visualize_results()\n",
        "\n",
        "# Call run_evaluation directly to see logs\n",
        "print(\"Starting evaluation...\")\n",
        "run_evaluation()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc826ff4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 9. Visualizations\n",
        "import os\n",
        "\n",
        "def visualize_results(outdir='model_evaluation_data'):\n",
        "    # Load distance metrics CSV\n",
        "    distances_path = os.path.join(outdir, 'distance_metrics.csv')\n",
        "    if not os.path.exists(distances_path):\n",
        "        print(f\"❌ Distance metrics file not found at {distances_path}. Please run evaluation first.\")\n",
        "        return\n",
        "    distances_df = pd.read_csv(distances_path)\n",
        "    \n",
        "    # Load kappa results CSV\n",
        "    kappa_path = os.path.join(outdir, 'kappa_results.csv')\n",
        "    if not os.path.exists(kappa_path):\n",
        "        print(f\"❌ Kappa results file not found at {kappa_path}. Please run evaluation first.\")\n",
        "        return\n",
        "    kappa_df = pd.read_csv(kappa_path)\n",
        "\n",
        "    # Ensure required columns\n",
        "    if 'model_name' not in distances_df.columns or 'overall_distance' not in distances_df.columns:\n",
        "        print(\"❌ Required columns missing in distances_df.\")\n",
        "        return\n",
        "\n",
        "    names = distances_df['model_name'].unique().tolist()\n",
        "\n",
        "    # 1. Boxplot of overall distances per model\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    data = [\n",
        "        distances_df.loc[distances_df['model_name'] == name, 'overall_distance'].dropna()\n",
        "        for name in names\n",
        "    ]\n",
        "    plt.boxplot(data, labels=names)\n",
        "    plt.title('Overall Distance per Model')\n",
        "    plt.ylabel('Normalized Distance')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "    # 2. Histograms of overall distances\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for name in names:\n",
        "        subset = distances_df.loc[distances_df['model_name'] == name, 'overall_distance'].dropna()\n",
        "        plt.hist(subset, bins=20, alpha=0.5, label=name)\n",
        "    plt.legend()\n",
        "    plt.title('Histogram of Overall Distances')\n",
        "    plt.xlabel('Normalized Distance')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.show()\n",
        "\n",
        "    # 3. Bar chart of average kappa per model\n",
        "    if 'model_name' in kappa_df.columns and 'kappa' in kappa_df.columns:\n",
        "        summary = kappa_df.groupby('model_name')['kappa'].mean().reset_index()\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.bar(summary['model_name'], summary['kappa'])\n",
        "        plt.title(\"Average Cohen's Kappa per Model\")\n",
        "        plt.ylim(-1, 1)\n",
        "        plt.xlabel('Model Name')\n",
        "        plt.ylabel(\"Cohen's Kappa\")\n",
        "        plt.grid(True, axis='y', alpha=0.3)\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"❌ Required columns missing in kappa_df.\")\n",
        "\n",
        "    print(\"✅ Visualizations complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5259f9d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 10. Save Intermediate Results\n",
        "def save_results():\n",
        "    if 'GLOBAL_DISTANCES_DF' in globals() and 'GLOBAL_KAPPA_DF' in globals():\n",
        "        GLOBAL_DISTANCES_DF.reset_index().to_csv('evaluation_distances.csv', index=False)\n",
        "        GLOBAL_KAPPA_DF.to_csv('evaluation_kappa.csv', index=False)\n",
        "        print('Results saved to evaluation_distances.csv and evaluation_kappa.csv')\n",
        "    else:\n",
        "        print('No results to save. Run evaluation first.')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Harvard",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
