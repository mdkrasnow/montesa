{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c253059b",
      "metadata": {},
      "source": [
        "# Model Performance Reporting Notebook\n",
        "\n",
        "This notebook generates comprehensive evaluation reports for each AI-based evaluator model. It reads the `*_evaluations.csv` files produced by the `5_Evaluation.ipynb` pipeline, computes summary statistics, inter-rater reliability metrics (Cohen's Kappa), component-specific distance averages, and domain-specific distance averages, and writes a neatly formatted Markdown report for each model into the `model_reports/` directory.\n",
        "\n",
        "**Notebook Structure**:\n",
        "1. Imports and Setup\n",
        "2. Directory Configuration\n",
        "3. Helper Functions\n",
        "4. Load Framework & Human Evaluations\n",
        "5. Report Generation Function\n",
        "6. Execute Report Generation\n",
        "7. Reliability Exam Functions\n",
        "8. Reliability Exam Visualizations\n",
        "9. Execute Reliability Exams and Generate Visualizations\n",
        "\n",
        "_Note: Make sure you have run `5_Evaluation.ipynb` and that the `model_evaluation_data/` folder contains your `*_evaluations.csv` files, and that you have access to your human evaluation data (`peru_cleaned_transcripts.csv`) and framework JSON (`Teach_1.json`)._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ab0f84bf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Imports and Setup\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fee83c37",
      "metadata": {},
      "source": [
        "## 2. Directory Configuration\n",
        "\n",
        "Define paths for input evaluation CSVs and output reports, and ensure the output directory exists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "71d54bdd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input evaluations directory: model_evaluation_data\n",
            "Output reports directory:    model_reports\n",
            "Visualizations directory:    model_visualizations\n"
          ]
        }
      ],
      "source": [
        "# Directories\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "sys.path.append('/Users/mkrasnow/Desktop/montesa')\n",
        "\n",
        "EVALS_DIR = 'model_evaluation_data'\n",
        "REPORTS_DIR = 'model_reports'\n",
        "VIZ_DIR = 'model_visualizations'\n",
        "os.makedirs(REPORTS_DIR, exist_ok=True)\n",
        "os.makedirs(VIZ_DIR, exist_ok=True)\n",
        "print(f\"Input evaluations directory: {EVALS_DIR}\")\n",
        "print(f\"Output reports directory:    {REPORTS_DIR}\")\n",
        "print(f\"Visualizations directory:    {VIZ_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2193b00",
      "metadata": {},
      "source": [
        "## 3. Helper Functions\n",
        "\n",
        "Define utility functions for converting labels to numeric, computing normalized distances, and computing Cohen's Kappa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2f415140",
      "metadata": {},
      "outputs": [],
      "source": [
        "def alpha_to_numeric(x):\n",
        "    \"\"\"\n",
        "    Convert string labels to numeric values:\n",
        "      - 'Y'/'y'/'Yes'/'1' -> 1.0\n",
        "      - 'N'/'n'/'No'/'0'  -> 0.0\n",
        "      - 'N/A' or empty   -> np.nan\n",
        "      - 'L' -> 1.0, 'M' -> 2.0, 'H' -> 3.0\n",
        "      - Numeric strings convertible to float -> float(x)\n",
        "    \"\"\"\n",
        "    if x is None:\n",
        "        return np.nan\n",
        "    s = str(x).strip()\n",
        "    if s in {'Y','y','Yes','1'}:\n",
        "        return 1.0\n",
        "    if s in {'N','n','No','0'}:\n",
        "        return 0.0\n",
        "    if s in {'N/A','','NA','na','nan'}:\n",
        "        return np.nan\n",
        "    if s in {'L','M','H'}:\n",
        "        return {'L':1.0,'M':2.0,'H':3.0}[s]\n",
        "    try:\n",
        "        return float(s)\n",
        "    except ValueError:\n",
        "        return np.nan\n",
        "\n",
        "def component_distance(human_score, ai_score, score_type):\n",
        "    \"\"\"\n",
        "    Compute normalized distance between human and AI scores.\n",
        "    score_type: 'YN' (max 1), 'LMH' (max 2), 'NUM' (1-5 scale, max diff 4).\n",
        "    Returns float in [0,1].\n",
        "    \"\"\"\n",
        "    h = alpha_to_numeric(human_score)\n",
        "    a = alpha_to_numeric(ai_score)\n",
        "    if math.isnan(h) and math.isnan(a):\n",
        "        return 0.0\n",
        "    if math.isnan(h) ^ math.isnan(a):\n",
        "        return 1.0\n",
        "    if score_type == 'YN':\n",
        "        dmax = 1.0\n",
        "    elif score_type == 'LMH':\n",
        "        dmax = 2.0\n",
        "    else:\n",
        "        dmax = 4.0\n",
        "    return min(max(abs(h - a) / dmax, 0.0), 1.0)\n",
        "\n",
        "def encode_for_kappa(series, score_list):\n",
        "    \"\"\"Map categorical labels to integers for Cohen's Kappa.\"\"\"\n",
        "    mapping = {lbl: i for i, lbl in enumerate(score_list)}\n",
        "    return series.map(lambda x: mapping.get(x, mapping.get('N/A', len(score_list)-1)))\n",
        "\n",
        "def compute_component_kappa(human_series, ai_series, score_list, weight=None):\n",
        "    \"\"\"Compute Cohen's Kappa for a single component.\"\"\"\n",
        "    h_enc = encode_for_kappa(human_series, score_list)\n",
        "    a_enc = encode_for_kappa(ai_series, score_list)\n",
        "    return cohen_kappa_score(h_enc, a_enc, weights=weight)\n",
        "\n",
        "def compute_distances_for_item(human_row, ai_row, framework):\n",
        "    \"\"\"\n",
        "    Compute domain-level and overall normalized distances for one item.\n",
        "    Returns (domain_distances: dict(domain_id->float), overall_distance:float).\n",
        "    \"\"\"\n",
        "    domain_dist = {}\n",
        "    num = 0.0\n",
        "    wsum = 0.0\n",
        "    for domain in framework['structure']['domains']:\n",
        "        did = str(domain['id'])\n",
        "        dnum = 0.0\n",
        "        dwsum = 0.0\n",
        "        stype = 'NUM'\n",
        "        for comp in domain['components']:\n",
        "            cname = comp['name']\n",
        "            cweight = float(comp.get('weight',1.0))\n",
        "            sl = comp.get('scoreList', [])\n",
        "            if set(sl) <= {'Y','N','N/A'}:\n",
        "                stype = 'YN'\n",
        "            elif set(sl) <= {'L','M','H','N/A'}:\n",
        "                stype = 'LMH'\n",
        "            d = component_distance(human_row.get(cname), ai_row.get(cname), stype)\n",
        "            dnum += cweight * d\n",
        "            dwsum += cweight\n",
        "        Dd = (dnum / dwsum) if dwsum>0 else 0.0\n",
        "        domain_dist[did] = Dd\n",
        "        num += float(domain.get('weight',1.0)) * Dd\n",
        "        wsum += float(domain.get('weight',1.0))\n",
        "    overall = (num/wsum) if wsum>0 else 0.0\n",
        "    return domain_dist, overall"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63899a1e",
      "metadata": {},
      "source": [
        "## 4. Load Framework & Human Evaluations\n",
        "\n",
        "Functions to load the evaluation framework JSON and the cleaned transcripts with human evaluation scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c2a4d451",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def load_framework(path):\n",
        "    with open(path, 'r') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def load_human_evaluations(cleaned_csv_path, framework_json_path):\n",
        "    df = pd.read_csv(cleaned_csv_path, dtype=str)\n",
        "    # Extract base_id and clip_number\n",
        "    clip_info = df['School_Clip'].str.extract(r'(?P<base_id>\\d{6,7})\\s*Clip\\s*(?P<clip_num>[12])')\n",
        "    df['base_id'] = clip_info['base_id']\n",
        "    df['clip_number'] = clip_info['clip_num'].map({'1':'first','2':'last'})\n",
        "    # Determine evaluation columns from framework\n",
        "    framework = load_framework(framework_json_path)\n",
        "    eval_cols = []\n",
        "    for domain in framework['structure']['domains']:\n",
        "        for comp in domain['components']:\n",
        "            if comp['name'] in df.columns:\n",
        "                eval_cols.append(comp['name'])\n",
        "    return df[['base_id','clip_number'] + eval_cols]\n",
        "\n",
        "# Example paths (adjust as needed)\n",
        "FRAMEWORK_PATH   = '/Users/mkrasnow/Desktop/montesa/new/models/_context/Teach_1.json'\n",
        "TRANSCRIPTS_PATH = '/Users/mkrasnow/Desktop/montesa/new/formattedData/peru_cleaned_transcripts.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f048be1f",
      "metadata": {},
      "source": [
        "## 5. Report Generation Function\n",
        "\n",
        "This function iterates over each `*_evaluations.csv` file, computes all metrics, and writes a Markdown report for each model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8d0ec119",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_reports(framework_path, transcripts_path, evals_dir, reports_dir):\n",
        "    framework = load_framework(framework_path)\n",
        "    human_df  = load_human_evaluations(transcripts_path, framework_path)\n",
        "\n",
        "    # Gather component definitions\n",
        "    components = []\n",
        "    for domain in framework['structure']['domains']:\n",
        "        for comp in domain['components']:\n",
        "            components.append({\n",
        "                'id': str(comp['id']),\n",
        "                'name': comp['name'],\n",
        "                'score_list': comp.get('scoreList', ['Y','N','N/A'])\n",
        "            })\n",
        "\n",
        "    # Process each evaluation CSV\n",
        "    for eval_file in glob.glob(os.path.join(evals_dir, '*_evaluations.csv')):\n",
        "        model_name = os.path.basename(eval_file).replace('_evaluations.csv','')\n",
        "        ai_df = pd.read_csv(eval_file, dtype=str)\n",
        "\n",
        "        # Compute distances for each item and track agreement\n",
        "        dist_records = []\n",
        "        for _, row in ai_df.iterrows():\n",
        "            hr = human_df[(human_df.base_id==row.base_id)&(human_df.clip_number==row.clip_number)]\n",
        "            if hr.empty: continue\n",
        "            hr = hr.iloc[0]\n",
        "            dom_dist, overall = compute_distances_for_item(hr, row, framework)\n",
        "            rec = {'base_id':row.base_id,'clip_number':row.clip_number,'overall_distance':overall}\n",
        "            for did, dval in dom_dist.items(): rec[f'domain_{did}_dist'] = dval\n",
        "            dist_records.append(rec)\n",
        "        distances_df = pd.DataFrame(dist_records)\n",
        "        # Add LLM-human agreement column (accuracy) as 1 - distance\n",
        "        distances_df['agreement'] = 1 - distances_df['overall_distance']\n",
        "\n",
        "        # Summary statistics for distance and agreement\n",
        "        overall_stats = distances_df['overall_distance'].describe()\n",
        "        agreement_stats = distances_df['agreement'].describe()\n",
        "\n",
        "        # Component-wise Cohen's Kappa\n",
        "        kappa_records = []\n",
        "        for comp in components:\n",
        "            comp_name = comp['name']\n",
        "            # Skip if component not present in either AI or human evaluations\n",
        "            if comp_name not in ai_df.columns or comp_name not in human_df.columns:\n",
        "                continue\n",
        "            h_ser = human_df[comp_name]\n",
        "            a_ser = ai_df[comp_name]\n",
        "            weight = 'quadratic' if any(lbl in ['L','M','H'] for lbl in comp['score_list']) else None\n",
        "            try:\n",
        "                k = compute_component_kappa(h_ser, a_ser, comp['score_list'], weight)\n",
        "            except:\n",
        "                k = np.nan\n",
        "            kappa_records.append({'component_name': comp_name, 'kappa': k})\n",
        "        kappa_df = pd.DataFrame(kappa_records)\n",
        "\n",
        "        # Component-specific distance averages\n",
        "        comp_dist_avgs = []\n",
        "        for comp in components:\n",
        "            cname = comp['name']\n",
        "            # Skip if component not present in either AI or human evaluations\n",
        "            if cname not in ai_df.columns or cname not in human_df.columns:\n",
        "                continue\n",
        "            # Compute mean normalized diff per component\n",
        "            diffs = ai_df.apply(lambda r: component_distance(\n",
        "                human_df[(human_df.base_id == r.base_id) & (human_df.clip_number == r.clip_number)].iloc[0].get(cname),\n",
        "                r.get(cname),\n",
        "                'YN' if set(comp['score_list']) <= {'Y', 'N', 'N/A'} else ('LMH' if set(comp['score_list']) <= {'L', 'M', 'H', 'N/A'} else 'NUM')\n",
        "            ), axis=1)\n",
        "            comp_dist_avgs.append({'component_name': cname, 'avg_distance': diffs.mean()})\n",
        "        comp_dists_df = pd.DataFrame(comp_dist_avgs)\n",
        "\n",
        "        # Domain-specific distance averages\n",
        "        domain_avgs = []\n",
        "        for domain in framework['structure']['domains']:\n",
        "            did = str(domain['id'])\n",
        "            col = f'domain_{did}_dist'\n",
        "            if col in distances_df.columns:\n",
        "                domain_avgs.append({'domain_id':did,'domain_name':domain['name'],'avg_distance':distances_df[col].mean()})\n",
        "        domain_dists_df = pd.DataFrame(domain_avgs)\n",
        "\n",
        "        # Build Markdown report\n",
        "        lines = []\n",
        "        lines.append(f\"# Report for Model: **{model_name}**\\n\")\n",
        "        lines.append(\"## 1. Distance and LLM-Human Agreement Summary Statistics\\n\")\n",
        "        lines.append(\"### 1.1 Overall Distance\\n\")\n",
        "        lines.append(overall_stats.to_markdown() + \"\\n\")\n",
        "        lines.append(\"### 1.2 LLM-Human Agreement\\n\")\n",
        "        lines.append(agreement_stats.to_markdown() + \"\\n\")\n",
        "        lines.append(\"## 2. Inter-Rater Reliability (Cohen's Kappa)\\n\")\n",
        "        lines.append(kappa_df.to_markdown(index=False) + \"\\n\")\n",
        "        lines.append(\"## 3. Component-Specific Distance Averages\\n\")\n",
        "        lines.append(comp_dists_df.to_markdown(index=False) + \"\\n\")\n",
        "        lines.append(\"## 4. Domain-Specific Distance Averages\\n\")\n",
        "        lines.append(domain_dists_df.to_markdown(index=False) + \"\\n\")\n",
        "\n",
        "        report_md = \"\\n\".join(lines)\n",
        "        out_path = os.path.join(reports_dir, f\"{model_name}_report.md\")\n",
        "        with open(out_path, 'w') as f:\n",
        "            f.write(report_md)\n",
        "        print(f\"âœ… Written report for {model_name} â†’ {out_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d98cb0ce",
      "metadata": {},
      "source": [
        "## 6. Execute Report Generation\n",
        "\n",
        "Run the `generate_reports` function with your configured paths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b98f07d3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tabulate in /opt/anaconda3/envs/Harvard/lib/python3.12/site-packages (0.9.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install tabulate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f882253a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Written report for 4-BaseEvaluator-Validate-no-NA-data â†’ model_reports/4-BaseEvaluator-Validate-no-NA-data_report.md\n",
            "âœ… Written report for 5-BaseEvaluator-Validate-no-NA-data-g-25-flash â†’ model_reports/5-BaseEvaluator-Validate-no-NA-data-g-25-flash_report.md\n",
            "âœ… Written report for 1-BaseEvaluator â†’ model_reports/1-BaseEvaluator_report.md\n",
            "âœ… Written report for 2-BaseEvaluator-Validate â†’ model_reports/2-BaseEvaluator-Validate_report.md\n",
            "âœ… Written report for 3-BaseEvaluator-Validate-no-NA â†’ model_reports/3-BaseEvaluator-Validate-no-NA_report.md\n",
            "\n",
            "All model reports generated successfully.\n"
          ]
        }
      ],
      "source": [
        "generate_reports(\n",
        "    framework_path=FRAMEWORK_PATH,\n",
        "    transcripts_path=TRANSCRIPTS_PATH,\n",
        "    evals_dir=EVALS_DIR,\n",
        "    reports_dir=REPORTS_DIR,\n",
        ")\n",
        "\n",
        "print(\"\\nAll model reports generated successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "reliability_intro",
      "metadata": {},
      "source": [
        "## 7. Reliability Exam Functions\n",
        "\n",
        "Evaluate AI models against the Teach Reliability Exam criteria:\n",
        "- **Time on Learning**: exact agreement on 2 of 3 snapshots per segment.\n",
        "- **Quality Elements**: within 1 point of master codes on at least 8 of 9 high-inference elements per segment.\n",
        "\n",
        "Two evaluation versions are implemented:\n",
        "1. **Random Set Exam**: Two attempts with random sets of three segments.\n",
        "2. **Average Exam**: Compute exam metrics across all available segments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "reliability_funcs",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reliability Exam Functions\n",
        "# Reliability Exam Functions\n",
        "import random\n",
        "\n",
        "# Define columns for Time on Learning snapshots and Quality elements\n",
        "snapshot_teacher = [\n",
        "    'Teacher provides learning activity - 1st Snapshot',\n",
        "    'Teacher provides learning activity - 2nd Snapshot',\n",
        "    'Teacher provides learning activity - 3rd Snapshot'\n",
        "]\n",
        "snapshot_students = [\n",
        "    'Students are on task - 1st Snapshot',\n",
        "    'Students are on task - 2nd Snapshot',\n",
        "    'Students are on task - 3rd Snapshot'\n",
        "]\n",
        "quality_elements = [\n",
        "    'Supportive Learning Environment',\n",
        "    'Positive Behavioral Expectations',\n",
        "    'Lesson Facilitation',\n",
        "    'Checks for understanding',\n",
        "    'Feedback',\n",
        "    'Critical Thinking',\n",
        "    'Autonomy',\n",
        "    'Perseverance',\n",
        "    'Social & Collaborative Skills'\n",
        "]\n",
        "\n",
        "def evaluate_segment(ai_row, human_row):\n",
        "    \"\"\"\n",
        "    Returns True if the segment is reliable on at least\n",
        "    8 of the 10 elements (time snapshots count as 1 element).\n",
        "    \"\"\"\n",
        "    # 1. Time on Learning element (1 point)\n",
        "    time_agreements = []\n",
        "    for t_col, s_col in zip(snapshot_teacher, snapshot_students):\n",
        "        t_match = ai_row[t_col] == human_row[t_col]\n",
        "        s_match = ai_row[s_col] == human_row[s_col]\n",
        "        time_agreements.append(t_match and s_match)\n",
        "    time_ok = sum(time_agreements) >= 2\n",
        "    # 2. Quality elements (up to 9 points)\n",
        "    quality_agreements = []\n",
        "    for col in quality_elements:\n",
        "        h = alpha_to_numeric(human_row[col])\n",
        "        a = alpha_to_numeric(ai_row[col])\n",
        "        if math.isnan(h) or math.isnan(a):\n",
        "            quality_agreements.append(False)\n",
        "        else:\n",
        "            quality_agreements.append(abs(h - a) <= 1.0)\n",
        "    quality_count = sum(quality_agreements)\n",
        "    # 3. Total reliable elements\n",
        "    total_reliable = (1 if time_ok else 0) + quality_count\n",
        "    # Pass if at least 8 of 10\n",
        "    return total_reliable >= 8\n",
        "\n",
        "\n",
        "def run_random_exam(models, ai_df, human_df, attempts=2, set_size=3, seed=42):\n",
        "    random.seed(seed)\n",
        "    results = {}\n",
        "    for model in models:\n",
        "        df_m = ai_df[ai_df['model_name'] == model]\n",
        "        keys = df_m[['base_id','clip_number']].drop_duplicates().apply(tuple, axis=1).tolist()\n",
        "        certified = False\n",
        "        attempt_results = []\n",
        "        for att in range(1, attempts+1):\n",
        "            sample = random.sample(keys, k=set_size)\n",
        "            passed_all = True\n",
        "            for seg in sample:\n",
        "                row_ai = df_m[(df_m['base_id'] == seg[0]) & (df_m['clip_number'] == seg[1])].iloc[0]\n",
        "                row_h  = human_df[(human_df['base_id'] == seg[0])  & (human_df['clip_number'] == seg[1])].iloc[0]\n",
        "                seg_pass = evaluate_segment(row_ai, row_h)\n",
        "                if not seg_pass:\n",
        "                    passed_all = False\n",
        "                    break\n",
        "            attempt_results.append({'attempt': att, 'passed': passed_all, 'segments': sample})\n",
        "            if passed_all:\n",
        "                certified = True\n",
        "                break\n",
        "        results[model] = {'certified': certified, 'attempts': attempt_results}\n",
        "    return results\n",
        "\n",
        "\n",
        "def run_average_exam(models, ai_df, human_df):\n",
        "    \"\"\"\n",
        "    Computes the proportion of segments each model passes (>=8/10 elements).\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    for model in models:\n",
        "        df_m = ai_df[ai_df['model_name'] == model]\n",
        "        pass_list = []\n",
        "        for _, row_ai in df_m.iterrows():\n",
        "            row_h = human_df[\n",
        "                (human_df['base_id'] == row_ai['base_id']) &\n",
        "                (human_df['clip_number'] == row_ai['clip_number'])\n",
        "            ].iloc[0]\n",
        "            seg_pass = evaluate_segment(row_ai, row_h)\n",
        "            pass_list.append(seg_pass)\n",
        "        pass_rate = sum(pass_list) / len(pass_list) if pass_list else float('nan')\n",
        "        results[model] = {'pass_rate': pass_rate}\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "viz_section",
      "metadata": {},
      "source": [
        "## 8. Reliability Exam Visualizations\n",
        "\n",
        "Create comprehensive visualizations for reliability exam results that are robust to handle any number of models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "viz_functions",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_exam_performance_dashboard(avg_results, random_results, save_path=None):\n",
        "    \"\"\"\n",
        "    Create a dashboard showing overall segment pass rate (>=8/10 elements)\n",
        "    and randomâ€exam certification status.\n",
        "    \"\"\"\n",
        "    models = list(avg_results.keys())\n",
        "    n_models = len(models)\n",
        "\n",
        "    # Overall pass rates\n",
        "    pass_rates = [avg_results[model]['pass_rate'] * 100 for model in models]\n",
        "\n",
        "    # Setup figure\n",
        "    fig = plt.figure(figsize=(16, 8))\n",
        "    gs = fig.add_gridspec(1, 2, width_ratios=[3, 1], wspace=0.4)\n",
        "\n",
        "    # 1. Pass Rate Bar Chart\n",
        "    ax1 = fig.add_subplot(gs[0])\n",
        "    x = np.arange(n_models)\n",
        "    bars = ax1.bar(x, pass_rates)\n",
        "    # TEACH certification requires â‰¥80% element reliability per segment\n",
        "    ax1.axhline(80, color='red', linestyle='--', linewidth=2,\n",
        "                label='Certification Threshold (80%)')\n",
        "    ax1.set_xticks(x)\n",
        "    ax1.set_xticklabels(models, rotation=45, ha='right')\n",
        "    ax1.set_ylabel('Segment Pass Rate (%)')\n",
        "    ax1.set_title('Average Segment Pass Rate by Model')\n",
        "    ax1.set_ylim(0, 100)\n",
        "    ax1.legend()\n",
        "\n",
        "    # Label values on bars\n",
        "    for bar, rate in zip(bars, pass_rates):\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2, rate + 1,\n",
        "                 f'{rate:.1f}%', ha='center', va='bottom')\n",
        "\n",
        "    # 2. Certification Status Pie Chart\n",
        "    ax2 = fig.add_subplot(gs[1])\n",
        "    certified = [random_results[m]['certified'] for m in models]\n",
        "    counts = [sum(certified), n_models - sum(certified)]\n",
        "    ax2.pie(counts, labels=['Certified','Failed'], autopct='%d', startangle=90)\n",
        "    ax2.set_title('Random Exam Certification')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "def create_performance_progression_chart(avg_results, save_path=None):\n",
        "    \"\"\"\n",
        "    Create a line chart showing pass_rate progression across models.\n",
        "    \"\"\"\n",
        "    models = list(avg_results.keys())\n",
        "    pass_rates = [avg_results[m]['pass_rate'] * 100 for m in models]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    x = np.arange(len(models))\n",
        "    ax.plot(x, pass_rates, marker='o', linewidth=2)\n",
        "    # Adjust to 80% threshold\n",
        "    ax.axhline(80, color='red', linestyle='--', linewidth=1.5,\n",
        "               label='Certification Threshold (80%)')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(models, rotation=45, ha='right')\n",
        "    ax.set_ylim(0, 100)\n",
        "    ax.set_ylabel('Segment Pass Rate (%)')\n",
        "    ax.set_title('Model Pass Rate Progression')\n",
        "    ax.legend()\n",
        "\n",
        "    for i, rate in enumerate(pass_rates):\n",
        "        ax.annotate(f'{rate:.1f}%', (i, rate), textcoords='offset points',\n",
        "                    xytext=(0,8), ha='center')\n",
        "    plt.tight_layout()\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "\n",
        "def create_detailed_metrics_table(avg_results, random_results, save_path=None):\n",
        "    \"\"\"\n",
        "    Build a table of pass rates and certification status for each model.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    for model, stats in avg_results.items():\n",
        "        pr = stats['pass_rate'] * 100\n",
        "        cert = random_results[model]['certified']\n",
        "        data.append({\n",
        "            'Model': model,\n",
        "            'Pass Rate (%)': f\"{pr:.1f}\",\n",
        "            'Certified': 'Yes' if cert else 'No'\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Render as a matplotlib table\n",
        "    fig, ax = plt.subplots(figsize=(8,  len(df)*0.5 + 1))\n",
        "    ax.axis('tight')\n",
        "    ax.axis('off')\n",
        "    tbl = ax.table(cellText=df.values, colLabels=df.columns, cellLoc='center', loc='center')\n",
        "    tbl.auto_set_font_size(False)\n",
        "    tbl.set_fontsize(10)\n",
        "    tbl.scale(1, 1.5)\n",
        "\n",
        "    # Highlight certification status\n",
        "    for i, cert in enumerate(df['Certified'], start=1):\n",
        "        color = '#90EE90' if cert=='Yes' else '#FFB6C1'\n",
        "        tbl[(i, df.columns.get_loc('Certified'))].set_facecolor(color)\n",
        "\n",
        "    plt.title('Model Pass Rates & Certification', pad=20)\n",
        "    plt.tight_layout()\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    return fig, df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "exec_section",
      "metadata": {},
      "source": [
        "## 9. Execute Reliability Exams and Generate Visualizations\n",
        "\n",
        "Run the reliability exams and create comprehensive visualizations for all models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "reliability_exec",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ” Loading evaluation data for reliability exams...\n",
            "ðŸ“Š Found 5 models for evaluation: ['4-BaseEvaluator-Validate-no-NA-data', '5-BaseEvaluator-Validate-no-NA-data-g-25-flash', '1-BaseEvaluator', '2-BaseEvaluator-Validate', '3-BaseEvaluator-Validate-no-NA']\n",
            "\n",
            "ðŸŽ¯ Running Random Certification Exams...\n",
            "Random Exam Results:\n",
            "{'4-BaseEvaluator-Validate-no-NA-data': {'certified': True, 'attempts': [{'attempt': 1, 'passed': True, 'segments': [('268516', 'last'), ('283416', 'first'), ('259358', 'first')]}]}, '5-BaseEvaluator-Validate-no-NA-data-g-25-flash': {'certified': True, 'attempts': [{'attempt': 1, 'passed': True, 'segments': [('838565', 'last'), ('389163', 'first'), ('723288', 'last')]}]}, '1-BaseEvaluator': {'certified': False, 'attempts': [{'attempt': 1, 'passed': False, 'segments': [('320978', 'first'), ('283416', 'last'), ('269860', 'last')]}, {'attempt': 2, 'passed': False, 'segments': [('237990', 'first'), ('829457', 'first'), ('269860', 'last')]}]}, '2-BaseEvaluator-Validate': {'certified': False, 'attempts': [{'attempt': 1, 'passed': False, 'segments': [('515650', 'last'), ('541433', 'first'), ('415992', 'first')]}, {'attempt': 2, 'passed': False, 'segments': [('1551092', 'first'), ('408856', 'last'), ('259358', 'last')]}]}, '3-BaseEvaluator-Validate-no-NA': {'certified': False, 'attempts': [{'attempt': 1, 'passed': False, 'segments': [('256776', 'first'), ('415992', 'first'), ('321356', 'first')]}, {'attempt': 2, 'passed': False, 'segments': [('312793', 'last'), ('292714', 'last'), ('379859', 'first')]}]}}\n",
            "\n",
            "ðŸ“ˆ Running Average Performance Exams...\n",
            "Average Exam Results:\n",
            "{'4-BaseEvaluator-Validate-no-NA-data': {'pass_rate': 0.8348623853211009}, '5-BaseEvaluator-Validate-no-NA-data-g-25-flash': {'pass_rate': 0.8348623853211009}, '1-BaseEvaluator': {'pass_rate': 0.4152542372881356}, '2-BaseEvaluator-Validate': {'pass_rate': 0.6440677966101694}, '3-BaseEvaluator-Validate-no-NA': {'pass_rate': 0.8050847457627118}}\n",
            "\n",
            "ðŸŽ¨ Generating comprehensive visualizations...\n"
          ]
        }
      ],
      "source": [
        "# Execute Reliability Exams and Generate Visualizations\n",
        "print(\"ðŸ” Loading evaluation data for reliability exams...\")\n",
        "\n",
        "# Get all models dynamically\n",
        "models = [os.path.basename(f).replace('_evaluations.csv','') \n",
        "          for f in glob.glob(os.path.join(EVALS_DIR, '*_evaluations.csv'))]\n",
        "\n",
        "print(f\"ðŸ“Š Found {len(models)} models for evaluation: {models}\")\n",
        "\n",
        "# Load all AI evaluation data and add model names\n",
        "ai_dfs = []\n",
        "for f in glob.glob(os.path.join(EVALS_DIR, '*_evaluations.csv')):\n",
        "    model_name = os.path.basename(f).replace('_evaluations.csv','')\n",
        "    df = pd.read_csv(f, dtype=str)\n",
        "    df['model_name'] = model_name\n",
        "    ai_dfs.append(df)\n",
        "\n",
        "ai_df_all = pd.concat(ai_dfs, ignore_index=True)\n",
        "human_df_all = load_human_evaluations(TRANSCRIPTS_PATH, FRAMEWORK_PATH)\n",
        "\n",
        "print(\"\\nðŸŽ¯ Running Random Certification Exams...\")\n",
        "random_results = run_random_exam(models, ai_df_all, human_df_all)\n",
        "print(\"Random Exam Results:\")\n",
        "print(random_results)\n",
        "\n",
        "print(\"\\nðŸ“ˆ Running Average Performance Exams...\")\n",
        "avg_results = run_average_exam(models, ai_df_all, human_df_all)\n",
        "print(\"Average Exam Results:\")\n",
        "print(avg_results)\n",
        "\n",
        "print(\"\\nðŸŽ¨ Generating comprehensive visualizations...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "viz_generation",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ“Š Creating Performance Dashboard...\n"
          ]
        },
        {
          "data": {},
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ“ˆ Creating Performance Progression Chart...\n"
          ]
        },
        {
          "data": {},
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ“‹ Creating Detailed Metrics Table...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Metrics CSV saved to: model_visualizations/model_performance_metrics.csv\n",
            "\n",
            "ðŸŽ‰ All visualizations completed successfully!\n",
            "\n",
            "ðŸ“ Visualizations saved to: model_visualizations\n",
            "Files generated:\n",
            "  - detailed_metrics_table.png\n",
            "  - model_performance_metrics.csv\n",
            "  - model_progression_chart.png\n",
            "  - reliability_exam_dashboard.png\n",
            "  - reliability_exam_summary.txt\n"
          ]
        }
      ],
      "source": [
        "# Generate all visualizations\n",
        "\n",
        "# 1. Main Performance Dashboard\n",
        "print(\"\\nðŸ“Š Creating Performance Dashboard...\")\n",
        "dashboard_path = os.path.join(VIZ_DIR, 'reliability_exam_dashboard.png')\n",
        "fig_dashboard = create_exam_performance_dashboard(avg_results, random_results, dashboard_path)\n",
        "\n",
        "# 2. Performance Progression Chart\n",
        "print(\"\\nðŸ“ˆ Creating Performance Progression Chart...\")\n",
        "progression_path = os.path.join(VIZ_DIR, 'model_progression_chart.png')\n",
        "fig_progression = create_performance_progression_chart(avg_results, progression_path)\n",
        "\n",
        "# 3. Detailed Metrics Table\n",
        "print(\"\\nðŸ“‹ Creating Detailed Metrics Table...\")\n",
        "metrics_path = os.path.join(VIZ_DIR, 'detailed_metrics_table.png')\n",
        "fig_table, metrics_df = create_detailed_metrics_table(avg_results, random_results, metrics_path)\n",
        "\n",
        "# 4. Save metrics as CSV for reference\n",
        "csv_path = os.path.join(VIZ_DIR, 'model_performance_metrics.csv')\n",
        "metrics_df.to_csv(csv_path, index=False)\n",
        "print(f\"âœ… Metrics CSV saved to: {csv_path}\")\n",
        "\n",
        "print(\"\\nðŸŽ‰ All visualizations completed successfully!\")\n",
        "print(f\"\\nðŸ“ Visualizations saved to: {VIZ_DIR}\")\n",
        "print(\"Files generated:\")\n",
        "for file in os.listdir(VIZ_DIR):\n",
        "    print(f\"  - {file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary_section",
      "metadata": {},
      "source": [
        "## 10. Summary and Interpretation\n",
        "\n",
        "Generate a comprehensive summary of the reliability exam results and provide interpretation guidance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "summary_analysis",
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'time_pass_rate'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[12], line 113\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_models\u001b[39m\u001b[38;5;124m'\u001b[39m: n_models,\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcertified_models\u001b[39m\u001b[38;5;124m'\u001b[39m: certified_count,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_quality_model\u001b[39m\u001b[38;5;124m'\u001b[39m: models[best_quality_idx]\n\u001b[1;32m    110\u001b[0m     }\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Generate comprehensive summary\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m summary_stats \u001b[38;5;241m=\u001b[39m generate_comprehensive_summary(avg_results, random_results)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Save summary to file\u001b[39;00m\n\u001b[1;32m    116\u001b[0m summary_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(VIZ_DIR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreliability_exam_summary.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "Cell \u001b[0;32mIn[12], line 9\u001b[0m, in \u001b[0;36mgenerate_comprehensive_summary\u001b[0;34m(avg_results, random_results)\u001b[0m\n\u001b[1;32m      6\u001b[0m n_models \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(models)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Calculate summary statistics\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m time_rates \u001b[38;5;241m=\u001b[39m [avg_results[model][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_pass_rate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models]\n\u001b[1;32m     10\u001b[0m quality_rates \u001b[38;5;241m=\u001b[39m [avg_results[model][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquality_pass_rate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models]\n\u001b[1;32m     11\u001b[0m certified_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([random_results[model][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcertified\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models])\n",
            "\u001b[0;31mKeyError\u001b[0m: 'time_pass_rate'"
          ]
        }
      ],
      "source": [
        "def generate_comprehensive_summary(avg_results, random_results):\n",
        "    \"\"\"\n",
        "    Generate a comprehensive text summary of the reliability exam results.\n",
        "    \"\"\"\n",
        "    models = list(avg_results.keys())\n",
        "    n_models = len(models)\n",
        "    \n",
        "    # Calculate summary statistics\n",
        "    time_rates = [avg_results[model]['time_pass_rate'] * 100 for model in models]\n",
        "    quality_rates = [avg_results[model]['quality_pass_rate'] * 100 for model in models]\n",
        "    certified_count = sum([random_results[model]['certified'] for model in models])\n",
        "    \n",
        "    # Find best and worst performers\n",
        "    best_time_idx = np.argmax(time_rates)\n",
        "    worst_time_idx = np.argmin(time_rates)\n",
        "    best_quality_idx = np.argmax(quality_rates)\n",
        "    worst_quality_idx = np.argmin(quality_rates)\n",
        "    \n",
        "    print(\"\\nðŸ” RELIABILITY EXAM ANALYSIS SUMMARY\")\n",
        "    print(\"=====================================\")\n",
        "    \n",
        "    print(f\"\\nðŸ“Š MODELS EVALUATED: {n_models}\")\n",
        "    print(f\"Models: {', '.join(models)}\")\n",
        "    \n",
        "    print(f\"\\nðŸŽ¯ CERTIFICATION RESULTS:\")\n",
        "    print(f\"- Models Certified: {certified_count} out of {n_models} ({certified_count/n_models*100:.1f}%)\")\n",
        "    print(f\"- Models Failed: {n_models-certified_count} out of {n_models} ({(n_models-certified_count)/n_models*100:.1f}%)\")\n",
        "    \n",
        "    print(f\"\\nðŸ“ˆ PERFORMANCE BENCHMARKS:\")\n",
        "    \n",
        "    print(f\"\\nTime on Learning Performance:\")\n",
        "    print(f\"- Best Model: {models[best_time_idx]} ({time_rates[best_time_idx]:.1f}%)\")\n",
        "    print(f\"- Worst Model: {models[worst_time_idx]} ({time_rates[worst_time_idx]:.1f}%)\")\n",
        "    print(f\"- Average: {np.mean(time_rates):.1f}%\")\n",
        "    print(f\"- Gap to Certification: {100 - np.mean(time_rates):.1f} percentage points\")\n",
        "    \n",
        "    print(f\"\\nQuality Elements Performance:\")\n",
        "    print(f\"- Best Model: {models[best_quality_idx]} ({quality_rates[best_quality_idx]:.1f}%)\")\n",
        "    print(f\"- Worst Model: {models[worst_quality_idx]} ({quality_rates[worst_quality_idx]:.1f}%)\")\n",
        "    print(f\"- Average: {np.mean(quality_rates):.1f}%\")\n",
        "    print(f\"- Gap to Certification: {100 - np.mean(quality_rates):.1f} percentage points\")\n",
        "    \n",
        "    # Model progression analysis\n",
        "    if len(models) > 1:\n",
        "        print(f\"\\nðŸ”„ MODEL IMPROVEMENTS:\")\n",
        "        for i in range(1, len(models)):\n",
        "            prev_model = models[i-1]\n",
        "            curr_model = models[i]\n",
        "            time_improvement = time_rates[i] - time_rates[i-1]\n",
        "            quality_improvement = quality_rates[i] - quality_rates[i-1]\n",
        "            time_rel_improvement = (time_improvement / time_rates[i-1]) * 100 if time_rates[i-1] > 0 else float('inf')\n",
        "            quality_rel_improvement = (quality_improvement / quality_rates[i-1]) * 100 if quality_rates[i-1] > 0 else float('inf')\n",
        "            \n",
        "            print(f\"{prev_model} â†’ {curr_model}:\")\n",
        "            print(f\"- Time on Learning: {time_improvement:+.1f} percentage points ({time_rel_improvement:+.1f}% relative improvement)\")\n",
        "            print(f\"- Quality Elements: {quality_improvement:+.1f} percentage points ({quality_rel_improvement:+.1f}% relative improvement)\")\n",
        "    \n",
        "    print(f\"\\nðŸ’¡ KEY INSIGHTS:\")\n",
        "    \n",
        "    insights = []\n",
        "    \n",
        "    if certified_count == 0:\n",
        "        insights.append(\"No models achieved certification - substantial improvements needed\")\n",
        "    elif certified_count < n_models:\n",
        "        insights.append(f\"Only {certified_count}/{n_models} models achieved certification\")\n",
        "    \n",
        "    avg_time = np.mean(time_rates)\n",
        "    avg_quality = np.mean(quality_rates)\n",
        "    \n",
        "    if avg_time < avg_quality:\n",
        "        insights.append(f\"Time on Learning is the primary bottleneck ({avg_time:.1f}% vs {avg_quality:.1f}% for Quality)\")\n",
        "    else:\n",
        "        insights.append(f\"Quality Elements is the primary bottleneck ({avg_quality:.1f}% vs {avg_time:.1f}% for Time)\")\n",
        "    \n",
        "    if len(models) > 1 and time_rates[-1] > time_rates[0] and quality_rates[-1] > quality_rates[0]:\n",
        "        insights.append(\"The latest version shows meaningful progress in both assessment areas\")\n",
        "    elif len(models) > 1 and (time_rates[-1] > time_rates[0] or quality_rates[-1] > quality_rates[0]):\n",
        "        if quality_rates[-1] > quality_rates[0]:\n",
        "            insights.append(\"The latest version shows meaningful progress, especially in Quality assessment\")\n",
        "        else:\n",
        "            insights.append(\"The latest version shows meaningful progress, especially in Time assessment\")\n",
        "    \n",
        "    if avg_quality > 50:\n",
        "        insights.append(\"Quality Elements show more promise for reaching certification standards\")\n",
        "    \n",
        "    if avg_time < 20:\n",
        "        insights.append(f\"Current models are in early development - roughly {100/avg_time:.0f}x improvement needed in Time assessment\")\n",
        "    \n",
        "    for i, insight in enumerate(insights, 1):\n",
        "        print(f\"{i}. {insight}\")\n",
        "    \n",
        "    print(f\"\\nðŸ“‹ RECOMMENDATIONS:\")\n",
        "    recommendations = [\n",
        "        \"Focus development efforts on Time on Learning snapshot assessment\" if avg_time < avg_quality else \"Focus development efforts on Quality Elements assessment\",\n",
        "        \"Continue refinements shown in latest model approach\" if len(models) > 1 else \"Develop systematic improvement approach\",\n",
        "        \"Consider specialized training for temporal classroom observation tasks\" if avg_time < 30 else \"Refine quality assessment calibration\",\n",
        "        \"Quality assessment framework shows promise - build on this success\" if avg_quality > 40 else \"Fundamental assessment approach needs revision\"\n",
        "    ]\n",
        "    \n",
        "    for i, rec in enumerate(recommendations, 1):\n",
        "        print(f\"{i}. {rec}\")\n",
        "    \n",
        "    return {\n",
        "        'total_models': n_models,\n",
        "        'certified_models': certified_count,\n",
        "        'avg_time_rate': avg_time,\n",
        "        'avg_quality_rate': avg_quality,\n",
        "        'best_time_model': models[best_time_idx],\n",
        "        'best_quality_model': models[best_quality_idx]\n",
        "    }\n",
        "\n",
        "# Generate comprehensive summary\n",
        "summary_stats = generate_comprehensive_summary(avg_results, random_results)\n",
        "\n",
        "# Save summary to file\n",
        "summary_path = os.path.join(VIZ_DIR, 'reliability_exam_summary.txt')\n",
        "with open(summary_path, 'w') as f:\n",
        "    # Capture the printed output (simplified version)\n",
        "    f.write(f\"Reliability Exam Summary\\n\")\n",
        "    f.write(f\"======================\\n\\n\")\n",
        "    f.write(f\"Models Evaluated: {summary_stats['total_models']}\\n\")\n",
        "    f.write(f\"Models Certified: {summary_stats['certified_models']}\\n\")\n",
        "    f.write(f\"Average Time Pass Rate: {summary_stats['avg_time_rate']:.1f}%\\n\")\n",
        "    f.write(f\"Average Quality Pass Rate: {summary_stats['avg_quality_rate']:.1f}%\\n\")\n",
        "    f.write(f\"Best Time Model: {summary_stats['best_time_model']}\\n\")\n",
        "    f.write(f\"Best Quality Model: {summary_stats['best_quality_model']}\\n\")\n",
        "\n",
        "print(f\"\\nReport generated successfully! âœ…\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Harvard",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
