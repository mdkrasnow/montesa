\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{titlesec}

% Fix headheight warning
\setlength{\headheight}{15pt}

\pagestyle{fancy}
\fancyhf{}
\rhead{Swiftscore Research}
\lhead{AI Classroom Evaluation Pipeline}
\cfoot{\thepage}

\title{\textbf{AI-Based Classroom Observation Evaluation: \\
A Multimodal Research Pipeline}}
\author{Matt Krasnow \\
\textit{Swiftscore $\mid$ Harvard} \\
\texttt{matt@swiftscore.org}}
\date{\today}

\begin{document}

\maketitle

\section{Abstract}

This document outlines a comprehensive validation and evaluation pipeline for comparing AI-based classroom observation evaluators against human expert evaluations. The pipeline incorporates multimodal AI models, defines robust distance metrics for mixed scoring systems, computes inter-rater reliability measures, and provides visualization tools for performance assessment.

\section{Pipeline Overview}

The evaluation pipeline consists of ten core components designed to systematically validate AI evaluators against human ground truth across multiple domains of classroom observation. The system supports both text-based and multimodal (audio + text) AI evaluators.

\subsection{Core Components}
\begin{enumerate}
    \item Data loading and preprocessing framework
    \item Distance metric computation for heterogeneous scoring systems
    \item Inter-rater reliability analysis (Cohen's $\kappa$)
    \item Multi-model evaluation orchestration
    \item Comprehensive visualization suite
    \item Test set evaluation controls with confirmation gates
\end{enumerate}

\section{Mathematical Framework}

\subsection{Score Normalization}

The pipeline handles three primary scoring types through the normalization function $\alpha: S \rightarrow \mathbb{R} \cup \{\text{NaN}\}$:

\begin{align}
\alpha(s) = \begin{cases}
1.0 & \text{if } s \in \{\text{Y, y, Yes, 1}\} \\
0.0 & \text{if } s \in \{\text{N, n, No, 0}\} \\
\text{NaN} & \text{if } s \in \{\text{N/A, NA, empty}\} \\
1.0, 2.0, 3.0 & \text{if } s \in \{\text{L, M, H}\} \text{ respectively} \\
\text{float}(s) & \text{if } s \text{ is numeric (1-5 scale)} \\
\text{NaN} & \text{otherwise}
\end{cases}
\end{align}

\subsection{Component Distance Metric}

For each component $c$, the distance between human score $h_c$ and AI score $a_c$ is computed as:

\begin{align}
d_c(h_c, a_c) = \begin{cases}
0.0 & \text{if } \alpha(h_c) = \alpha(a_c) = \text{NaN} \\
1.0 & \text{if } \alpha(h_c) = \text{NaN} \oplus \alpha(a_c) = \text{NaN} \\
\frac{|\alpha(h_c) - \alpha(a_c)|}{d_{\max}} & \text{otherwise}
\end{cases}
\end{align}

where $d_{\max}$ is the maximum possible difference for the scoring type:
\begin{align}
d_{\max} = \begin{cases}
1.0 & \text{for Y/N scoring} \\
2.0 & \text{for L/M/H scoring} \\
4.0 & \text{for 1-5 numeric scoring}
\end{cases}
\end{align}

\subsection{Domain and Overall Distance Aggregation}

For domain $D$ containing components $\{c_1, c_2, \ldots, c_n\}$ with weights $\{w_1, w_2, \ldots, w_n\}$:

\begin{align}
D_{\text{domain}} = \frac{\sum_{i=1}^{n} w_i \cdot d_{c_i}}{\sum_{i=1}^{n} w_i}
\end{align}

The overall distance across all domains $\{D_1, D_2, \ldots, D_m\}$ with domain weights $\{W_1, W_2, \ldots, W_m\}$:

\begin{align}
D_{\text{overall}} = \frac{\sum_{j=1}^{m} W_j \cdot D_j}{\sum_{j=1}^{m} W_j}
\end{align}

\subsection{Inter-Rater Reliability}

Cohen's Kappa coefficient is computed for each component using encoded categorical scores. For ordinal scales (L/M/H, 1-5), weighted kappa is employed:

\begin{align}
\kappa = \frac{p_o - p_e}{1 - p_e}
\end{align}

where $p_o$ is the observed agreement and $p_e$ is the expected agreement by chance. For weighted kappa with quadratic weights:

\begin{align}
\kappa_w = \frac{\sum_{i,j} w_{ij} p_{ij} - \sum_{i,j} w_{ij} p_{i \cdot} p_{\cdot j}}{1 - \sum_{i,j} w_{ij} p_{i \cdot} p_{\cdot j}}
\end{align}

where $w_{ij} = 1 - \frac{(i-j)^2}{(k-1)^2}$ for $k$ categories.

\section{Implementation Architecture}

\subsection{Data Pipeline}
The system loads evaluation data from cleaned transcript CSVs and framework JSON specifications. Base IDs are extracted using regex pattern matching on school clip identifiers: $\text{base\_id} \leftarrow \text{extract}(\text{School\_Clip}, \text{pattern} = \backslash d\{6,7\})$.

\subsection{Model Integration}
The pipeline supports multiple evaluator architectures:
\begin{itemize}
    \item \texttt{SimpleModelEvaluator}: Text-only evaluation
    \item \texttt{MultiModalModelEvaluator}: Audio + text evaluation
    \item \texttt{AdvancedModelEvaluatorX}: Extended feature evaluation
\end{itemize}

Each evaluator implements a standardized interface: \texttt{evaluate(transcript\_text, audio\_file\_path=None)} $\rightarrow$ evaluation dictionary.

\subsection{Evaluation Orchestration}
For each selected model $M$ and dataset split $S$:
\begin{enumerate}
    \item Load human evaluations $H = \{h_1, h_2, \ldots, h_n\}$
    \item Generate AI evaluations $A_M = \{a_1^M, a_2^M, \ldots, a_n^M\}$
    \item Compute distance matrix $D_M = [d_c(h_i, a_i^M)]_{i,c}$
    \item Calculate reliability metrics $\kappa_M = [\kappa_c^M]_c$
\end{enumerate}

\section{Visualization and Analysis}

The pipeline generates comprehensive performance visualizations:

\begin{itemize}
    \item \textbf{Distance Analysis}: Boxplots and histograms of component and overall distances
    \item \textbf{Reliability Assessment}: Bar charts of Cohen's $\kappa$ values per model and component
    \item \textbf{Performance Correlation}: Scatter plots of transcript length vs. evaluation distance
    \item \textbf{Model Comparison}: Comparative analysis across multiple AI evaluators
\end{itemize}

\section{Quality Assurance}

\subsection{Test Set Protection}
The pipeline implements mandatory confirmation controls for test set evaluation to prevent accidental model validation leakage. Test set access requires explicit user confirmation via checkbox validation.

\subsection{Error Handling}
Robust exception handling ensures graceful degradation when model calls fail, with standardized error reporting and result aggregation.

\section{Future Extensions}

\begin{itemize}
    \item \textbf{LLM-as-Judge Integration}: Third-party LLM comparison of human vs. AI evaluation summaries
    \item \textbf{Parallel Processing}: Multi-threaded model evaluation for large-scale deployment
    \item \textbf{Advanced Metrics}: Implementation of weighted F1-scores and custom domain-specific metrics
    \item \textbf{Real-time Evaluation}: Streaming evaluation pipeline for live classroom observation
\end{itemize}

\section{Conclusion}

This pipeline provides a systematic framework for validating AI-based classroom observation tools against expert human evaluation. The mathematical rigor of the distance metrics, combined with established inter-rater reliability measures, enables robust assessment of AI evaluator performance across diverse classroom contexts and observation frameworks.

\end{document}