{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Formatting\n",
    "This notebook loads the raw TEACH evaluation CSV files and standardizes them according to the column requirements described in the Montesa research plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14ca853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use paths relative to this notebook's directory\n",
    "raw_dir = Path('.')\n",
    "output_dir = Path('..') / 'formattedData'\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
  "def load_dataset(path):\n",
  "    with path.open('r', encoding='latin-1') as f:\n",
  "        lines = f.readlines()\n",
  "    header1 = [h.strip() for h in lines[0].split(',')]\n",
  "    header2 = [h.strip() for h in lines[1].split(',')]\n",
  "    base_cols = header1[:3] + header2[3:]\n",
  "    cols = []\n",
  "    counts = {}\n",
  "    for col in base_cols:\n",
  "        col = col or 'Unnamed'\n",
  "        if col in counts:\n",
  "            counts[col] += 1\n",
  "            cols.append(f'{col}_{counts[col]}')\n",
  "        else:\n",
  "            counts[col] = 0\n",
  "            cols.append(col)\n",
  "    df = pd.read_csv(path, header=None, skiprows=[0,2], names=cols, encoding='latin-1')\n",
  "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Duplicate names are not allowed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m peru \u001b[38;5;241m=\u001b[39m load_dataset(raw_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPeru\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTEACH_Final_Scores_4 - Peru.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m peru[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCountry of Origin\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPeru\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m rwanda \u001b[38;5;241m=\u001b[39m load_dataset(raw_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRwanda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTeach_Final_Scores_v1(ALL_Scores).csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 12\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     10\u001b[0m header2 \u001b[38;5;241m=\u001b[39m [h\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m lines[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m     11\u001b[0m cols \u001b[38;5;241m=\u001b[39m header1[:\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m+\u001b[39m header2[\u001b[38;5;241m3\u001b[39m:]\n\u001b[0;32m---> 12\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(path, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, skiprows\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m], names\u001b[38;5;241m=\u001b[39mcols)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Harvard/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Harvard/lib/python3.12/site-packages/pandas/io/parsers/readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    614\u001b[0m nrows \u001b[38;5;241m=\u001b[39m kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    616\u001b[0m \u001b[38;5;66;03m# Check for duplicates in names.\u001b[39;00m\n\u001b[0;32m--> 617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m    620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Harvard/lib/python3.12/site-packages/pandas/io/parsers/readers.py:576\u001b[0m, in \u001b[0;36m_validate_names\u001b[0;34m(names)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(names) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(names)):\n\u001b[0;32m--> 576\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDuplicate names are not allowed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[1;32m    578\u001b[0m         is_list_like(names, allow_sets\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(names, abc\u001b[38;5;241m.\u001b[39mKeysView)\n\u001b[1;32m    579\u001b[0m     ):\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNames should be an ordered collection.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Duplicate names are not allowed."
     ]
    }
   ],
   "source": [
    "peru = load_dataset(raw_dir / 'Peru' / 'TEACH_Final_Scores_4 - Peru.csv')\n",
    "peru['Country of Origin'] = 'Peru'\n",
    "\n",
    "rwanda = load_dataset(raw_dir / 'Rwanda' / 'Teach_Final_Scores_v1(ALL_Scores).csv')\n",
    "rwanda['Country of Origin'] = 'Rwanda'\n",
    "\n",
    "df = pd.concat([peru, rwanda], ignore_index=True)\n",
    "\n",
    "for col in ['Identifier', 'Audio File 1', 'Audio File 2', 'Transcription 1', 'Transcription 2', 'Language', 'Context']:\n",
    "    if col not in df.columns:\n",
    "        df[col] = ''\n",
    "df['Identifier'] = df.index.map(lambda i: f'observation_{i:05d}')\n",
    "\n",
    "df.to_csv(output_dir / 'montesa_formatted.csv', index=False)\n",
    "df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
