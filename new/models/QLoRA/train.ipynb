{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f1451a20",
      "metadata": {},
      "source": [
        "# Fine‚Äëtuning **Llama‚Äë4‚ÄëMaverick‚ÄØ17B‚Äë128E** with QLoRA on Google¬†Colab  \n",
        "\n",
        "This end‚Äëto‚Äëend notebook shows how to:\n",
        "1. Install **[Unsloth](https://github.com/unslothai/unsloth)** for fast 4‚Äëbit loading & LoRA training.\n",
        "2. Convert our **Teach** human‚Äëlabelled CSV¬†+¬†framework JSON into **ChatML¬†`.jsonl`**.\n",
        "3. Fine‚Äëtune Meta‚Äôs *Llama‚Äë4‚ÄëMaverick* with **Transformers¬†‚úï¬†PEFT¬†‚úï¬†bitsandbytes** (QLoRA under the hood).\n",
        "4. Push the LoRA adapter **and** a merged FP16 model to the Hugging¬†Face Hub.\n",
        "\n",
        "> **Colab¬†requirements** ¬∑ A100‚Äë40‚ÄØGB, L4‚Äë24‚ÄØGB or A10G‚Äë24‚ÄØGB tier (Pro/Pro+).  \n",
        "> The Maverick MoE model loads in ‚àº19‚ÄØGB VRAM at 4‚Äëbit, leaving room for LoRA params.\n",
        "\n",
        "References: Unsloth Llama‚Äë4 guide¬†:contentReference[oaicite:0]{index=0}, Colab install doc¬†:contentReference[oaicite:1]{index=1}."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d802547",
      "metadata": {
        "id": "env_check"
      },
      "outputs": [],
      "source": [
        "# ‚öôÔ∏è 0. Environment check\n",
        "import os, subprocess, json, torch, sys\n",
        "print(f\"Torch¬†version: {torch.__version__}\")\n",
        "!nvidia-smi -L\n",
        "!nvidia-smi --query-gpu=name,memory.total --format=csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0cf75b0",
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "# ‚öôÔ∏è 1. Install libraries (takes 2‚Äì3¬†min)\n",
        "# NOTE: The unsloth[colab] meta‚Äëpackage chooses the right CUDA/torch wheels automatically.\n",
        "!pip install --quiet --upgrade \"unsloth[colab]\" \\\n",
        "                                 transformers==4.41.2 \\\n",
        "                                 peft==0.10.0 \\\n",
        "                                 bitsandbytes==0.43.1 \\\n",
        "                                 datasets==2.19.1 \\\n",
        "                                 trl==0.8.6 \\\n",
        "                                 accelerate==0.29.3 \\\n",
        "                                 huggingface_hub==0.23.3 \n",
        "\n",
        "import importlib, platform, torch; print(\"‚úîÔ∏è¬†Install complete¬†on\", platform.platform())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b24ed18",
      "metadata": {
        "id": "login_hub"
      },
      "outputs": [],
      "source": [
        "# üîë 2. Login to the Hugging¬†Face Hub (one‚Äëtime per session)\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3dfa619",
      "metadata": {},
      "source": [
        "## 3‚ÄÇData upload / mounting\n",
        "Place the following files in **Google¬†Drive** ‚Üí `/MyDrive/teach_data/`:\n",
        "```\n",
        "Teach_1.json                     # framework spec\n",
        "peru_cleaned_transcripts.csv     # human labels + transcripts\n",
        "```\n",
        "Alternatively, upload them directly to the Colab *Files* pane and adjust paths below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d92a011",
      "metadata": {
        "id": "mount_drive"
      },
      "outputs": [],
      "source": [
        "# ‚ñ∂Ô∏è¬†3a¬†(Optional)¬†Mount Google¬†Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77e9e7bc",
      "metadata": {
        "id": "paths_setup"
      },
      "outputs": [],
      "source": [
        "# üìÇ 3b¬†Define paths\n",
        "DATA_DIR = \"/content/drive/MyDrive/teach_data\"   # change if not using Drive\n",
        "CSV_PATH = f\"{DATA_DIR}/peru_cleaned_transcripts.csv\"\n",
        "FRAME_PATH = f\"{DATA_DIR}/Teach_1.json\"\n",
        "\n",
        "assert os.path.exists(CSV_PATH), \"‚ùå CSV not found ‚Äì check path!\"\n",
        "assert os.path.exists(FRAME_PATH), \"‚ùå Framework JSON not found!\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f374d10",
      "metadata": {},
      "source": [
        "## 4‚ÄÇMaterialise ChatML¬†`.jsonl`\n",
        "Every training example is a triple of *system‚ÄØ/‚ÄØuser‚ÄØ/‚ÄØassistant* messages.  \n",
        "We reuse the helper functions shipped in the evaluation repo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef33a42a",
      "metadata": {
        "id": "jsonl_build"
      },
      "outputs": [],
      "source": [
        "# üõ†Ô∏è 4. Build train/val¬†JSONL\n",
        "import pandas as pd, json, random, math, tqdm, numpy as np\n",
        "\n",
        "RNG = random.Random(42)\n",
        "TRAIN_OUT = \"train_chatml.jsonl\"\n",
        "VAL_OUT   = \"val_chatml.jsonl\"\n",
        "\n",
        "df = pd.read_csv(CSV_PATH, dtype=str)\n",
        "framework = json.load(open(FRAME_PATH))\n",
        "\n",
        "# Extract IDs\n",
        "clip_info = df['School_Clip'].str.extract(r'(?P<base_id>\\d{6,7})\\s*Clip\\s*(?P<clip>[12])')\n",
        "df['base_id'] = clip_info['base_id']\n",
        "df['clip_number'] = clip_info['clip'].map({'1':'first','2':'last'})\n",
        "\n",
        "# Helper: prompt builder mirroring evaluation pipeline\n",
        "def make_prompt(comp, transcript):\n",
        "    system = \"You are an expert Teach framework scorer. Reply ONLY with valid JSON: {\\\"score\\\":<label>,\\\"analysis\\\":<text>}\"\n",
        "    user = (\n",
        "        f\"### Component: {comp['name']}\\n\"\n",
        "        f\"### Allowed labels: {', '.join(map(str, comp.get('scoreList',['Y','N'])))}\\n\\n\"\n",
        "        f\"Transcript:\\n{transcript}\\n\"\n",
        "    )\n",
        "    return system, user\n",
        "\n",
        "rows_train, rows_val = [], []\n",
        "\n",
        "for _, row in tqdm.tqdm(df.iterrows(), total=len(df)):\n",
        "    transcript = (\n",
        "        row.get('First Audio Transcript Text', '') if row['clip_number']=='first'\n",
        "        else row.get('Last Audio Transcript Text', '')\n",
        "    )\n",
        "    if not isinstance(transcript, str) or len(transcript.strip())==0:\n",
        "        continue  # skip blank transcripts\n",
        "    for domain in framework['structure']['domains']:\n",
        "        for comp in domain['components']:\n",
        "            cname = comp['name']\n",
        "            if cname not in row or pd.isna(row[cname]):\n",
        "                continue\n",
        "            label = str(row[cname]).strip()\n",
        "            sys_msg, user_msg = make_prompt(comp, transcript)\n",
        "            assistant_msg = json.dumps({\"score\": label, \"analysis\": \"\"}, ensure_ascii=False)\n",
        "            example = {\n",
        "                \"messages\": [\n",
        "                    {\"role\": \"system\", \"content\": sys_msg},\n",
        "                    {\"role\": \"user\", \"content\": user_msg},\n",
        "                    {\"role\": \"assistant\", \"content\": assistant_msg}\n",
        "                ]\n",
        "            }\n",
        "            # 80‚Äë20 split on the fly\n",
        "            (rows_val if RNG.random()<0.2 else rows_train).append(example)\n",
        "\n",
        "# Write files\n",
        "for path, rows in [(TRAIN_OUT, rows_train),(VAL_OUT,rows_val)]:\n",
        "    with open(path,'w',encoding='utf-8') as f:\n",
        "        for ex in rows:\n",
        "            f.write(json.dumps(ex, ensure_ascii=False)+'\\n')\n",
        "    print(f\"Wrote {len(rows):,} examples ‚Üí {path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "372baf00",
      "metadata": {},
      "source": [
        "## 5‚ÄÇTraining hyper‚Äëparameters\n",
        "* **r**¬†=¬†64, **Œ±**¬†=¬†16¬†‚Üí good trade‚Äëoff for 17‚ÄØB models.  \n",
        "* **batch_size** auto‚Äëscales to GPU memory via gradient accumulation.  \n",
        "* **packing**¬†=¬†`True` packs multiple small conversations into one window (Unsloth feature)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb38d789",
      "metadata": {
        "id": "train_setup"
      },
      "outputs": [],
      "source": [
        "# ‚öôÔ∏è 6. Load model (4‚Äëbit) & attach LoRA\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import BitsAndBytesConfig\n",
        "from peft import LoraConfig\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "\n",
        "MODEL_NAME = \"unsloth/Llama-4-Maverick-17B-128E-bnb-4bit\"  # 4‚Äëbit checkpoint\n",
        "MAX_LEN = 2048\n",
        "\n",
        "bnb_cfg = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=MODEL_NAME,\n",
        "    max_seq_length=MAX_LEN,\n",
        "    dtype=None,               # Autodetect bfloat16 support\n",
        "    load_in_4bit=True,\n",
        "    quantization_config=bnb_cfg,\n",
        ")\n",
        "\n",
        "peft_cfg = LoraConfig(\n",
        "    r=64, lora_alpha=16, lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"down_proj\",\"up_proj\"]\n",
        ")\n",
        "model = FastLanguageModel.get_peft_model(model, peft_cfg)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token  # safer packing\n",
        "print(f\"Model loaded ‚Äì trainable params: {model.num_parameters(only_trainable=True):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6552878",
      "metadata": {
        "id": "load_datasets"
      },
      "outputs": [],
      "source": [
        "# üìö 7. Load dataset into ü§ó¬†Datasets\n",
        "train_ds = load_dataset('json', data_files=TRAIN_OUT, split='train')\n",
        "val_ds   = load_dataset('json', data_files=VAL_OUT,   split='train')\n",
        "print(train_ds[0]['messages'][0].keys(), \"| total train ex:\", len(train_ds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6aafeac0",
      "metadata": {
        "id": "trainer_init"
      },
      "outputs": [],
      "source": [
        "# üöÄ 8. Configure SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model                 = model,\n",
        "    train_dataset         = train_ds,\n",
        "    eval_dataset          = val_ds,\n",
        "    dataset_text_field    = \"messages\",\n",
        "    max_seq_length        = MAX_LEN,\n",
        "    packing               = True,\n",
        "    gradient_checkpointing= True,\n",
        "    bf16                  = torch.cuda.is_bf16_supported(),\n",
        "    logging_steps         = 25,\n",
        "    eval_steps            = 200,\n",
        "    save_steps            = 200,\n",
        "    num_train_epochs      = 3,\n",
        "    learning_rate         = 2e-4,\n",
        "    per_device_train_batch_size = 1,   # fits 24‚ÄØGB with 4‚Äëbit\n",
        "    gradient_accumulation_steps = 8,   # effective batch 8\n",
        "    warmup_ratio          = 0.05,\n",
        ")\n",
        "print(\"Trainer initialised ‚Äì starting fine‚Äëtuning ‚Ä¶\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d126fce",
      "metadata": {
        "id": "train_run"
      },
      "outputs": [],
      "source": [
        "# üèãÔ∏è 9. Train (‚âà¬†1‚Äì3¬†hrs depending on GPU & dataset size)\n",
        "trainer.train()\n",
        "\n",
        "# Save adapter locally\n",
        "ADAPTER_DIR = \"lora-teach-llama4-maverick\"\n",
        "trainer.model.save_pretrained(ADAPTER_DIR)\n",
        "tokenizer.save_pretrained(ADAPTER_DIR)\n",
        "print(f\"Adapter saved to {ADAPTER_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ef768aa",
      "metadata": {
        "id": "push_hub"
      },
      "outputs": [],
      "source": [
        "# ‚òÅÔ∏è 10.¬†Push to the Hub (LoRA only)\n",
        "HF_REPO = \"mattkrasnow/llama4-maverick-teach-lora\"\n",
        "trainer.push_to_hub(HF_REPO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2de2302f",
      "metadata": {
        "id": "merge_full"
      },
      "outputs": [],
      "source": [
        "# ‚ûï 11. (Optional) Merge LoRA into FP16 base & push full model\n",
        "MERGED = \"llama4-maverick-teach-merged\"\n",
        "merged_model = model.merge_and_unload()\n",
        "merged_model.save_pretrained(MERGED, safetensors=True)\n",
        "tokenizer.save_pretrained(MERGED)\n",
        "\n",
        "from huggingface_hub import HfApi\n",
        "api = HfApi()\n",
        "api.create_repo(repo_id=HF_REPO+\"-merged\", exist_ok=True)\n",
        "api.upload_folder(folder_path=MERGED, repo_id=HF_REPO+\"-merged\")\n",
        "print(\"Merged model pushed ‚úîÔ∏è\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "410406f3",
      "metadata": {
        "id": "inference_demo"
      },
      "outputs": [],
      "source": [
        "# üî¨ 12. Quick inference sanity‚Äëcheck\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\",\n",
        "                model=ADAPTER_DIR,\n",
        "                tokenizer=ADAPTER_DIR,\n",
        "                device_map=\"auto\")\n",
        "prompt = (\n",
        "    \"### Component: Supportive Learning Environment\\n\"\n",
        "    \"### Allowed labels: Y, N, N/A\\n\\n\"\n",
        "    \"Transcript:\\nTeacher greets each student by name and checks how they feel about their homework‚Ä¶\"\n",
        ")\n",
        "gen = pipe(prompt, max_new_tokens=64, do_sample=False)[0]['generated_text']\n",
        "print(gen)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b846ac63",
      "metadata": {},
      "source": [
        "## 13‚ÄÇNext steps\n",
        "* Evaluate the fine‚Äëtuned model with your existing *`run_evaluation()`* pipeline.  \n",
        "* Perform hyper‚Äëparameter sweeps (rank, learning rate) to maximise Cohen‚Äôs‚ÄØŒ∫ and Teach reliability pass‚Äërate.  "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python¬†3 (Colab)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
