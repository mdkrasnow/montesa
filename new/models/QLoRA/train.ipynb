{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f1451a20",
      "metadata": {
        "id": "f1451a20"
      },
      "source": [
        "# Fine‚Äëtuning **Gemma 3 27B IT** with QLoRA on Google Colab A100  \n",
        "\n",
        "This end‚Äëto‚Äëend notebook shows how to:\n",
        "1. Install **[Unsloth](https://github.com/unslothai/unsloth)** for fast 4‚Äëbit loading & LoRA training.\n",
        "2. Convert our **Teach** human‚Äëlabelled CSV + framework JSON into **ChatML `.jsonl`**.\n",
        "3. Fine‚Äëtune with **Transformers ‚úï PEFT ‚úï bitsandbytes** (QLoRA under the hood).\n",
        "4. **Advanced Checkpointing**: Save every 50 steps, keep 5 checkpoints, auto-resume training.\n",
        "5. Push the LoRA adapter **and** a merged FP16 model to the Hugging Face Hub.\n",
        "\n",
        "> **Colab requirements** ¬∑ A100‚Äë40 GB tier (Pro+).  \n",
        "> The Gemma 3 27B model loads in ‚àº19 GB VRAM at 4‚Äëbit, leaving room for LoRA params.\n",
        "\n",
        "**Key Features:**\n",
        "- ‚úÖ **Built-in Checkpoint Support** with automatic resume\n",
        "- ‚úÖ **Connection Keepalive** to prevent disconnections\n",
        "- ‚úÖ **Optimized for A100** with BF16 precision\n",
        "- ‚úÖ **Memory Efficient** with gradient checkpointing\n",
        "\n",
        "References: Unsloth documentation, HuggingFace Transformers guide, Google Gemma 3 model card."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "connection_keepalive",
      "metadata": {
        "id": "connection_keepalive"
      },
      "outputs": [],
      "source": [
        "# üîå 0. Setup Connection Keepalive (Run this first!)\n",
        "from IPython.display import display, HTML, JavaScript\n",
        "\n",
        "# JavaScript to prevent Colab disconnection\n",
        "keepalive_js = \"\"\"\n",
        "function ClickConnect(){\n",
        "    console.log(\"Colab keepalive: Clicking connect button\"); \n",
        "    var buttons = document.querySelectorAll(\"colab-connect-button\");\n",
        "    if(buttons.length > 0){\n",
        "        buttons[0].shadowRoot.querySelector(\"#connect\").click();\n",
        "    }\n",
        "    // Also try alternative selectors\n",
        "    var altButton = document.querySelector('colab-connect-button');\n",
        "    if(altButton && altButton.shadowRoot){\n",
        "        var connectBtn = altButton.shadowRoot.querySelector('#connect');\n",
        "        if(connectBtn) connectBtn.click();\n",
        "    }\n",
        "}\n",
        "\n",
        "// Click every 60 seconds (60000 ms)\n",
        "var keepAliveInterval = setInterval(ClickConnect, 60000);\n",
        "console.log(\"Colab keepalive started - will click connect every 60 seconds\");\n",
        "\n",
        "// Function to stop keepalive\n",
        "function stopKeepAlive(){\n",
        "    clearInterval(keepAliveInterval);\n",
        "    console.log(\"Colab keepalive stopped\");\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "display(JavaScript(keepalive_js))\n",
        "print(\"‚úÖ Connection keepalive activated - your session will stay connected!\")\n",
        "print(\"üí° To stop keepalive later, run: stopKeepAlive() in browser console\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d802547",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d802547",
        "outputId": "e426cf61-4dd8-40cd-b1c7-89426a9256ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Torch version: 2.6.0+cu124\n",
            "GPU 0: NVIDIA A100-SXM4-40GB (UUID: GPU-192476bc-ac85-af36-d7bd-055f87277638)\n",
            "name, memory.total [MiB]\n",
            "NVIDIA A100-SXM4-40GB, 40960 MiB\n"
          ]
        }
      ],
      "source": [
        "# ‚öôÔ∏è 1. Environment check\n",
        "import os, subprocess, json, torch, sys\n",
        "print(f\"Torch version: {torch.__version__}\")\n",
        "!nvidia-smi -L\n",
        "!nvidia-smi --query-gpu=name,memory.total --format=csv\n",
        "\n",
        "# Verify A100 GPU\n",
        "gpu_info = !nvidia-smi --query-gpu=name --format=csv,noheader,nounits\n",
        "if 'A100' not in str(gpu_info):\n",
        "    print(\"‚ö†Ô∏è WARNING: A100 GPU not detected. This notebook is optimized for A100.\")\n",
        "    print(f\"Current GPU: {gpu_info}\")\n",
        "else:\n",
        "    print(\"‚úÖ A100 GPU detected - ready for Gemma 3 27B training!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0cf75b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0cf75b0",
        "outputId": "921ccdbe-2180-45c7-d6de-0c647bcfdc83"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "set -e  # stop on first failure\n",
        "\n",
        "# 0Ô∏è‚É£  Clean out leftovers that would confuse the resolver\n",
        "pip uninstall -y unsloth torch torchvision torchaudio fsspec protobuf gcsfs || true\n",
        "pip cache purge\n",
        "\n",
        "# 1Ô∏è‚É£  Core PyTorch stack for CUDA 12.4 (current Colab backend, 2025-06)\n",
        "pip install --upgrade pip\n",
        "pip install --extra-index-url https://download.pytorch.org/whl/cu124 \\\n",
        "           torch==2.6.0 torchvision==0.21.0+cu124 torchaudio==2.6.0+cu124 triton\n",
        "\n",
        "# 2Ô∏è‚É£  Unsloth compiled **for Torch 2.6 + CUDA 12.4**\n",
        "pip install \"unsloth[cu124-torch260] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "\n",
        "# 3Ô∏è‚É£  Your usual helpers, but tell pip **not to touch Torch again**\n",
        "pip install --no-deps peft transformers bitsandbytes datasets trl accelerate huggingface_hub\n",
        "\n",
        "# 4Ô∏è‚É£  Bring the remaining stragglers in line\n",
        "pip install fsspec==2025.3.2 protobuf==5.29.1 gcsfs==2025.3.2 tensorflow-metadata -U\n",
        "\n",
        "# 5Ô∏è‚É£  Install additional dependencies for advanced training\n",
        "pip install wandb psutil humanize GPUtil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b24ed18",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415,
          "referenced_widgets": []
        },
        "id": "7b24ed18",
        "outputId": "dee8b841-2c29-4b3a-c253-baf1fa0e89ab"
      },
      "outputs": [],
      "source": [
        "# üîë 2. Login to the Hugging Face Hub (one‚Äëtime per session)\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3dfa619",
      "metadata": {
        "id": "a3dfa619"
      },
      "source": [
        "## 3 Data upload / mounting\n",
        "Place the following files in **Google Drive** ‚Üí `/MyDrive/teach_data/`:\n",
        "```\n",
        "high_Teach_1.json                # framework spec\n",
        "peru_cleaned_transcripts.csv     # human labels + transcripts\n",
        "```\n",
        "Alternatively, upload them directly to the Colab *Files* pane and adjust paths below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d92a011",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "2d92a011",
        "outputId": "ebebce1a-8cba-4213-d7b9-951032a9e7a1"
      },
      "outputs": [],
      "source": [
        "# ‚ñ∂Ô∏è 3a (Optional) Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77e9e7bc",
      "metadata": {
        "id": "77e9e7bc"
      },
      "outputs": [],
      "source": [
        "# üìÇ 3b Define paths\n",
        "DATA_DIR = \"/content/drive/MyDrive/teach_data\"   # change if not using Drive\n",
        "CSV_PATH = f\"{DATA_DIR}/peru_cleaned_transcripts.csv\"\n",
        "FRAME_PATH = f\"{DATA_DIR}/high_Teach_1.json\"\n",
        "\n",
        "assert os.path.exists(CSV_PATH), \"‚ùå CSV not found ‚Äì check path!\"\n",
        "assert os.path.exists(FRAME_PATH), \"‚ùå Framework JSON not found!\"\n",
        "print(\"‚úÖ Data files found successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f374d10",
      "metadata": {
        "id": "9f374d10"
      },
      "source": [
        "## 4 Materialise ChatML `.jsonl`\n",
        "Every training example is a triple of *system / user / assistant* messages.  \n",
        "We reuse the helper functions shipped in the evaluation repo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef33a42a",
      "metadata": {
        "id": "ef33a42a"
      },
      "outputs": [],
      "source": [
        "# üõ†Ô∏è 4. Build train/val JSONL\n",
        "import pandas as pd\n",
        "import json\n",
        "import random\n",
        "import math\n",
        "import tqdm\n",
        "import numpy as np\n",
        "\n",
        "RNG = random.Random(42)\n",
        "TRAIN_OUT = \"train_chatml.jsonl\"\n",
        "VAL_OUT   = \"val_chatml.jsonl\"\n",
        "\n",
        "df = pd.read_csv(CSV_PATH, dtype=str)\n",
        "framework = json.load(open(FRAME_PATH))\n",
        "\n",
        "print(f\"üìä Loaded {len(df)} rows from CSV\")\n",
        "print(f\"üéØ Framework has {len(framework['structure']['domains'])} domains\")\n",
        "\n",
        "# Extract IDs\n",
        "clip_info = df['School_Clip'].str.extract(r'(?P<base_id>\\d{6,7})\\s*Clip\\s*(?P<clip>[12])')\n",
        "df['base_id'] = clip_info['base_id']\n",
        "df['clip_number'] = clip_info['clip'].map({'1':'first','2':'last'})\n",
        "\n",
        "# Helper: prompt builder mirroring evaluation pipeline\n",
        "def make_prompt(comp, transcript):\n",
        "    system = \"You are an expert Teach framework scorer. Reply ONLY with valid JSON: {\\\"score\\\":<label>,\\\"analysis\\\":<text>}\"\n",
        "    user = (\n",
        "        f\"### Component: {comp['name']}\\n\"\n",
        "        f\"### Allowed labels: {', '.join(map(str, comp.get('scoreList',['Y','N'])))}\\n\\n\"\n",
        "        f\"Transcript:\\n{transcript}\\n\"\n",
        "    )\n",
        "    return system, user\n",
        "\n",
        "rows_train, rows_val = [], []\n",
        "\n",
        "for _, row in tqdm.tqdm(df.iterrows(), total=len(df), desc=\"Processing examples\"):\n",
        "    transcript = (\n",
        "        row.get('First Audio Transcript Text', '') if row['clip_number']=='first'\n",
        "        else row.get('Last Audio Transcript Text', '')\n",
        "    )\n",
        "    if not isinstance(transcript, str) or len(transcript.strip())==0:\n",
        "        continue  # skip blank transcripts\n",
        "    for domain in framework['structure']['domains']:\n",
        "        for comp in domain['components']:\n",
        "            cname = comp['name']\n",
        "            if cname not in row or pd.isna(row[cname]):\n",
        "                continue\n",
        "            label = str(row[cname]).strip()\n",
        "            sys_msg, user_msg = make_prompt(comp, transcript)\n",
        "            assistant_msg = json.dumps({\"score\": label }, ensure_ascii=False)\n",
        "            example = {\n",
        "                \"messages\": [\n",
        "                    {\"role\": \"system\", \"content\": sys_msg},\n",
        "                    {\"role\": \"user\", \"content\": user_msg},\n",
        "                    {\"role\": \"assistant\", \"content\": assistant_msg}\n",
        "                ]\n",
        "            }\n",
        "            # 80‚Äë20 split on the fly\n",
        "            (rows_val if RNG.random()<0.2 else rows_train).append(example)\n",
        "\n",
        "# Write files\n",
        "for path, rows in [(TRAIN_OUT, rows_train),(VAL_OUT,rows_val)]:\n",
        "    with open(path,'w',encoding='utf-8') as f:\n",
        "        for ex in rows:\n",
        "            f.write(json.dumps(ex, ensure_ascii=False)+'\\n')\n",
        "    print(f\"‚úÖ Wrote {len(rows):,} examples ‚Üí {path}\")\n",
        "\n",
        "print(f\"\\nüìà Dataset summary:\")\n",
        "print(f\"  ‚Ä¢ Training examples: {len(rows_train):,}\")\n",
        "print(f\"  ‚Ä¢ Validation examples: {len(rows_val):,}\")\n",
        "print(f\"  ‚Ä¢ Total examples: {len(rows_train + rows_val):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "372baf00",
      "metadata": {
        "id": "372baf00"
      },
      "source": [
        "## 5 Model Loading with Advanced Configuration\n",
        "* **Gemma 3 27B IT** - Latest multimodal model from Google  \n",
        "* **BF16** precision for A100 optimal performance  \n",
        "* **4-bit quantization** for memory efficiency  \n",
        "* **LoRA r=64, Œ±=16** ‚Üí good trade‚Äëoff for 27B models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb38d789",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419,
          "referenced_widgets": []
        },
        "id": "fb38d789",
        "outputId": "7ae23936-3237-4837-d257-5fc45c984e9a"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "# üöÄ Model configuration - Gemma 3 27B IT\n",
        "MODEL_NAME = \"google/gemma-3-27b-it\"\n",
        "print(f\"üéØ Loading model: {MODEL_NAME}\")\n",
        "\n",
        "# BitsAndBytes configuration for 4-bit quantization\n",
        "bnb_cfg = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,  # BF16 for A100\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "print(\"üì• Loading model in 4-bit precision...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=bnb_cfg,\n",
        "    trust_remote_code=True,\n",
        "    attn_implementation=\"flash_attention_2\"  # Flash Attention 2 for efficiency\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# LoRA configuration for 27B model\n",
        "peft_cfg = LoraConfig(\n",
        "    r=64,                    # Higher rank for 27B model\n",
        "    lora_alpha=16,           # Conservative alpha\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"down_proj\",\"up_proj\"]\n",
        ")\n",
        "model = get_peft_model(model, peft_cfg)\n",
        "\n",
        "# Tokenizer setup\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Required for Gemma\n",
        "tokenizer.padding_side = \"right\"  # Recommended for training\n",
        "\n",
        "# Model statistics\n",
        "total_params = model.num_parameters()\n",
        "trainable_params = model.num_parameters(only_trainable=True)\n",
        "print(f\"\\nüìä Model Statistics:\")\n",
        "print(f\"  ‚Ä¢ Total parameters: {total_params:,}\")\n",
        "print(f\"  ‚Ä¢ Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"  ‚Ä¢ Trainable %: {100*trainable_params/total_params:.4f}%\")\n",
        "\n",
        "# Memory cleanup\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"‚úÖ Model loaded successfully with LoRA adapters!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6552878",
      "metadata": {
        "id": "c6552878"
      },
      "outputs": [],
      "source": [
        "# üìö 6. Load dataset into ü§ó Datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "train_ds = load_dataset('json', data_files=TRAIN_OUT, split='train')\n",
        "val_ds   = load_dataset('json', data_files=VAL_OUT,   split='val')\n",
        "\n",
        "print(f\"üìä Dataset loaded:\")\n",
        "print(f\"  ‚Ä¢ Training examples: {len(train_ds):,}\")\n",
        "print(f\"  ‚Ä¢ Validation examples: {len(val_ds):,}\")\n",
        "print(f\"  ‚Ä¢ Example message structure: {list(train_ds[0]['messages'][0].keys())}\")\n",
        "\n",
        "# Show a sample\n",
        "print(f\"\\nüìù Sample training example:\")\n",
        "sample = train_ds[0]['messages']\n",
        "print(f\"System: {sample[0]['content'][:100]}...\")\n",
        "print(f\"User: {sample[1]['content'][:100]}...\")\n",
        "print(f\"Assistant: {sample[2]['content']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "training_config",
      "metadata": {
        "id": "training_config"
      },
      "source": [
        "## 7 Advanced Training Configuration\n",
        "\n",
        "### üéØ Key Features:\n",
        "- **Advanced Checkpointing**: Save every 50 steps, keep 5 most recent\n",
        "- **Auto-Resume**: Automatically resume from last checkpoint\n",
        "- **Memory Optimization**: Gradient checkpointing + BF16\n",
        "- **A100 Optimized**: Perfect settings for 40GB A100 GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6aafeac0",
      "metadata": {
        "id": "6aafeac0"
      },
      "outputs": [],
      "source": [
        "# üöÄ 7. Configure Advanced SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "import os\n",
        "\n",
        "# Training configuration\n",
        "MAX_LEN = 2048  # Sequence length\n",
        "OUTPUT_DIR = \"./gemma3-27b-teach-checkpoints\"\n",
        "HF_REPO = \"your-username/gemma3-27b-teach-lora\"  # Change this!\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Advanced Training Arguments with Checkpointing\n",
        "training_args = TrainingArguments(\n",
        "    # === Output & Checkpointing ===\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    overwrite_output_dir=False,  # Don't overwrite existing checkpoints\n",
        "    \n",
        "    # === Advanced Checkpointing Configuration ===\n",
        "    save_strategy=\"steps\",           # Save based on steps, not epochs\n",
        "    save_steps=50,                   # Save every 50 steps (frequent for 6k examples)\n",
        "    save_total_limit=5,              # Keep only 5 most recent checkpoints\n",
        "    resume_from_checkpoint=True,     # Auto-resume from latest checkpoint\n",
        "    \n",
        "    # === Training Schedule ===\n",
        "    num_train_epochs=3,\n",
        "    max_steps=-1,                    # Use epochs instead of max_steps\n",
        "    \n",
        "    # === Learning Configuration ===\n",
        "    learning_rate=2e-4,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.05,\n",
        "    \n",
        "    # === Memory & Performance Optimization ===\n",
        "    per_device_train_batch_size=1,   # Conservative for 27B model\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=8,    # Effective batch size: 8\n",
        "    gradient_checkpointing=True,      # Essential for memory efficiency\n",
        "    dataloader_num_workers=2,         # Parallel data loading\n",
        "    dataloader_pin_memory=True,       # Speed up data transfer\n",
        "    \n",
        "    # === Precision & Hardware ===\n",
        "    bf16=True,                        # BF16 for A100 (better than FP16)\n",
        "    fp16=False,                       # Disable FP16 when using BF16\n",
        "    tf32=True,                        # Enable TF32 for A100\n",
        "    \n",
        "    # === Evaluation & Logging ===\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=100,                   # Evaluate every 100 steps\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=10,                 # Log every 10 steps\n",
        "    \n",
        "    # === Model Selection ===\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    \n",
        "    # === Regularization ===\n",
        "    weight_decay=0.01,\n",
        "    max_grad_norm=1.0,\n",
        "    \n",
        "    # === Hub Integration ===\n",
        "    push_to_hub=False,                # We'll push manually later\n",
        "    report_to=\"tensorboard\",          # Logging backend\n",
        "    \n",
        "    # === Stability ===\n",
        "    seed=42,\n",
        "    data_seed=42,\n",
        "    \n",
        "    # === Advanced Features ===\n",
        "    remove_unused_columns=False,      # Keep all columns for SFT\n",
        "    group_by_length=True,             # Group similar lengths for efficiency\n",
        "    ddp_find_unused_parameters=False, # DDP optimization\n",
        ")\n",
        "\n",
        "# SFT-specific configuration\n",
        "sft_config = SFTConfig(\n",
        "    max_seq_length=MAX_LEN,\n",
        "    packing=True,                     # Pack multiple conversations\n",
        "    dataset_text_field=\"messages\",   # Field containing the conversation\n",
        "    dataset_num_proc=2,              # Parallel dataset processing\n",
        ")\n",
        "\n",
        "print(f\"üéØ Training Configuration:\")\n",
        "print(f\"  ‚Ä¢ Model: {MODEL_NAME}\")\n",
        "print(f\"  ‚Ä¢ Output dir: {OUTPUT_DIR}\")\n",
        "print(f\"  ‚Ä¢ Max sequence length: {MAX_LEN}\")\n",
        "print(f\"  ‚Ä¢ Batch size (effective): {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"  ‚Ä¢ Learning rate: {training_args.learning_rate}\")\n",
        "print(f\"  ‚Ä¢ Epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"  ‚Ä¢ Save every: {training_args.save_steps} steps\")\n",
        "print(f\"  ‚Ä¢ Keep checkpoints: {training_args.save_total_limit}\")\n",
        "print(f\"  ‚Ä¢ Precision: {'BF16' if training_args.bf16 else 'FP16' if training_args.fp16 else 'FP32'}\")\n",
        "print(f\"  ‚Ä¢ Resume from checkpoint: {training_args.resume_from_checkpoint}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "init_trainer",
      "metadata": {
        "id": "init_trainer"
      },
      "outputs": [],
      "source": [
        "# üèãÔ∏è 8. Initialize SFTTrainer with Advanced Configuration\n",
        "print(\"üöÄ Initializing SFTTrainer with advanced checkpointing...\")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    processing_class=tokenizer,\n",
        "    \n",
        "    # SFT-specific parameters\n",
        "    max_seq_length=MAX_LEN,\n",
        "    packing=True,                     # Pack multiple conversations for efficiency\n",
        "    dataset_text_field=\"messages\",   # Field containing conversation data\n",
        "    dataset_num_proc=2,              # Parallel dataset processing\n",
        "    \n",
        "    # Memory optimizations\n",
        "    neftune_noise_alpha=None,        # Disable NEFTune for stability\n",
        ")\n",
        "\n",
        "# Check for existing checkpoints\n",
        "checkpoints = [d for d in os.listdir(OUTPUT_DIR) if d.startswith('checkpoint-')] if os.path.exists(OUTPUT_DIR) else []\n",
        "if checkpoints:\n",
        "    latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('-')[1]))\n",
        "    checkpoint_path = os.path.join(OUTPUT_DIR, latest_checkpoint)\n",
        "    print(f\"üìÇ Found existing checkpoints: {len(checkpoints)}\")\n",
        "    print(f\"üìç Latest checkpoint: {latest_checkpoint}\")\n",
        "    print(f\"üîÑ Training will resume from: {checkpoint_path}\")\n",
        "else:\n",
        "    print(f\"üÜï No existing checkpoints found - starting fresh training\")\n",
        "\n",
        "print(\"‚úÖ SFTTrainer initialized successfully!\")\n",
        "print(f\"\\nüìä Training Overview:\")\n",
        "print(f\"  ‚Ä¢ Total training examples: {len(train_ds):,}\")\n",
        "print(f\"  ‚Ä¢ Total validation examples: {len(val_ds):,}\")\n",
        "print(f\"  ‚Ä¢ Estimated training steps: {len(train_ds) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs:,}\")\n",
        "print(f\"  ‚Ä¢ Checkpoints will be saved every: {training_args.save_steps} steps\")\n",
        "print(f\"  ‚Ä¢ Evaluation every: {training_args.eval_steps} steps\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "training_section",
      "metadata": {
        "id": "training_section"
      },
      "source": [
        "## 8 Training with Advanced Monitoring\n",
        "\n",
        "### üöÄ Ready to train with:\n",
        "- ‚úÖ **Auto-resume** from checkpoints\n",
        "- ‚úÖ **Memory optimization** for 27B model\n",
        "- ‚úÖ **A100-optimized** settings\n",
        "- ‚úÖ **Connection keepalive** active\n",
        "\n",
        "**Training will automatically resume if interrupted!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d126fce",
      "metadata": {
        "id": "0d126fce"
      },
      "outputs": [],
      "source": [
        "# üèãÔ∏è 9. Start/Resume Training with Advanced Monitoring\n",
        "import time\n",
        "import psutil\n",
        "import GPUtil\n",
        "from datetime import datetime\n",
        "\n",
        "def print_system_stats():\n",
        "    \"\"\"Print current system and GPU statistics\"\"\"\n",
        "    # GPU stats\n",
        "    gpus = GPUtil.getGPUs()\n",
        "    if gpus:\n",
        "        gpu = gpus[0]\n",
        "        print(f\"üñ•Ô∏è  GPU: {gpu.memoryUsed:.0f}MB / {gpu.memoryTotal:.0f}MB ({gpu.memoryUtil*100:.1f}%) | Util: {gpu.load*100:.1f}%\")\n",
        "    \n",
        "    # RAM stats\n",
        "    ram = psutil.virtual_memory()\n",
        "    print(f\"üíæ RAM: {ram.used/1024**3:.1f}GB / {ram.total/1024**3:.1f}GB ({ram.percent:.1f}%)\")\n",
        "\n",
        "print(f\"üöÄ Starting training at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"üéØ Model: {MODEL_NAME}\")\n",
        "print(f\"üìä Dataset: {len(train_ds):,} train, {len(val_ds):,} val examples\")\n",
        "print(f\"‚öôÔ∏è  Configuration: BF16, LoRA r={peft_cfg.r}, Œ±={peft_cfg.lora_alpha}\")\n",
        "print_system_stats()\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üèÉ‚Äç‚ôÇÔ∏è TRAINING STARTED - Auto-resume enabled\")\n",
        "print(\"üîÑ Checkpoints saved every 50 steps to:\", OUTPUT_DIR)\n",
        "print(\"üíæ Only 5 most recent checkpoints will be kept\")\n",
        "print(\"üìä Tensorboard logs available in: runs/\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "try:\n",
        "    # Train with automatic resume\n",
        "    trainer.train(resume_from_checkpoint=True)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üéâ TRAINING COMPLETED SUCCESSFULLY!\")\n",
        "    print(f\"‚è∞ Finished at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print_system_stats()\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n‚ö†Ô∏è Training interrupted by user\")\n",
        "    print(\"üíæ Latest checkpoint saved - you can resume training later\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Training error: {e}\")\n",
        "    print(\"üíæ Check for saved checkpoints in:\", OUTPUT_DIR)\n",
        "    raise\n",
        "\n",
        "# Save final adapter\n",
        "ADAPTER_DIR = \"gemma3-27b-teach-lora-final\"\n",
        "print(f\"\\nüíæ Saving final LoRA adapter to: {ADAPTER_DIR}\")\n",
        "trainer.model.save_pretrained(ADAPTER_DIR)\n",
        "tokenizer.save_pretrained(ADAPTER_DIR)\n",
        "print(f\"‚úÖ Final adapter saved successfully!\")\n",
        "\n",
        "# Memory cleanup\n",
        "del trainer\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "print(\"üßπ Memory cleanup completed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hub_upload",
      "metadata": {
        "id": "hub_upload"
      },
      "source": [
        "## 9 Model Upload & Deployment\n",
        "\n",
        "Upload both the LoRA adapter and merged model to Hugging Face Hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ef768aa",
      "metadata": {
        "id": "8ef768aa"
      },
      "outputs": [],
      "source": [
        "# ‚òÅÔ∏è 10a. Push LoRA Adapter to Hub\n",
        "from huggingface_hub import HfApi, create_repo\n",
        "\n",
        "# Update this with your username!\n",
        "HF_USERNAME = \"your-username\"  # CHANGE THIS!\n",
        "LORA_REPO = f\"{HF_USERNAME}/gemma3-27b-teach-lora\"\n",
        "\n",
        "print(f\"‚òÅÔ∏è Uploading LoRA adapter to: {LORA_REPO}\")\n",
        "\n",
        "try:\n",
        "    # Create repository\n",
        "    create_repo(repo_id=LORA_REPO, exist_ok=True, repo_type=\"model\")\n",
        "    \n",
        "    # Upload adapter\n",
        "    api = HfApi()\n",
        "    api.upload_folder(\n",
        "        folder_path=ADAPTER_DIR,\n",
        "        repo_id=LORA_REPO,\n",
        "        commit_message=\"Add Gemma 3 27B LoRA adapter for Teach framework\"\n",
        "    )\n",
        "    \n",
        "    print(f\"‚úÖ LoRA adapter uploaded successfully!\")\n",
        "    print(f\"üîó Model available at: https://huggingface.co/{LORA_REPO}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Upload failed: {e}\")\n",
        "    print(f\"üí° Make sure to update HF_USERNAME and check your permissions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2de2302f",
      "metadata": {
        "id": "2de2302f"
      },
      "outputs": [],
      "source": [
        "# ‚ûï 10b. (Optional) Merge LoRA and Upload Full Model\n",
        "# WARNING: This creates a large model file (~50GB) - ensure you have space!\n",
        "\n",
        "MERGE_MODEL = input(\"ü§î Do you want to merge and upload the full model? (y/N): \").lower().strip()\n",
        "\n",
        "if MERGE_MODEL in ['y', 'yes']:\n",
        "    print(\"üîÑ Loading and merging LoRA with base model...\")\n",
        "    print(\"‚ö†Ô∏è This will take several minutes and use significant memory\")\n",
        "    \n",
        "    try:\n",
        "        # Load the trained model\n",
        "        from peft import PeftModel\n",
        "        \n",
        "        # Load base model in full precision for merging\n",
        "        base_model = AutoModelForCausalLM.from_pretrained(\n",
        "            MODEL_NAME,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.bfloat16\n",
        "        )\n",
        "        \n",
        "        # Load and merge LoRA\n",
        "        peft_model = PeftModel.from_pretrained(base_model, ADAPTER_DIR)\n",
        "        merged_model = peft_model.merge_and_unload()\n",
        "        \n",
        "        # Save merged model\n",
        "        MERGED_DIR = \"gemma3-27b-teach-merged\"\n",
        "        print(f\"üíæ Saving merged model to: {MERGED_DIR}\")\n",
        "        merged_model.save_pretrained(MERGED_DIR, safetensors=True)\n",
        "        tokenizer.save_pretrained(MERGED_DIR)\n",
        "        \n",
        "        # Upload merged model\n",
        "        MERGED_REPO = f\"{HF_USERNAME}/gemma3-27b-teach-merged\"\n",
        "        print(f\"‚òÅÔ∏è Uploading merged model to: {MERGED_REPO}\")\n",
        "        \n",
        "        create_repo(repo_id=MERGED_REPO, exist_ok=True, repo_type=\"model\")\n",
        "        api.upload_folder(\n",
        "            folder_path=MERGED_DIR,\n",
        "            repo_id=MERGED_REPO,\n",
        "            commit_message=\"Add merged Gemma 3 27B model fine-tuned for Teach framework\"\n",
        "        )\n",
        "        \n",
        "        print(f\"‚úÖ Merged model uploaded successfully!\")\n",
        "        print(f\"üîó Model available at: https://huggingface.co/{MERGED_REPO}\")\n",
        "        \n",
        "        # Cleanup\n",
        "        del base_model, peft_model, merged_model\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Merge/upload failed: {e}\")\n",
        "        print(f\"üí° You can still use the LoRA adapter with the base model\")\n",
        "        \n",
        "else:\n",
        "    print(\"‚è≠Ô∏è Skipping model merge - LoRA adapter is sufficient for most use cases\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "testing_section",
      "metadata": {
        "id": "testing_section"
      },
      "source": [
        "## 10 Model Testing & Inference\n",
        "\n",
        "Test the fine-tuned model with some sample inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "410406f3",
      "metadata": {
        "id": "410406f3"
      },
      "outputs": [],
      "source": [
        "# üî¨ 11. Quick Inference Test\n",
        "from transformers import pipeline\n",
        "from peft import PeftModel\n",
        "import json\n",
        "\n",
        "print(\"üß™ Testing the fine-tuned model...\")\n",
        "\n",
        "try:\n",
        "    # Load the fine-tuned model for inference\n",
        "    base_model_inference = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        load_in_4bit=True,\n",
        "        quantization_config=bnb_cfg\n",
        "    )\n",
        "    \n",
        "    # Load LoRA adapter\n",
        "    model_with_lora = PeftModel.from_pretrained(base_model_inference, ADAPTER_DIR)\n",
        "    \n",
        "    # Create pipeline\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model_with_lora,\n",
        "        tokenizer=tokenizer,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.bfloat16\n",
        "    )\n",
        "    \n",
        "    # Test prompts\n",
        "    test_cases = [\n",
        "        {\n",
        "            \"component\": \"Supportive Learning Environment\",\n",
        "            \"labels\": \"Y, N, N/A\",\n",
        "            \"transcript\": \"Teacher greets each student by name and asks how they feel about their homework. She provides encouraging feedback and helps struggling students.\"\n",
        "        },\n",
        "        {\n",
        "            \"component\": \"Clear Learning Objectives\",\n",
        "            \"labels\": \"Y, N, N/A\",\n",
        "            \"transcript\": \"Today we will learn about fractions. By the end of this lesson, you should be able to add and subtract fractions with the same denominator.\"\n",
        "        },\n",
        "        {\n",
        "            \"component\": \"Student Engagement\",\n",
        "            \"labels\": \"Y, N, N/A\",\n",
        "            \"transcript\": \"Students are talking among themselves and not paying attention to the teacher's instructions.\"\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"üéØ INFERENCE TESTS\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    for i, case in enumerate(test_cases, 1):\n",
        "        # Format the prompt\n",
        "        system_msg = \"You are an expert Teach framework scorer. Reply ONLY with valid JSON: {\\\"score\\\":<label>,\\\"analysis\\\":<text>}\"\n",
        "        user_msg = f\"### Component: {case['component']}\\n### Allowed labels: {case['labels']}\\n\\nTranscript:\\n{case['transcript']}\\n\"\n",
        "        \n",
        "        # Create conversation format\n",
        "        conversation = [\n",
        "            {\"role\": \"system\", \"content\": system_msg},\n",
        "            {\"role\": \"user\", \"content\": user_msg}\n",
        "        ]\n",
        "        \n",
        "        # Apply chat template\n",
        "        prompt = tokenizer.apply_chat_template(\n",
        "            conversation,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "        \n",
        "        print(f\"\\nüß™ Test {i}: {case['component']}\")\n",
        "        print(f\"üìù Transcript: {case['transcript'][:100]}...\")\n",
        "        \n",
        "        # Generate response\n",
        "        response = pipe(\n",
        "            prompt,\n",
        "            max_new_tokens=150,\n",
        "            do_sample=False,\n",
        "            temperature=0.1,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )[0]['generated_text']\n",
        "        \n",
        "        # Extract just the assistant's response\n",
        "        assistant_response = response.split(\"<|im_start|>assistant\\n\")[-1].split(\"<|im_end|>\")[0].strip()\n",
        "        \n",
        "        print(f\"ü§ñ Response: {assistant_response}\")\n",
        "        \n",
        "        # Try to parse as JSON\n",
        "        try:\n",
        "            parsed = json.loads(assistant_response)\n",
        "            print(f\"‚úÖ Valid JSON - Score: {parsed.get('score', 'N/A')}\")\n",
        "        except:\n",
        "            print(f\"‚ö†Ô∏è Invalid JSON format\")\n",
        "        \n",
        "        print(\"-\" * 60)\n",
        "    \n",
        "    print(\"\\n‚úÖ Inference testing completed!\")\n",
        "    \n",
        "    # Cleanup\n",
        "    del pipe, model_with_lora, base_model_inference\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Inference test failed: {e}\")\n",
        "    print(\"üí° The model may still be valid - check the saved adapter manually\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b846ac63",
      "metadata": {
        "id": "b846ac63"
      },
      "source": [
        "## 11 Summary & Next Steps\n",
        "\n",
        "### ‚úÖ Completed:\n",
        "- **Model**: Fine-tuned Gemma 3 27B IT with LoRA (r=64, Œ±=16)\n",
        "- **Dataset**: Processed Teach framework data into ChatML format\n",
        "- **Training**: Advanced checkpointing with auto-resume capability\n",
        "- **Hardware**: Optimized for A100 40GB with BF16 precision\n",
        "- **Monitoring**: Connection keepalive to prevent disconnections\n",
        "\n",
        "### üöÄ Next Steps:\n",
        "1. **Evaluate Performance**: Run the model on your Teach evaluation pipeline\n",
        "2. **Hyperparameter Tuning**: Experiment with different LoRA ranks and learning rates\n",
        "3. **Production Deployment**: Use the uploaded model for inference\n",
        "4. **Continuous Training**: Resume training with additional data if needed\n",
        "\n",
        "### üìä Model Files:\n",
        "- **LoRA Adapter**: `{ADAPTER_DIR}/` (lightweight, ~100MB)\n",
        "- **Checkpoints**: `{OUTPUT_DIR}/checkpoint-*/` (auto-saved every 50 steps)\n",
        "- **Hub Repository**: `https://huggingface.co/{LORA_REPO}` (if uploaded)\n",
        "\n",
        "### üõ†Ô∏è Usage Example:\n",
        "```python\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "# Load base model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-27b-it\")\n",
        "\n",
        "# Load fine-tuned adapter\n",
        "model = PeftModel.from_pretrained(base_model, \"your-username/gemma3-27b-teach-lora\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"your-username/gemma3-27b-teach-lora\")\n",
        "\n",
        "# Use for Teach framework scoring!\n",
        "```\n",
        "\n",
        "**üéâ Training Complete! Your Gemma 3 27B model is ready for Teach framework evaluation.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cleanup_final",
      "metadata": {
        "id": "cleanup_final"
      },
      "outputs": [],
      "source": [
        "# üßπ Final Cleanup and Summary\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"üßπ Performing final cleanup...\")\n",
        "\n",
        "# Memory cleanup\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Print final summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéâ TRAINING SESSION COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "print(f\"‚è∞ Completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"üéØ Model: {MODEL_NAME}\")\n",
        "print(f\"üìä Training examples: {len(train_ds):,}\")\n",
        "print(f\"üìä Validation examples: {len(val_ds):,}\")\n",
        "print(f\"‚öôÔ∏è LoRA configuration: r={peft_cfg.r}, Œ±={peft_cfg.lora_alpha}\")\n",
        "print(f\"üíæ LoRA adapter saved: {ADAPTER_DIR}/\")\n",
        "print(f\"üìÅ Checkpoints saved: {OUTPUT_DIR}/\")\n",
        "\n",
        "# List saved checkpoints\n",
        "if os.path.exists(OUTPUT_DIR):\n",
        "    checkpoints = [d for d in os.listdir(OUTPUT_DIR) if d.startswith('checkpoint-')]\n",
        "    if checkpoints:\n",
        "        print(f\"üîÑ Available checkpoints: {len(checkpoints)}\")\n",
        "        latest = max(checkpoints, key=lambda x: int(x.split('-')[1]))\n",
        "        print(f\"üìç Latest checkpoint: {latest}\")\n",
        "\n",
        "# Print file sizes\n",
        "if os.path.exists(ADAPTER_DIR):\n",
        "    adapter_size = sum(os.path.getsize(os.path.join(ADAPTER_DIR, f)) \n",
        "                      for f in os.listdir(ADAPTER_DIR) \n",
        "                      if os.path.isfile(os.path.join(ADAPTER_DIR, f)))\n",
        "    print(f\"üíæ LoRA adapter size: {adapter_size / 1024**2:.1f} MB\")\n",
        "\n",
        "print(\"\\nüöÄ Ready for evaluation and deployment!\")\n",
        "print(\"üîó Don't forget to update the HF_USERNAME variable if uploading\")\n",
        "print(\"üéØ Use the fine-tuned model in your Teach evaluation pipeline\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Stop keepalive (optional)\n",
        "stop_keepalive = input(\"\\nüîå Stop connection keepalive? (y/N): \").lower().strip()\n",
        "if stop_keepalive in ['y', 'yes']:\n",
        "    display(JavaScript(\"stopKeepAlive();\"))\n",
        "    print(\"üîå Connection keepalive stopped\")\n",
        "else:\n",
        "    print(\"üîå Connection keepalive still active\")\n",
        "\n",
        "print(\"\\n‚ú® All done! Happy evaluating! ‚ú®\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
