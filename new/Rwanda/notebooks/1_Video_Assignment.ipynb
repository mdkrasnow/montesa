{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab09aa09",
   "metadata": {},
   "source": [
    "# Data Formatting â€“ Rwanda (Manual Clipping with moviepy)\n",
    "\n",
    "This notebook loads the **TEACH Final Scores** CSV for *Rwanda*, discovers classroom videos stored on SharePoint, manually splits each full-length video into first and last 15-minute clips using `moviepy`, and saves only these clips to Google Drive. It also attaches clip links to the dataset and exports a final formatted CSV.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028fec2d",
   "metadata": {},
   "source": [
    "### Workflow\n",
    "1. Detect runtime, install missing packages.\n",
    "2. Mount / locate Google Drive container.\n",
    "3. Authenticate to SharePoint using browser cookies.\n",
    "4. Discover every video from SharePoint Rwanda 2020 folder.\n",
    "5. Load TEACH CSV dataset and prepare output columns.\n",
    "6. Manually split each video into first and last 15-minute clips in parallel using `ThreadPoolExecutor`.\n",
    "7. Attach clip links to the dataset and save final CSV.\n",
    "8. Log and retry errors, skip videos shorter than 30 minutes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69557e39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06d878c3",
   "metadata": {},
   "source": [
    "## Dependencies & Environment Setup\n",
    "\n",
    "Install required Python packages and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4ed12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q python-dotenv requests pandas moviepy\n",
    "!pip install -q google-auth google-auth-oauthlib google-auth-httplib2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfc5897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment detection & dependency install\n",
    "import importlib.util\n",
    "import subprocess, sys, os\n",
    "from pathlib import Path\n",
    "\n",
    "IN_COLAB = importlib.util.find_spec(\"google.colab\") is not None\n",
    "\n",
    "def _ensure(pkgs):\n",
    "    missing = [p for p in pkgs if importlib.util.find_spec(p.replace('-', '_')) is None]\n",
    "    if missing:\n",
    "        print(\"Installing:\", missing)\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *missing])\n",
    "\n",
    "_ensure([\"python-dotenv\", \"requests\", \"pandas\", \"moviepy\"])\n",
    "if IN_COLAB:\n",
    "    _ensure([\"google-auth\", \"google-auth-oauthlib\", \"google-auth-httplib2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e5876e",
   "metadata": {},
   "source": [
    "## Paths & Google Drive Mount\n",
    "\n",
    "Configure paths for raw data, outputs, and mount Google Drive if running in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29644e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    RAW_DIR = Path('/content/drive/My Drive/world bank/data/Rwanda')\n",
    "else:\n",
    "    RAW_DIR = Path.cwd()\n",
    "\n",
    "RAW_CSV = RAW_DIR / 'evals/Teach_Final_Scores_v1(ALL_Scores).csv'\n",
    "OUTPUT_DIR = RAW_DIR / 'evals/formattedData'\n",
    "VIDEO_OUTPUT_DIR = RAW_DIR / 'videos'\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "VIDEO_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"RAW_CSV: {RAW_CSV}\\nOUTPUT_DIR: {OUTPUT_DIR}\\nVIDEO_OUTPUT_DIR: {VIDEO_OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a51607",
   "metadata": {},
   "source": [
    "## SharePoint Authentication\n",
    "\n",
    "Authenticate to SharePoint using browser cookies stored in the `cookie` environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc87258",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import requests, os\n",
    "\n",
    "load_dotenv()\n",
    "cookie_string = os.getenv('cookie')\n",
    "if not cookie_string:\n",
    "    raise RuntimeError(\"Set 'cookie' environment variable with your SharePoint cookies.\")\n",
    "cookies = {kv.split('=')[0]: kv.split('=')[1] for kv in cookie_string.split(';') if '=' in kv}\n",
    "\n",
    "SP_BASE_URL = 'https://worldbankgroup.sharepoint.com/teams/TeachDashboardVideoLibrary-WBGroup'\n",
    "SP_FOLDER_PATH = '/teams/TeachDashboardVideoLibrary-WBGroup/Shared Documents/General/Rwanda 2020'\n",
    "SP_HEADERS = {'Accept': 'application/json;odata=verbose','User-Agent':'Mozilla/5.0','Referer':'https://worldbankgroup.sharepoint.com/'}\n",
    "r = requests.get(f\"{SP_BASE_URL}/_api/web\", cookies=cookies, headers=SP_HEADERS)\n",
    "print(f\"SharePoint connection status: {r.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0619e352",
   "metadata": {},
   "source": [
    "## SharePoint Video Discovery\n",
    "\n",
    "Discover all video files in the specified SharePoint folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cba8ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "VIDEO_EXTS = {'.mp4','.MP4','.mov','.MOV','.avi','.AVI','.mts','.MTS'}\n",
    "\n",
    "def get_file_metadata(server_relative_url):\n",
    "    url = f\"{SP_BASE_URL}/_api/web/GetFileByServerRelativeUrl('{server_relative_url}')\"\n",
    "    r = requests.get(url, cookies=cookies, headers=SP_HEADERS)\n",
    "    if r.status_code==200:\n",
    "        data=r.json()['d']\n",
    "        return {'TimeCreated':data.get('TimeCreated'),'Length':data.get('Length',0),'success':True}\n",
    "    return {'success':False}\n",
    "\n",
    "def get_folder_contents(folder_path):\n",
    "    url = f\"{SP_BASE_URL}/_api/web/GetFolderByServerRelativeUrl('{folder_path}')/Files\"\n",
    "    r = requests.get(url, cookies=cookies, headers=SP_HEADERS)\n",
    "    return r.json().get('d',{}).get('results',[]) if r.status_code==200 else []\n",
    "\n",
    "def discover_rwanda_videos():\n",
    "    vids=[]\n",
    "    print(\"Scanning SharePoint folder for videos...\")\n",
    "    for f in get_folder_contents(SP_FOLDER_PATH):\n",
    "        ext=Path(f['Name']).suffix\n",
    "        if ext in VIDEO_EXTS:\n",
    "            vids.append({'name':f['Name'],'url':f['ServerRelativeUrl'],'metadata':get_file_metadata(f['ServerRelativeUrl'])})\n",
    "    print(f\"Found {len(vids)} videos.\")\n",
    "    return vids\n",
    "\n",
    "discovered_videos = discover_rwanda_videos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2c5fb0",
   "metadata": {},
   "source": [
    "## Load TEACH Dataset & Prepare Clip Columns\n",
    "\n",
    "Load the CSV and ensure `First Video Clip` and `Last Video Clip` columns exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6338e388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_dataset(path):\n",
    "    lines=path.read_text(encoding='latin-1').splitlines()\n",
    "    h1=[h.strip() for h in lines[0].split(',')]\n",
    "    h2=[h.strip() for h in lines[1].split(',')]\n",
    "    base=h1[:3]+h2[3:]\n",
    "    cols,seen=[],{}\n",
    "    for c in base:\n",
    "        if not c: c='Unnamed'\n",
    "        seen[c]=seen.get(c,0)\n",
    "        cols.append(c if seen[c]==0 else f\"{c}_{seen[c]}\")\n",
    "        seen[c]+=1\n",
    "    return pd.read_csv(path,header=None,skiprows=[0,2],names=cols,encoding='latin-1')\n",
    "\n",
    "print(f\"Loading dataset from {RAW_CSV}\")\n",
    "df=load_dataset(RAW_CSV)\n",
    "for col in ['First Video Clip','Last Video Clip']:\n",
    "    if col not in df.columns: df[col]=''\n",
    "print('Dataset prepared.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a08d848",
   "metadata": {},
   "source": [
    "## Manual Clipping Function with Retry and Parallel Execution\n",
    "\n",
    "Define a function to download, split, and save clips with up to 3 retries, skipping videos shorter than 30 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a778780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, tempfile, os\n",
    "from moviepy.editor import VideoFileClip\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "id_re=re.compile(r\"(\\\\d{6,7})\")\n",
    "errors=[]\n",
    "skipped=[]\n",
    "\n",
    "def download_video(srv_rel_url, target):\n",
    "    url=f\"https://worldbankgroup.sharepoint.com{srv_rel_url}?download=1\"\n",
    "    with requests.get(url,cookies=cookies,headers=SP_HEADERS,stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(target,'wb') as f:\n",
    "            for chunk in r.iter_content(8192):\n",
    "                f.write(chunk)\n",
    "\n",
    "def process_video(v):\n",
    "    name, url = v['name'], v['url']\n",
    "    m=id_re.search(name)\n",
    "    if not m:\n",
    "        errors.append((name,'No ID'))\n",
    "        return None\n",
    "    ident=m.group(1)\n",
    "    for attempt in range(1,4):\n",
    "        try:\n",
    "            tmp=tempfile.NamedTemporaryFile(delete=False,suffix=os.path.splitext(name)[1]).name\n",
    "            download_video(url,tmp)\n",
    "            clip=VideoFileClip(tmp)\n",
    "            dur=clip.duration\n",
    "            if dur<1800:\n",
    "                skipped.append(name)\n",
    "                clip.close()\n",
    "                os.remove(tmp)\n",
    "                return (ident,None,None)\n",
    "            c1=clip.subclip(0,900)\n",
    "            c2=clip.subclip(dur-900,dur)\n",
    "            out1=VIDEO_OUTPUT_DIR/f\"{ident} Clip 1.mp4\"\n",
    "            out2=VIDEO_OUTPUT_DIR/f\"{ident} Clip 2.mp4\"\n",
    "            c1.write_videofile(str(out1),codec='libx264',audio_codec='aac',verbose=False,logger=None)\n",
    "            c2.write_videofile(str(out2),codec='libx264',audio_codec='aac',verbose=False,logger=None)\n",
    "            clip.close();c1.close();c2.close();os.remove(tmp)\n",
    "            return (ident,str(out1),str(out2))\n",
    "        except Exception as e:\n",
    "            errors.append((name,f\"{attempt}: {e}\"))\n",
    "            if attempt==3: return None\n",
    "\n",
    "results={}\n",
    "with ThreadPoolExecutor(max_workers=os.cpu_count()) as ex:\n",
    "    futures={ex.submit(process_video,v):v for v in discovered_videos}\n",
    "    for f in as_completed(futures):\n",
    "        r=f.result()\n",
    "        if r:\n",
    "            i,o1,o2=r; results[i]={'first':o1,'last':o2}\n",
    "\n",
    "print(f\"Done. Clips: {len(results)}, Skipped: {len(skipped)}, Errors: {len(errors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d9e04b",
   "metadata": {},
   "source": [
    "## Attach Clip Links & Save Final CSV\n",
    "\n",
    "Populate the DataFrame with clip paths and export the final CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2faac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,row in df.iterrows():\n",
    "    m=id_re.search(str(row.get('School_Clip','')))\n",
    "    if m:\n",
    "        ident=m.group(1)\n",
    "        r=results.get(ident)\n",
    "        if r:\n",
    "            if r['first']: df.at[idx,'First Video Clip']=r['first']\n",
    "            if r['last']: df.at[idx,'Last Video Clip']=r['last']\n",
    "\n",
    "out_csv=OUTPUT_DIR/'rwanda_manual_clips.csv'\n",
    "df.to_csv(out_csv,index=False)\n",
    "print(f\"Saved CSV at {out_csv}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Harvard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
