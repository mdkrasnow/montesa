{
    "cells": [
      {
        "cell_type": "markdown",
        "id": "ab09aa09",
        "metadata": {
          "id": "ab09aa09"
        },
        "source": [
          "# Data Formatting – Rwanda (Manual Clipping with moviepy)\n",
          "\n",
          "This notebook loads the **TEACH Final Scores** CSV for *Rwanda*, discovers classroom videos stored on SharePoint, manually splits each full-length video into first and last 15-minute clips using `moviepy`, and saves only these clips to Google Drive. It also attaches clip links to the dataset and exports a final formatted CSV.\n",
          "\n",
          "**Enhanced Features:**\n",
          "- ✅ Progress bar with tqdm\n",
          "- ✅ Progressive saving every 5 videos processed\n",
          "- ✅ Resume capability from last saved progress\n",
          "- ✅ Robust error handling and retry logic\n"
        ]
      },
      {
        "cell_type": "markdown",
        "id": "028fec2d",
        "metadata": {
          "id": "028fec2d"
        },
        "source": [
          "### Workflow\n",
          "1. Detect runtime, install missing packages.\n",
          "2. Mount / locate Google Drive container.\n",
          "3. Authenticate to SharePoint using browser cookies.\n",
          "4. Discover every video from SharePoint Rwanda 2020 folder.\n",
          "5. Load TEACH CSV dataset and prepare output columns.\n",
          "6. **NEW:** Check for existing progress and resume from last checkpoint.\n",
          "7. **NEW:** Process videos with progress bar and save every 5 videos.\n",
          "8. Manually split each video into first and last 15-minute clips in parallel using `ThreadPoolExecutor`.\n",
          "9. Attach clip links to the dataset and save final CSV.\n",
          "10. Log and retry errors, skip videos shorter than 30 minutes.\n"
        ]
      },
      {
        "cell_type": "markdown",
        "id": "06d878c3",
        "metadata": {
          "id": "06d878c3"
        },
        "source": [
          "## Dependencies & Environment Setup\n",
          "\n",
          "Install required Python packages and configure the environment."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "id": "af4ed12f",
        "metadata": {
          "id": "af4ed12f"
        },
        "outputs": [],
        "source": [
          "!pip install -q python-dotenv requests pandas moviepy tqdm\n",
          "!pip install -q google-auth google-auth-oauthlib google-auth-httplib2"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "id": "abfc5897",
        "metadata": {
          "colab": {
            "base_uri": "https://localhost:8080/"
          },
          "id": "abfc5897",
          "outputId": "c0eae73c-bacb-43fc-cf97-ff6c37ab0795"
        },
        "outputs": [
          {
            "name": "stdout",
            "output_type": "stream",
            "text": [
              "Installing: ['python-dotenv']\n",
              "Installing: ['google-auth']\n"
            ]
          }
        ],
        "source": [
          "# Environment detection & dependency install\n",
          "import importlib.util\n",
          "import subprocess, sys, os\n",
          "from pathlib import Path\n",
          "\n",
          "IN_COLAB = importlib.util.find_spec(\"google.colab\") is not None\n",
          "\n",
          "def _ensure(pkgs):\n",
          "    missing = [p for p in pkgs if importlib.util.find_spec(p.replace('-', '_')) is None]\n",
          "    if missing:\n",
          "        print(\"Installing:\", missing)\n",
          "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *missing])\n",
          "\n",
          "_ensure([\"python-dotenv\", \"requests\", \"pandas\", \"moviepy\", \"tqdm\"])\n",
          "if IN_COLAB:\n",
          "    _ensure([\"google-auth\", \"google-auth-oauthlib\", \"google-auth-httplib2\"])"
        ]
      },
      {
        "cell_type": "markdown",
        "id": "52e5876e",
        "metadata": {
          "id": "52e5876e"
        },
        "source": [
          "## Paths & Google Drive Mount\n",
          "\n",
          "Configure paths for raw data, outputs, and mount Google Drive if running in Colab."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "id": "29644e6e",
        "metadata": {
          "colab": {
            "base_uri": "https://localhost:8080/"
          },
          "id": "29644e6e",
          "outputId": "3096040a-37cd-48b2-f78a-ebf52373d56e"
        },
        "outputs": [
          {
            "name": "stdout",
            "output_type": "stream",
            "text": [
              "Mounted at /content/drive\n",
              "RAW_CSV: /content/drive/My Drive/world bank/data/Rwanda/evals/Teach_Final_Scores_v1(ALL_Scores).csv\n",
              "OUTPUT_DIR: /content/drive/My Drive/world bank/data/Rwanda/evals/formattedData\n",
              "VIDEO_OUTPUT_DIR: /content/drive/My Drive/world bank/data/Rwanda/videos\n",
              "PROGRESS_FILE: /content/drive/My Drive/world bank/data/Rwanda/evals/formattedData/progress.json\n",
              "CHECKPOINT_CSV: /content/drive/My Drive/world bank/data/Rwanda/evals/formattedData/rwanda_manual_clips_checkpoint.csv\n"
            ]
          }
        ],
        "source": [
          "if IN_COLAB:\n",
          "    from google.colab import drive\n",
          "    drive.mount('/content/drive')\n",
          "    RAW_DIR = Path('/content/drive/My Drive/world bank/data/Rwanda')\n",
          "else:\n",
          "    RAW_DIR = Path.cwd()\n",
          "\n",
          "RAW_CSV = RAW_DIR / 'evals/Teach_Final_Scores_v1(ALL_Scores).csv'\n",
          "OUTPUT_DIR = RAW_DIR / 'evals/formattedData'\n",
          "VIDEO_OUTPUT_DIR = RAW_DIR / 'videos'\n",
          "PROGRESS_FILE = OUTPUT_DIR / 'progress.json'\n",
          "CHECKPOINT_CSV = OUTPUT_DIR / 'rwanda_manual_clips_checkpoint.csv'\n",
          "\n",
          "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
          "VIDEO_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
          "print(f\"RAW_CSV: {RAW_CSV}\\nOUTPUT_DIR: {OUTPUT_DIR}\\nVIDEO_OUTPUT_DIR: {VIDEO_OUTPUT_DIR}\")\n",
          "print(f\"PROGRESS_FILE: {PROGRESS_FILE}\\nCHECKPOINT_CSV: {CHECKPOINT_CSV}\")"
        ]
      },
      {
        "cell_type": "markdown",
        "id": "f5a51607",
        "metadata": {
          "id": "f5a51607"
        },
        "source": [
          "## SharePoint Authentication\n",
          "\n",
          "Authenticate to SharePoint using browser cookies stored in the `cookie` environment variable."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "id": "5cc87258",
        "metadata": {
          "colab": {
            "base_uri": "https://localhost:8080/"
          },
          "id": "5cc87258",
          "outputId": "4a088902-03fd-4617-8484-8d9621f66ee7"
        },
        "outputs": [
          {
            "name": "stdout",
            "output_type": "stream",
            "text": [
              "SharePoint connection status: 200\n",
              "✅ Authentication successful!\n",
              "Response: {\"d\":{\"__metadata\":{\"id\":\"https://worldbankgroup.sharepoint.com/teams/TeachDashboardVideoLibrary-WBGroup/_api/Web\",\"uri\":\"https://worldbankgroup.sharepoint.com/teams/TeachDashboardVideoLibrary-WBGroup...\n"
            ]
          }
        ],
        "source": [
          "from dotenv import load_dotenv\n",
          "import requests, os\n",
          "from google.colab import userdata\n",
          "\n",
          "load_dotenv()\n",
          "\n",
          "# Method 1: Use the exact cookies from your browser\n",
          "SHAREPOINT_COOKIES = {\n",
          "    'rtFa': userdata.get('rtFa'),\n",
          "    'SIMI': userdata.get('SIMI'),\n",
          "    'FedAuth': userdata.get('FedAuth'),\n",
          "}\n",
          "\n",
          "SP_BASE_URL = 'https://worldbankgroup.sharepoint.com/teams/TeachDashboardVideoLibrary-WBGroup'\n",
          "SP_FOLDER_PATH = '/teams/TeachDashboardVideoLibrary-WBGroup/Shared Documents/General/Rwanda 2020'\n",
          "\n",
          "# Complete headers matching your successful browser request\n",
          "SP_HEADERS = {\n",
          "    'Accept': 'application/json;odata=verbose',\n",
          "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36',\n",
          "    'Referer': 'https://worldbankgroup.sharepoint.com/',\n",
          "    'X-RequestDigest': '0xF565A91A7BD37466546E94C89866BE8BC7ED8EE11DECF380BB7DB547ED52D9F749B0E629C90EC7955287641B5169B358AF12D615AF1FE996DCCDD0DD65F31311,16 Jun 2025 20:44:18 -0000',\n",
          "    'Content-Type': 'application/json;odata=verbose'\n",
          "}\n",
          "\n",
          "# Test the connection\n",
          "r = requests.get(f\"{SP_BASE_URL}/_api/web\", cookies=SHAREPOINT_COOKIES, headers=SP_HEADERS)\n",
          "print(f\"SharePoint connection status: {r.status_code}\")\n",
          "\n",
          "if r.status_code == 200:\n",
          "    print(\"✅ Authentication successful!\")\n",
          "    print(f\"Response: {r.text[:200]}...\")\n",
          "else:\n",
          "    print(f\"❌ Authentication failed: {r.status_code}\")\n",
          "    print(f\"Response: {r.text}\")"
        ]
      },
      {
        "cell_type": "markdown",
        "id": "0619e352",
        "metadata": {
          "id": "0619e352"
        },
        "source": [
          "## SharePoint Video Discovery\n",
          "\n",
          "Discover all video files in the specified SharePoint folder."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "id": "4cba8ef1",
        "metadata": {
          "colab": {
            "base_uri": "https://localhost:8080/"
          },
          "id": "4cba8ef1",
          "outputId": "71e8f504-2e9e-4646-d7ca-d0a32651ead6"
        },
        "outputs": [
          {
            "name": "stdout",
            "output_type": "stream",
            "text": [
              "✅ ENHANCED SharePoint video discovery functions ready\n"
            ]
          }
        ],
        "source": [
          "# -----------------------------------------------------------\n",
          "# 3  ENHANCED SharePoint Functions with Timestamp Metadata\n",
          "# -----------------------------------------------------------\n",
          "\n",
          "VIDEO_EXTS = {'.mp4', '.MP4', '.mov', '.MOV', '.mts', '.MTS', '.avi', '.AVI'}\n",
          "\n",
          "def get_file_metadata_with_timestamps(server_relative_url):\n",
          "    \"\"\"Get detailed file metadata including timestamps from SharePoint.\"\"\"\n",
          "    try:\n",
          "        metadata_url = f\"{SP_BASE_URL}/_api/web/GetFileByServerRelativeUrl('{server_relative_url}')\"\n",
          "        response = requests.get(metadata_url, cookies=SHAREPOINT_COOKIES, headers=SP_HEADERS)\n",
          "\n",
          "        if response.status_code == 200:\n",
          "            data = response.json()['d']\n",
          "            return {\n",
          "                'TimeCreated': data.get('TimeCreated'),\n",
          "                'TimeLastModified': data.get('TimeLastModified'),\n",
          "                'Length': data.get('Length', 0),\n",
          "                'UIVersionString': data.get('UIVersionString', '1.0'),\n",
          "                'success': True\n",
          "            }\n",
          "        else:\n",
          "            return {'success': False, 'error': f\"HTTP {response.status_code}\"}\n",
          "    except Exception as e:\n",
          "        return {'success': False, 'error': str(e)}\n",
          "\n",
          "def get_folder_contents(folder_path):\n",
          "    \"\"\"Get files and subfolders in a SharePoint folder.\"\"\"\n",
          "    print(f\"📂 Scanning folder: {folder_path}\")\n",
          "\n",
          "    files = []\n",
          "    folders = []\n",
          "\n",
          "    try:\n",
          "        # Get files in folder\n",
          "        files_url = f\"{SP_BASE_URL}/_api/web/GetFolderByServerRelativeUrl('{folder_path}')/Files\"\n",
          "        response = requests.get(files_url, cookies=SHAREPOINT_COOKIES, headers=SP_HEADERS)\n",
          "\n",
          "        if response.status_code == 200:\n",
          "            data = response.json()\n",
          "            files = data['d']['results'] if 'results' in data['d'] else []\n",
          "            print(f\"   📄 Found {len(files)} files\")\n",
          "        else:\n",
          "            print(f\"   ❌ Failed to get files: {response.status_code}\")\n",
          "\n",
          "        # Get subfolders\n",
          "        folders_url = f\"{SP_BASE_URL}/_api/web/GetFolderByServerRelativeUrl('{folder_path}')/Folders\"\n",
          "        response = requests.get(folders_url, cookies=SHAREPOINT_COOKIES, headers=SP_HEADERS)\n",
          "\n",
          "        if response.status_code == 200:\n",
          "            data = response.json()\n",
          "            all_folders = data['d']['results'] if 'results' in data['d'] else []\n",
          "            # Filter out system folders\n",
          "            folders = [f for f in all_folders if not f['Name'].startswith('_') and f['Name'] not in ['Forms']]\n",
          "            print(f\"   📁 Found {len(folders)} subfolders\")\n",
          "        else:\n",
          "            print(f\"   ❌ Failed to get folders: {response.status_code}\")\n",
          "\n",
          "    except Exception as e:\n",
          "        print(f\"   💥 Error scanning {folder_path}: {e}\")\n",
          "\n",
          "    return files, folders\n",
          "\n",
          "def create_sharepoint_url(server_relative_url):\n",
          "    \"\"\"Create a standardized SharePoint URL from server relative URL.\"\"\"\n",
          "    return f\"https://worldbankgroup.sharepoint.com{server_relative_url}\"\n",
          "\n",
          "def discover_videos_recursive(folder_path, discovered_videos, progress_info):\n",
          "    \"\"\"Recursively discover all videos in SharePoint folder with enhanced metadata.\"\"\"\n",
          "    files, folders = get_folder_contents(folder_path)\n",
          "\n",
          "    # Process files in current folder\n",
          "    for file_info in files:\n",
          "        file_name = file_info['Name']\n",
          "        file_ext = Path(file_name).suffix\n",
          "\n",
          "        # Only catalog video files\n",
          "        if file_ext not in VIDEO_EXTS:\n",
          "            continue\n",
          "\n",
          "        progress_info['total_found'] += 1\n",
          "\n",
          "        # Create standardized SharePoint URL\n",
          "        sharepoint_url = create_sharepoint_url(file_info['ServerRelativeUrl'])\n",
          "\n",
          "        # Get enhanced metadata with timestamps\n",
          "        metadata = get_file_metadata_with_timestamps(file_info['ServerRelativeUrl'])\n",
          "\n",
          "        # Store video info with metadata\n",
          "        video_info = {\n",
          "            'name': file_name,\n",
          "            'url': sharepoint_url,\n",
          "            'server_path': file_info['ServerRelativeUrl'],\n",
          "            'folder_path': folder_path,\n",
          "            'metadata': metadata\n",
          "        }\n",
          "\n",
          "        discovered_videos.append(video_info)\n",
          "\n",
          "        if progress_info['total_found'] % 10 == 0:\n",
          "            print(f\"   📊 Discovered {progress_info['total_found']} videos so far...\")\n",
          "\n",
          "    # Process subfolders recursively\n",
          "    for folder_info in folders:\n",
          "        folder_name = folder_info['Name']\n",
          "        if folder_name == 'Videos':\n",
          "          folder_server_path = folder_info['ServerRelativeUrl']\n",
          "          # Recurse into subfolder\n",
          "          discover_videos_recursive(folder_server_path, discovered_videos, progress_info)\n",
          "\n",
          "print(\"✅ ENHANCED SharePoint video discovery functions ready\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "id": "TTBdodMqjJbR",
        "metadata": {
          "colab": {
            "base_uri": "https://localhost:8080/"
          },
          "id": "TTBdodMqjJbR",
          "outputId": "69921e80-6351-4d39-806a-c7c1c406cd1b"
        },
        "outputs": [
          {
            "name": "stdout",
            "output_type": "stream",
            "text": [
              "================================================================================\n",
              "🎬 STARTING ENHANCED SHAREPOINT VIDEO DISCOVERY\n",
              "================================================================================\n",
              "📍 Source: /teams/TeachDashboardVideoLibrary-WBGroup/Shared Documents/General/Rwanda 2020\n",
              "🎯 Target Extensions: .avi, .MTS, .mp4, .MOV, .mts, .AVI, .mov, .MP4\n",
              "🕒 Enhanced with timestamp metadata for ordering verification\n",
              "\n",
              "📂 Scanning folder: /teams/TeachDashboardVideoLibrary-WBGroup/Shared Documents/General/Rwanda 2020\n",
              "   📄 Found 9 files\n",
              "   📁 Found 1 subfolders\n",
              "📂 Scanning folder: /teams/TeachDashboardVideoLibrary-WBGroup/Shared Documents/General/Rwanda 2020/Videos\n",
              "   📄 Found 199 files\n",
              "   📁 Found 2 subfolders\n",
              "   📊 Discovered 10 videos so far...\n",
              "   📊 Discovered 20 videos so far...\n",
              "   📊 Discovered 30 videos so far...\n",
              "   📊 Discovered 40 videos so far...\n",
              "   📊 Discovered 50 videos so far...\n",
              "   📊 Discovered 60 videos so far...\n",
              "   📊 Discovered 70 videos so far...\n",
              "   📊 Discovered 80 videos so far...\n",
              "   📊 Discovered 90 videos so far...\n",
              "   📊 Discovered 100 videos so far...\n",
              "   📊 Discovered 110 videos so far...\n",
              "   📊 Discovered 120 videos so far...\n",
              "   📊 Discovered 130 videos so far...\n",
              "   📊 Discovered 140 videos so far...\n",
              "   📊 Discovered 150 videos so far...\n",
              "   📊 Discovered 160 videos so far...\n",
              "   📊 Discovered 170 videos so far...\n",
              "   📊 Discovered 180 videos so far...\n",
              "   📊 Discovered 190 videos so far...\n",
              "\n",
              "================================================================================\n",
              "📊 ENHANCED DISCOVERY COMPLETE - SUMMARY\n",
              "================================================================================\n",
              "🔍 Videos discovered: 199\n",
              "🕒 Metadata retrieved: 199/199 (100.0%)\n",
              "⏱️  Time elapsed: 85.5 seconds\n",
              "\n",
              "📋 Sample discovered videos with metadata:\n",
              "   1. 107212.mp4\n",
              "      URL: https://worldbankgroup.sharepoint.com/teams/TeachDashboardVideoLibrary-WBGroup/Shared Documents/General/Rwanda 2020/Videos/107212.mp4\n",
              "      Created: 2020-07-14T16:00:37Z\n",
              "      Size: 273703194 bytes\n",
              "   2. 168207.mp4\n",
              "      URL: https://worldbankgroup.sharepoint.com/teams/TeachDashboardVideoLibrary-WBGroup/Shared Documents/General/Rwanda 2020/Videos/168207.mp4\n",
              "      Created: 2020-10-02T14:32:58Z\n",
              "      Size: 270943672 bytes\n",
              "   3. 122189.mp4\n",
              "      URL: https://worldbankgroup.sharepoint.com/teams/TeachDashboardVideoLibrary-WBGroup/Shared Documents/General/Rwanda 2020/Videos/122189.mp4\n",
              "      Created: 2020-07-14T12:34:19Z\n",
              "      Size: 273478819 bytes\n",
              "   ... and 196 more\n",
              "================================================================================\n",
              "✅ Ready to process 199 discovered videos with enhanced metadata!\n"
            ]
          }
        ],
        "source": [
          "# -----------------------------------------------------------\n",
          "# 4  Execute SharePoint Video Discovery with Enhanced Metadata\n",
          "# -----------------------------------------------------------\n",
          "import time as time\n",
          "\n",
          "def discover_rwanda_videos():\n",
          "    \"\"\"Main function to discover all videos in Rwanda 2020 folder with timestamps.\"\"\"\n",
          "    print(\"=\"*80)\n",
          "    print(\"🎬 STARTING ENHANCED SHAREPOINT VIDEO DISCOVERY\")\n",
          "    print(\"=\"*80)\n",
          "    print(f\"📍 Source: {SP_FOLDER_PATH}\")\n",
          "    print(f\"🎯 Target Extensions: {', '.join(VIDEO_EXTS)}\")\n",
          "    print(f\"🕒 Enhanced with timestamp metadata for ordering verification\")\n",
          "    print()\n",
          "\n",
          "    # Initialize progress tracking\n",
          "    progress_info = {\n",
          "        'total_found': 0\n",
          "    }\n",
          "\n",
          "    discovered_videos = []\n",
          "    start_time = time.time()\n",
          "\n",
          "    try:\n",
          "        # Start recursive discovery from Rwanda 2020 folder\n",
          "        discover_videos_recursive(SP_FOLDER_PATH, discovered_videos, progress_info)\n",
          "\n",
          "        # Calculate final stats\n",
          "        elapsed = time.time() - start_time\n",
          "\n",
          "        # Count metadata success rate\n",
          "        successful_metadata = sum(1 for v in discovered_videos if v['metadata']['success'])\n",
          "\n",
          "        print()\n",
          "        print(\"=\"*80)\n",
          "        print(\"📊 ENHANCED DISCOVERY COMPLETE - SUMMARY\")\n",
          "        print(\"=\"*80)\n",
          "        print(f\"🔍 Videos discovered: {len(discovered_videos)}\")\n",
          "        print(f\"🕒 Metadata retrieved: {successful_metadata}/{len(discovered_videos)} ({successful_metadata/len(discovered_videos)*100:.1f}%)\")\n",
          "        print(f\"⏱️  Time elapsed: {elapsed:.1f} seconds\")\n",
          "\n",
          "        if len(discovered_videos) > 0:\n",
          "            print(\"\\n📋 Sample discovered videos with metadata:\")\n",
          "            for i, video in enumerate(discovered_videos[:3]):\n",
          "                print(f\"   {i+1}. {video['name']}\")\n",
          "                print(f\"      URL: {video['url']}\")\n",
          "                if video['metadata']['success']:\n",
          "                    print(f\"      Created: {video['metadata'].get('TimeCreated', 'N/A')}\")\n",
          "                    print(f\"      Size: {video['metadata'].get('Length', 'N/A')} bytes\")\n",
          "                else:\n",
          "                    print(f\"      Metadata: {video['metadata']['error']}\")\n",
          "            if len(discovered_videos) > 3:\n",
          "                print(f\"   ... and {len(discovered_videos) - 3} more\")\n",
          "\n",
          "        print(\"=\"*80)\n",
          "        return discovered_videos\n",
          "\n",
          "    except Exception as e:\n",
          "        print(f\"💥 Discovery failed: {e}\")\n",
          "        return []\n",
          "\n",
          "# Execute the discovery\n",
          "discovered_videos = discover_rwanda_videos()\n",
          "\n",
          "if not discovered_videos:\n",
          "    print(\"❌ No videos were discovered. Check your SharePoint access and folder path.\")\n",
          "else:\n",
          "    print(f\"✅ Ready to process {len(discovered_videos)} discovered videos with enhanced metadata!\")"
        ]
      },
      {
        "cell_type": "markdown",
        "id": "1e2c5fb0",
        "metadata": {
          "id": "1e2c5fb0"
        },
        "source": [
          "## Load TEACH Dataset & Prepare Clip Columns\n",
          "\n",
          "Load the CSV and ensure `First Video Clip` and `Last Video Clip` columns exist."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "id": "6338e388",
        "metadata": {
          "colab": {
            "base_uri": "https://localhost:8080/"
          },
          "id": "6338e388",
          "outputId": "bb64f205-57aa-40c3-8b31-9bfe78606d7c"
        },
        "outputs": [
          {
            "name": "stdout",
            "output_type": "stream",
            "text": [
              "Loading dataset from /content/drive/My Drive/world bank/data/Rwanda/evals/Teach_Final_Scores_v1(ALL_Scores).csv\n",
              "Dataset prepared.\n"
            ]
          }
        ],
        "source": [
          "import pandas as pd\n",
          "\n",
          "def load_dataset(path):\n",
          "    lines=path.read_text(encoding='latin-1').splitlines()\n",
          "    h1=[h.strip() for h in lines[0].split(',')]\n",
          "    h2=[h.strip() for h in lines[1].split(',')]\n",
          "    base=h1[:3]+h2[3:]\n",
          "    cols,seen=[],{}\n",
          "    for c in base:\n",
          "        if not c: c='Unnamed'\n",
          "        seen[c]=seen.get(c,0)\n",
          "        cols.append(c if seen[c]==0 else f\"{c}_{seen[c]}\")\n",
          "        seen[c]+=1\n",
          "    return pd.read_csv(path,header=None,skiprows=[0,2],names=cols,encoding='latin-1')\n",
          "\n",
          "print(f\"Loading dataset from {RAW_CSV}\")\n",
          "df=load_dataset(RAW_CSV)\n",
          "for col in ['First Video Clip','Last Video Clip']:\n",
          "    if col not in df.columns: df[col]=''\n",
          "print('Dataset prepared.')"
        ]
      },
      {
        "cell_type": "markdown",
        "id": "progress_management",
        "metadata": {
          "id": "progress_management"
        },
        "source": [
          "## Progress Management & Resume Capability\n",
          "\n",
          "Define functions to save and load progress, allowing the notebook to resume from where it left off."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "id": "progress_functions",
        "metadata": {
          "id": "progress_functions"
        },
        "outputs": [],
        "source": [
          "import json\n",
          "import pandas as pd\n",
          "from datetime import datetime\n",
          "\n",
          "def save_progress(processed_ids, results, checkpoint_number):\n",
          "    \"\"\"Save current progress to JSON file and checkpoint CSV.\"\"\"\n",
          "    progress_data = {\n",
          "        'timestamp': datetime.now().isoformat(),\n",
          "        'checkpoint_number': checkpoint_number,\n",
          "        'processed_ids': list(processed_ids),\n",
          "        'results': results,\n",
          "        'total_processed': len(processed_ids)\n",
          "    }\n",
          "    \n",
          "    # Save progress JSON\n",
          "    with open(PROGRESS_FILE, 'w') as f:\n",
          "        json.dump(progress_data, f, indent=2)\n",
          "    \n",
          "    # Save checkpoint CSV\n",
          "    df_copy = df.copy()\n",
          "    \n",
          "    # Update dataframe with current results\n",
          "    for idx, row in df_copy.iterrows():\n",
          "        import re\n",
          "        id_re = re.compile(r\"(\\d{6,7})\")\n",
          "        m = id_re.search(str(row.get('School_Clip', '')))\n",
          "        if m:\n",
          "            ident = m.group(1)\n",
          "            r = results.get(ident)\n",
          "            if r:\n",
          "                if r['first']: df_copy.at[idx, 'First Video Clip'] = r['first']\n",
          "                if r['last']: df_copy.at[idx, 'Last Video Clip'] = r['last']\n",
          "    \n",
          "    df_copy.to_csv(CHECKPOINT_CSV, index=False)\n",
          "    print(f\"💾 Progress saved: {len(processed_ids)} videos processed (Checkpoint #{checkpoint_number})\")\n",
          "\n",
          "def load_progress():\n",
          "    \"\"\"Load existing progress if available.\"\"\"\n",
          "    if PROGRESS_FILE.exists():\n",
          "        try:\n",
          "            with open(PROGRESS_FILE, 'r') as f:\n",
          "                progress_data = json.load(f)\n",
          "            \n",
          "            processed_ids = set(progress_data.get('processed_ids', []))\n",
          "            results = progress_data.get('results', {})\n",
          "            checkpoint_number = progress_data.get('checkpoint_number', 0)\n",
          "            \n",
          "            print(f\"📂 Resuming from checkpoint #{checkpoint_number}\")\n",
          "            print(f\"🔄 Already processed: {len(processed_ids)} videos\")\n",
          "            \n",
          "            return processed_ids, results, checkpoint_number\n",
          "            \n",
          "        except Exception as e:\n",
          "            print(f\"⚠️  Could not load progress file: {e}\")\n",
          "            print(\"🆕 Starting fresh...\")\n",
          "    else:\n",
          "        print(\"🆕 No previous progress found. Starting fresh...\")\n",
          "    \n",
          "    return set(), {}, 0\n",
          "\n",
          "def get_pending_videos(discovered_videos, processed_ids):\n",
          "    \"\"\"Get list of videos that haven't been processed yet.\"\"\"\n",
          "    import re\n",
          "    id_re = re.compile(r\"(\\d{6,7})\")\n",
          "    \n",
          "    pending_videos = []\n",
          "    for video in discovered_videos:\n",
          "        # Extract ID from video name or URL\n",
          "        match = id_re.search(video.get('name', '') or video.get('url', ''))\n",
          "        if match:\n",
          "            ident = match.group(1)\n",
          "            if ident not in processed_ids:\n",
          "                pending_videos.append((ident, video))\n",
          "    \n",
          "    return pending_videos\n",
          "\n",
          "print(\"✅ Progress management functions ready\")"
        ]
      },
      {
        "cell_type": "markdown",
        "id": "1a08d848",
        "metadata": {
          "id": "1a08d848"
        },
        "source": [
          "## Enhanced Video Processing with Progress Tracking\n",
          "\n",
          "Process videos with progress bar, incremental saving every 5 videos, and resume capability."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "id": "5a778780",
        "metadata": {
          "colab": {
            "background_save": true
          },
          "id": "5a778780",
          "outputId": "d50a0ff6-5255-4383-8ee5-43b53b09ba88"
        },
        "outputs": [
          {
            "name": "stderr",
            "output_type": "stream",
            "text": [
              "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file /tmp/tmpldx3pri7.mp4, 1232640 bytes wanted but 0 bytes read,at frame 118611/118614, at time 2372.22/2372.26 sec. Using the last valid frame instead.\n",
              "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
              "\n",
              "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file /tmp/tmpldx3pri7.mp4, 1232640 bytes wanted but 0 bytes read,at frame 118612/118614, at time 2372.24/2372.26 sec. Using the last valid frame instead.\n",
              "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
              "\n"
            ]
          }
        ],
        "source": [
          "import re\n",
          "import tempfile\n",
          "import os\n",
          "import logging\n",
          "from moviepy.editor import VideoFileClip\n",
          "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
          "from tqdm import tqdm\n",
          "from urllib.parse import urljoin\n",
          "import time\n",
          "import threading\n",
          "\n",
          "# ─── Logging Setup ─────────────────────────────────────────────────────────────\n",
          "logging.basicConfig(\n",
          "    level=logging.INFO,\n",
          "    format=\"%(asctime)s %(levelname)s %(message)s\",\n",
          ")\n",
          "logger = logging.getLogger(__name__)\n",
          "\n",
          "# ─── Compile ID Regex ──────────────────────────────────────────────────────────\n",
          "# Match exactly six or seven digits for the video identifier\n",
          "id_re = re.compile(r\"(\\d{6,7})\")\n",
          "\n",
          "# ─── Progress tracking variables ────────────────────────────────────────────────\n",
          "progress_lock = threading.Lock()\n",
          "\n",
          "def download_video(srv_rel_url, target):\n",
          "    \"\"\"\n",
          "    Download a video from SharePoint (or any HTTP URL) into `target`.\n",
          "    Retries up to 3 times on failure, logging each attempt.\n",
          "    \"\"\"\n",
          "    # 1) Build the correct URL\n",
          "    if isinstance(srv_rel_url, str) and srv_rel_url.lower().startswith(\"http\"):\n",
          "        url = srv_rel_url\n",
          "    else:\n",
          "        url = urljoin(\"https://worldbankgroup.sharepoint.com\", srv_rel_url)\n",
          "\n",
          "    logger.debug(\"Downloading from URL: %s → %s\", srv_rel_url, url)\n",
          "\n",
          "    # 2) Attempt download up to 3 times\n",
          "    for attempt in range(1, 4):\n",
          "        try:\n",
          "            with requests.get(url,\n",
          "                              cookies=SHAREPOINT_COOKIES,\n",
          "                              headers=SP_HEADERS,\n",
          "                              stream=True) as response:\n",
          "                response.raise_for_status()\n",
          "                with open(target, \"wb\") as f:\n",
          "                    for chunk in response.iter_content(chunk_size=8192):\n",
          "                        f.write(chunk)\n",
          "            logger.info(\"✅ Download succeeded (%s) → %s\", attempt, target)\n",
          "            return True\n",
          "\n",
          "        except Exception as e:\n",
          "            logger.error(\"❌ Download error on attempt %d for URL %s: %s\", attempt, url, str(e))\n",
          "            if attempt < 3:\n",
          "                time.sleep(2)\n",
          "            else:\n",
          "                return False\n",
          "\n",
          "def process_single_video(video_id, video_metadata, pbar, errors, skipped, results, processed_ids, checkpoint_number):\n",
          "    \"\"\"\n",
          "    Process a single video: download, split into clips, and update progress.\n",
          "    Returns True if successful, False otherwise.\n",
          "    \"\"\"\n",
          "    srv_rel_url = video_metadata.get('url')\n",
          "    name = video_metadata.get('name', '<unknown>')\n",
          "\n",
          "    logger.info(\"⏳ Starting processing of %s (ID %s)\", name, video_id)\n",
          "\n",
          "    # Prepare temp file and output paths\n",
          "    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".mp4\").name\n",
          "    out1 = OUTPUT_DIR / f\"{video_id}_first.mp4\"\n",
          "    out2 = OUTPUT_DIR / f\"{video_id}_last.mp4\"\n",
          "\n",
          "    # Process with up to 3 retries\n",
          "    for attempt in range(1, 4):\n",
          "        try:\n",
          "            # Download video\n",
          "            if not download_video(srv_rel_url, tmp):\n",
          "                raise Exception(\"Download failed\")\n",
          "            \n",
          "            logger.debug(\"Downloaded %s to %s\", name, tmp)\n",
          "\n",
          "            # Process video\n",
          "            clip = VideoFileClip(tmp)\n",
          "            \n",
          "            # Check if video is long enough (at least 30 minutes)\n",
          "            if clip.duration < 1800:  # 30 minutes in seconds\n",
          "                logger.warning(\"⏭️  Skipping %s - too short (%.1f minutes)\", name, clip.duration/60)\n",
          "                with progress_lock:\n",
          "                    skipped.append((video_id, f\"Too short: {clip.duration/60:.1f} min\"))\n",
          "                clip.close()\n",
          "                os.remove(tmp)\n",
          "                return False\n",
          "\n",
          "            # Split video in half\n",
          "            mid = clip.duration / 2\n",
          "            c1 = clip.subclip(0, mid)\n",
          "            c2 = clip.subclip(mid, clip.duration)\n",
          "\n",
          "            # Write clips\n",
          "            c1.write_videofile(str(out1), audio_codec=\"aac\", verbose=False, logger=None)\n",
          "            c2.write_videofile(str(out2), audio_codec=\"aac\", verbose=False, logger=None)\n",
          "\n",
          "            logger.info(\"✅ Finished clipping %s into %s and %s\", name, out1, out2)\n",
          "\n",
          "            # Update results\n",
          "            with progress_lock:\n",
          "                results[video_id] = {\"first\": str(out1), \"last\": str(out2)}\n",
          "                processed_ids.add(video_id)\n",
          "\n",
          "            # Cleanup\n",
          "            clip.close(); c1.close(); c2.close()\n",
          "            os.remove(tmp)\n",
          "\n",
          "            return True\n",
          "\n",
          "        except Exception as e:\n",
          "            logger.error(\n",
          "                \"❌ Error on attempt %d processing %s (ID %s): %s\",\n",
          "                attempt, name, video_id, str(e)\n",
          "            )\n",
          "            if attempt == 3:\n",
          "                with progress_lock:\n",
          "                    errors.append((video_id, f\"Failed after 3 attempts: {str(e)}\"))\n",
          "                return False\n",
          "            time.sleep(2)  # Wait before retry\n",
          "        finally:\n",
          "            # Clean up temp file if it exists\n",
          "            if os.path.exists(tmp):\n",
          "                try:\n",
          "                    os.remove(tmp)\n",
          "                except:\n",
          "                    pass\n",
          "\n",
          "    return False\n",
          "\n",
          "# ─── Main Processing Function with Progress Bar ──────────────────────────────────\n",
          "\n",
          "def process_videos_with_progress():\n",
          "    \"\"\"\n",
          "    Main function to process videos with progress bar and incremental saving.\n",
          "    \"\"\"\n",
          "    print(\"\\n\" + \"=\"*80)\n",
          "    print(\"🎬 STARTING VIDEO PROCESSING WITH PROGRESS TRACKING\")\n",
          "    print(\"=\"*80)\n",
          "    \n",
          "    # Load existing progress\n",
          "    processed_ids, results, checkpoint_number = load_progress()\n",
          "    \n",
          "    # Get pending videos\n",
          "    pending_videos = get_pending_videos(discovered_videos, processed_ids)\n",
          "    \n",
          "    if not pending_videos:\n",
          "        print(\"✅ All videos have already been processed!\")\n",
          "        return results, [], []\n",
          "    \n",
          "    print(f\"📊 Total videos discovered: {len(discovered_videos)}\")\n",
          "    print(f\"✅ Already processed: {len(processed_ids)}\")\n",
          "    print(f\"⏳ Remaining to process: {len(pending_videos)}\")\n",
          "    print(f\"💾 Save frequency: Every 5 videos\")\n",
          "    print()\n",
          "    \n",
          "    # Initialize tracking\n",
          "    errors = []\n",
          "    skipped = []\n",
          "    videos_since_last_save = 0\n",
          "    \n",
          "    # Create progress bar\n",
          "    pbar = tqdm(\n",
          "        total=len(pending_videos),\n",
          "        desc=\"Processing Videos\",\n",
          "        unit=\"video\",\n",
          "        position=0,\n",
          "        leave=True\n",
          "    )\n",
          "    \n",
          "    # Process videos with ThreadPoolExecutor\n",
          "    max_workers = min(4, os.cpu_count())  # Limit concurrent downloads\n",
          "    \n",
          "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
          "        # Submit all jobs\n",
          "        future_to_video = {\n",
          "            executor.submit(\n",
          "                process_single_video,\n",
          "                video_id, video_metadata, pbar, errors, skipped, \n",
          "                results, processed_ids, checkpoint_number\n",
          "            ): (video_id, video_metadata)\n",
          "            for video_id, video_metadata in pending_videos\n",
          "        }\n",
          "        \n",
          "        # Process completed jobs\n",
          "        for future in as_completed(future_to_video):\n",
          "            video_id, video_metadata = future_to_video[future]\n",
          "            \n",
          "            try:\n",
          "                success = future.result()\n",
          "                \n",
          "                # Update progress bar\n",
          "                pbar.update(1)\n",
          "                \n",
          "                if success:\n",
          "                    videos_since_last_save += 1\n",
          "                    pbar.set_postfix_str(f\"✅ Processed: {len(results)}, Errors: {len(errors)}, Skipped: {len(skipped)}\")\n",
          "                    \n",
          "                    # Save progress every 5 videos\n",
          "                    if videos_since_last_save >= 5:\n",
          "                        checkpoint_number += 1\n",
          "                        save_progress(processed_ids, results, checkpoint_number)\n",
          "                        videos_since_last_save = 0\n",
          "                else:\n",
          "                    pbar.set_postfix_str(f\"⚠️  Processed: {len(results)}, Errors: {len(errors)}, Skipped: {len(skipped)}\")\n",
          "                \n",
          "            except Exception as e:\n",
          "                logger.error(f\"💥 Unexpected error processing {video_id}: {e}\")\n",
          "                with progress_lock:\n",
          "                    errors.append((video_id, f\"Unexpected error: {str(e)}\"))\n",
          "                pbar.update(1)\n",
          "    \n",
          "    pbar.close()\n",
          "    \n",
          "    # Final save\n",
          "    if videos_since_last_save > 0:\n",
          "        checkpoint_number += 1\n",
          "        save_progress(processed_ids, results, checkpoint_number)\n",
          "    \n",
          "    # Print summary\n",
          "    print(\"\\n\" + \"=\"*80)\n",
          "    print(\"📊 PROCESSING COMPLETE - FINAL SUMMARY\")\n",
          "    print(\"=\"*80)\n",
          "    print(f\"✅ Successfully processed: {len(results)} videos\")\n",
          "    print(f\"⏭️  Skipped (too short): {len(skipped)} videos\")\n",
          "    print(f\"❌ Errors: {len(errors)} videos\")\n",
          "    print(f\"📁 Total files created: {len(results) * 2} clips\")\n",
          "    \n",
          "    if errors:\n",
          "        print(\"\\n❌ Error Summary:\")\n",
          "        for video_id, error_msg in errors[:5]:  # Show first 5 errors\n",
          "            print(f\"   - {video_id}: {error_msg}\")\n",
          "        if len(errors) > 5:\n",
          "            print(f\"   ... and {len(errors) - 5} more errors\")\n",
          "    \n",
          "    if skipped:\n",
          "        print(\"\\n⏭️  Skipped Summary:\")\n",
          "        for video_id, skip_reason in skipped[:5]:  # Show first 5 skipped\n",
          "            print(f\"   - {video_id}: {skip_reason}\")\n",
          "        if len(skipped) > 5:\n",
          "            print(f\"   ... and {len(skipped) - 5} more skipped\")\n",
          "    \n",
          "    print(\"=\"*80)\n",
          "    \n",
          "    return results, errors, skipped\n",
          "\n",
          "# ─── Execute Processing ─────────────────────────────────────────────────────────\n",
          "\n",
          "if discovered_videos:\n",
          "    results, errors, skipped = process_videos_with_progress()\n",
          "else:\n",
          "    print(\"❌ No videos discovered. Cannot proceed with processing.\")\n",
          "    results, errors, skipped = {}, [], []"
        ]
      },
      {
        "cell_type": "markdown",
        "id": "b9d9e04b",
        "metadata": {
          "id": "b9d9e04b"
        },
        "source": [
          "## Attach Clip Links & Save Final CSV\n",
          "\n",
          "Populate the DataFrame with clip paths and export the final CSV."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "id": "fc2faac6",
        "metadata": {
          "id": "fc2faac6"
        },
        "outputs": [],
        "source": [
          "print(\"\\n\" + \"=\"*60)\n",
          "print(\"📝 FINALIZING DATASET AND SAVING RESULTS\")\n",
          "print(\"=\"*60)\n",
          "\n",
          "# Update DataFrame with clip links\n",
          "updated_rows = 0\n",
          "for idx, row in df.iterrows():\n",
          "    m = id_re.search(str(row.get('School_Clip', '')))\n",
          "    if m:\n",
          "        ident = m.group(1)\n",
          "        r = results.get(ident)\n",
          "        if r:\n",
          "            if r['first']: \n",
          "                df.at[idx, 'First Video Clip'] = r['first']\n",
          "            if r['last']: \n",
          "                df.at[idx, 'Last Video Clip'] = r['last']\n",
          "            updated_rows += 1\n",
          "\n",
          "print(f\"📊 Updated {updated_rows} rows with clip links\")\n",
          "\n",
          "# Save final CSV\n",
          "out_csv = OUTPUT_DIR / 'rwanda_manual_clips_final.csv'\n",
          "df.to_csv(out_csv, index=False)\n",
          "print(f\"💾 Saved final CSV: {out_csv}\")\n",
          "\n",
          "# Save summary report\n",
          "summary_file = OUTPUT_DIR / 'processing_summary.txt'\n",
          "with open(summary_file, 'w') as f:\n",
          "    f.write(\"Rwanda Video Processing Summary\\n\")\n",
          "    f.write(\"=\" * 40 + \"\\n\\n\")\n",
          "    f.write(f\"Total videos discovered: {len(discovered_videos)}\\n\")\n",
          "    f.write(f\"Successfully processed: {len(results)}\\n\")\n",
          "    f.write(f\"Skipped (too short): {len(skipped)}\\n\")\n",
          "    f.write(f\"Errors: {len(errors)}\\n\")\n",
          "    f.write(f\"Dataset rows updated: {updated_rows}\\n\")\n",
          "    f.write(f\"Total clips created: {len(results) * 2}\\n\\n\")\n",
          "    \n",
          "    if errors:\n",
          "        f.write(\"Errors:\\n\")\n",
          "        for video_id, error_msg in errors:\n",
          "            f.write(f\"  - {video_id}: {error_msg}\\n\")\n",
          "        f.write(\"\\n\")\n",
          "    \n",
          "    if skipped:\n",
          "        f.write(\"Skipped videos:\\n\")\n",
          "        for video_id, skip_reason in skipped:\n",
          "            f.write(f\"  - {video_id}: {skip_reason}\\n\")\n",
          "\n",
          "print(f\"📋 Saved processing summary: {summary_file}\")\n",
          "\n",
          "print(\"\\n✅ All processing complete!\")\n",
          "print(f\"📁 Final CSV: {out_csv}\")\n",
          "print(f\"📁 Checkpoint CSV: {CHECKPOINT_CSV}\")\n",
          "print(f\"📁 Progress file: {PROGRESS_FILE}\")\n",
          "print(f\"📁 Summary: {summary_file}\")"
        ]
      },
      {
        "cell_type": "markdown",
        "id": "cleanup_section",
        "metadata": {
          "id": "cleanup_section"
        },
        "source": [
          "## Cleanup & Verification\n",
          "\n",
          "Optional cleanup and verification of the processed clips."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "id": "cleanup_code",
        "metadata": {
          "id": "cleanup_code"
        },
        "outputs": [],
        "source": [
          "# Optional: Verify created clips\n",
          "def verify_clips():\n",
          "    \"\"\"Verify that all expected clip files were created.\"\"\"\n",
          "    print(\"\\n🔍 Verifying created clips...\")\n",
          "    \n",
          "    missing_clips = []\n",
          "    total_expected = len(results) * 2\n",
          "    found_clips = 0\n",
          "    \n",
          "    for video_id, clip_paths in results.items():\n",
          "        for clip_type, clip_path in clip_paths.items():\n",
          "            if os.path.exists(clip_path):\n",
          "                found_clips += 1\n",
          "            else:\n",
          "                missing_clips.append(f\"{video_id}_{clip_type}: {clip_path}\")\n",
          "    \n",
          "    print(f\"📊 Clip verification results:\")\n",
          "    print(f\"   Expected clips: {total_expected}\")\n",
          "    print(f\"   Found clips: {found_clips}\")\n",
          "    print(f\"   Missing clips: {len(missing_clips)}\")\n",
          "    \n",
          "    if missing_clips:\n",
          "        print(\"\\n❌ Missing clips:\")\n",
          "        for missing in missing_clips[:10]:  # Show first 10\n",
          "            print(f\"   - {missing}\")\n",
          "        if len(missing_clips) > 10:\n",
          "            print(f\"   ... and {len(missing_clips) - 10} more\")\n",
          "    else:\n",
          "        print(\"✅ All expected clips found!\")\n",
          "\n",
          "# Run verification\n",
          "if results:\n",
          "    verify_clips()\n",
          "\n",
          "print(\"\\n🎉 Rwanda video processing notebook completed successfully!\")\n",
          "print(\"\\n📖 To resume processing later:\")\n",
          "print(\"   1. Run all cells up to 'Enhanced Video Processing'\")\n",
          "print(\"   2. The notebook will automatically resume from the last checkpoint\")\n",
          "print(\"   3. Progress is saved every 5 videos and can be found in:\")\n",
          "print(f\"      - {PROGRESS_FILE}\")\n",
          "print(f\"      - {CHECKPOINT_CSV}\")"
        ]
      }
    ],
    "metadata": {
      "colab": {
        "provenance": []
      },
      "kernelspec": {
        "display_name": "Python 3",
        "name": "python3"
      },
      "language_info": {
        "file_extension": ".py",
        "mimetype": "text/x-python",
        "name": "python",
        "version": "3.12.5"
      }
    },
    "nbformat": 4,
    "nbformat_minor": 5
  }