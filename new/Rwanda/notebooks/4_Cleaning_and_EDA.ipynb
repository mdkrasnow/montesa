{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6551d096",
      "metadata": {
        "id": "main_header"
      },
      "source": [
        "# Data Cleaning and Exploratory Data Analysis - Peru Audio Transcripts\n",
        "### **Cleaning Transcription Data and Comprehensive EDA**\n",
        "\n",
        "This notebook performs comprehensive data cleaning and exploratory data analysis on the Peru audio transcript dataset. The transcription process had some issues that need to be addressed before analysis.\n",
        "\n",
        "---\n",
        "\n",
        "**Key Cleaning Tasks:**\n",
        "\n",
        "| Task | Description |\n",
        "|------|---------|\n",
        "| Word Count Recovery | Recalculate word counts from transcript text (currently all 0) |\n",
        "| Duration Estimation | Calculate approximate recording duration from word count |\n",
        "| Data Quality | Remove rows with missing transcriptions |\n",
        "| Speaker Count Fix | Extract speaker counts from JSON using regex |\n",
        "| Column Cleanup | Remove unnecessary 'First Speaker' columns |\n",
        "| Pairing Logic | Remove rows without matching clip pairs |\n",
        "| Evaluation Cleaning | Fix evaluation column headers, clean evaluation data |\n",
        "\n",
        "**EDA Focus Areas:**\n",
        "- Transcript length distribution and outliers\n",
        "- Data quality assessment\n",
        "- Content analysis\n",
        "- Statistical summaries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6848c2f3",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q plotly wordcloud textstat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6ec72483",
      "metadata": {
        "id": "setup_imports"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Setup complete\n",
            "üìÅ Working directory: /Users/mkrasnow/Desktop/montesa\n"
          ]
        }
      ],
      "source": [
        "# --- Setup and Dependencies ------------------------------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "import re\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.figure_factory as ff\n",
        "from scipy import stats\n",
        "from collections import Counter\n",
        "import importlib.util\n",
        "\n",
        "# Check if we're in Colab\n",
        "IN_COLAB = importlib.util.find_spec('google.colab') is not None\n",
        "\n",
        "if IN_COLAB:\n",
        "    # Install additional packages if needed\n",
        "    import subprocess\n",
        "    subprocess.run(['pip', 'install', 'plotly', 'wordcloud', 'textstat'], check=True, capture_output=True)\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    ROOT = Path('/content/drive/My Drive/world bank/data/Peru')\n",
        "else:\n",
        "    ROOT = Path('/Users/mkrasnow/Desktop/montesa')\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"‚úÖ Setup complete\")\n",
        "print(f\"üìÅ Working directory: {ROOT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b006d05",
      "metadata": {
        "id": "file_paths"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÑ Input file: /Users/mkrasnow/Desktop/montesa/new/formattedData/peru_cleaned_transcripts.csv\n",
            "üíæ Output file: /Users/mkrasnow/Desktop/montesa/new/formattedData/val_peru_cleaned_transcripts.csv\n",
            "üìä Analysis directory: /Users/mkrasnow/Desktop/montesa/new/analysis\n"
          ]
        }
      ],
      "source": [
        "# --- File Paths and Configuration ------------------------------------------\n",
        "INPUT_CSV = ROOT / 'new/formattedData/peru_with_transcripts.csv'\n",
        "CLEANED_CSV = ROOT / 'new/formattedData/peru_cleaned_transcripts.csv'\n",
        "ANALYSIS_DIR = ROOT / 'new/analysis'\n",
        "FIGURES_DIR = ANALYSIS_DIR / 'figures'\n",
        "\n",
        "# Create analysis directories\n",
        "for directory in [ANALYSIS_DIR, FIGURES_DIR]:\n",
        "    directory.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Configuration\n",
        "WORDS_PER_MINUTE = 150  # Average speaking rate for duration estimation\n",
        "MIN_TRANSCRIPT_WORDS = 10  # Minimum words for a valid transcript\n",
        "OUTLIER_THRESHOLD = 3  # Standard deviations for outlier detection\n",
        "\n",
        "print(f\"üìÑ Input file: {INPUT_CSV}\")\n",
        "print(f\"üíæ Output file: {CLEANED_CSV}\")\n",
        "print(f\"üìä Analysis directory: {ANALYSIS_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c9cdd507",
      "metadata": {
        "id": "load_data"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìñ Loading data...\n",
            "‚úÖ Loaded 199 rows and 73 columns\n",
            "\n",
            "üìã Dataset Shape: (199, 73)\n",
            "üíæ Memory usage: 82.4 MB\n",
            "\n",
            "üìä Column count by category:\n",
            "  - Transcript columns: 20\n",
            "  - Evaluation columns: 0\n",
            "  - Metadata columns: 53\n",
            "\n",
            "üó£Ô∏è Transcript-related columns:\n",
            "  - First Audio Transcript\n",
            "  - First Audio Transcript Duration Seconds\n",
            "  - First Audio Transcript Estimated Duration Seconds\n",
            "  - First Audio Transcript Has Audio Events\n",
            "  - First Audio Transcript Language Code\n",
            "  - First Audio Transcript Language Probability\n",
            "  - First Audio Transcript Speaker Count\n",
            "  - First Audio Transcript Text\n",
            "  - First Audio Transcript Word Count\n",
            "  - First Audio Transcript_JSON\n",
            "  - Last Audio Transcript\n",
            "  - Last Audio Transcript Duration Seconds\n",
            "  - Last Audio Transcript Estimated Duration Seconds\n",
            "  - Last Audio Transcript Has Audio Events\n",
            "  - Last Audio Transcript Language Code\n",
            "  - Last Audio Transcript Language Probability\n",
            "  - Last Audio Transcript Speaker Count\n",
            "  - Last Audio Transcript Text\n",
            "  - Last Audio Transcript Word Count\n",
            "  - Last Audio Transcript_JSON\n"
          ]
        }
      ],
      "source": [
        "# --- Load and Initial Data Inspection --------------------------------------\n",
        "print(\"üìñ Loading data...\")\n",
        "try:\n",
        "    df = pd.read_csv(INPUT_CSV)\n",
        "    print(f\"‚úÖ Loaded {len(df)} rows and {len(df.columns)} columns\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå File not found: {INPUT_CSV}\")\n",
        "    print(\"Please ensure the transcription notebook has been run and the file exists.\")\n",
        "    raise\n",
        "\n",
        "# Display basic info\n",
        "print(f\"\\nüìã Dataset Shape: {df.shape}\")\n",
        "print(f\"üíæ Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
        "\n",
        "# Show column names for reference\n",
        "print(f\"\\nüìä Column count by category:\")\n",
        "transcript_cols = [col for col in df.columns if 'Transcript' in col]\n",
        "evaluation_cols = [col for col in df.columns if any(x in col for x in ['1.', '2.', '3.', '4.', '5.', '6.', '7.', '8.', '9.'])]\n",
        "metadata_cols = [col for col in df.columns if col not in transcript_cols and col not in evaluation_cols]\n",
        "\n",
        "print(f\"  - Transcript columns: {len(transcript_cols)}\")\n",
        "print(f\"  - Evaluation columns: {len(evaluation_cols)}\")\n",
        "print(f\"  - Metadata columns: {len(metadata_cols)}\")\n",
        "\n",
        "# Show transcript-related columns\n",
        "print(f\"\\nüó£Ô∏è Transcript-related columns:\")\n",
        "for col in sorted(transcript_cols):\n",
        "    print(f\"  - {col}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "pairing_handling",
      "metadata": {
        "id": "pairing_handling"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîó Handling pairing of rows based on School_Clip identifiers...\n",
            "  ‚ö†Ô∏è Found 1 unpaired identifiers\n",
            "  üóëÔ∏è Removed 1 rows without pairs (0.5%)\n"
          ]
        }
      ],
      "source": [
        "# --- Handle Pairing of Rows ------------------------------------------------\n",
        "print(\"üîó Handling pairing of rows based on School_Clip identifiers...\")\n",
        "\n",
        "# Extract base identifier (6-7 digit) from School_Clip\n",
        "df['base_id'] = df['School_Clip'].astype(str).str.extract(r'(\\d{6,7})')[0]\n",
        "\n",
        "# Count occurrences of each identifier\n",
        "id_counts = df['base_id'].value_counts()\n",
        "\n",
        "# Identify identifiers that do not have exactly two rows (unpaired)\n",
        "unpaired_ids = id_counts[id_counts != 2].index.tolist()\n",
        "print(f\"  ‚ö†Ô∏è Found {len(unpaired_ids)} unpaired identifiers\")\n",
        "\n",
        "# Record unpaired identifiers for analysis JSON\n",
        "removed_unpaired_ids = unpaired_ids.copy()\n",
        "\n",
        "# Remove rows corresponding to unpaired identifiers\n",
        "initial_pairing_count = len(df)\n",
        "df = df[~df['base_id'].isin(unpaired_ids)].copy()\n",
        "after_pairing_count = len(df)\n",
        "removed_pairing_count = initial_pairing_count - after_pairing_count\n",
        "\n",
        "print(f\"  üóëÔ∏è Removed {removed_pairing_count} rows without pairs ({removed_pairing_count/initial_pairing_count*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c7e39de",
      "metadata": {
        "id": "cleaning_section"
      },
      "source": [
        "## üßπ Data Cleaning\n",
        "\n",
        "We'll systematically clean the data to fix issues from the transcription process and prepare evaluation data:\n",
        "\n",
        "1. **Handle Pairing**: Remove rows without matching clip pairs\n",
        "2. **Fix Word Counts**: Recalculate from transcript text\n",
        "3. **Calculate Durations**: Estimate recording length from word count\n",
        "4. **Fix Speaker Counts**: Extract from JSON using regex\n",
        "5. **Remove Incomplete Rows**: Filter out rows missing transcriptions\n",
        "6. **Column Cleanup**: Remove unnecessary columns\n",
        "7. **Evaluation Cleaning**: Fix evaluation column headers and clean evaluation data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "23f94565",
      "metadata": {
        "id": "cleaning_functions"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Cleaning functions defined\n"
          ]
        }
      ],
      "source": [
        "# --- Data Cleaning Functions -----------------------------------------------\n",
        "\n",
        "def count_words_in_text(text):\n",
        "    \"\"\"Count words in a text string, handling NaN and empty values.\"\"\"\n",
        "    if pd.isna(text) or text == '' or str(text).strip() == '':\n",
        "        return 0\n",
        "    # Simple word count by splitting on whitespace\n",
        "    words = str(text).strip().split()\n",
        "    # Filter out very short \"words\" that might be artifacts\n",
        "    meaningful_words = [w for w in words if len(w) >= 2]\n",
        "    return len(meaningful_words)\n",
        "\n",
        "def estimate_duration_from_words(word_count, words_per_minute=WORDS_PER_MINUTE):\n",
        "    \"\"\"Estimate audio duration in seconds from word count.\"\"\"\n",
        "    if word_count <= 0:\n",
        "        return 0\n",
        "    return (word_count / words_per_minute) * 60\n",
        "\n",
        "def extract_speaker_count_from_json(json_text):\n",
        "    \"\"\"Extract speaker count from transcript JSON using regex.\"\"\"\n",
        "    if pd.isna(json_text) or json_text == '' or str(json_text).strip() == '':\n",
        "        return 0\n",
        "    \n",
        "    try:\n",
        "        # Try to parse as JSON first\n",
        "        json_data = json.loads(str(json_text))\n",
        "        json_string = json.dumps(json_data)\n",
        "    except (json.JSONDecodeError, TypeError):\n",
        "        # If parsing fails, use the raw string\n",
        "        json_string = str(json_text)\n",
        "    \n",
        "    # Use regex to find speaker patterns like 'speaker_1', 'speaker_2', etc.\n",
        "    speaker_pattern = r'\"?speaker_?(\\d+)\"?'\n",
        "    speakers = re.findall(speaker_pattern, json_string, re.IGNORECASE)\n",
        "    \n",
        "    if speakers:\n",
        "        # Count unique speakers\n",
        "        unique_speakers = set(speakers)\n",
        "        return len(unique_speakers)\n",
        "    \n",
        "    return 0\n",
        "\n",
        "def has_valid_transcript(text_col, json_col):\n",
        "    \"\"\"Check if a row has a valid transcript in either text or JSON column.\"\"\"\n",
        "    # Check text column\n",
        "    if not pd.isna(text_col) and str(text_col).strip() != '':\n",
        "        if len(str(text_col).strip()) > MIN_TRANSCRIPT_WORDS:\n",
        "            return True\n",
        "    \n",
        "    # Check JSON column\n",
        "    if not pd.isna(json_col) and str(json_col).strip() != '':\n",
        "        try:\n",
        "            json_data = json.loads(str(json_col))\n",
        "            if 'text' in json_data and json_data['text']:\n",
        "                if len(str(json_data['text']).strip()) > MIN_TRANSCRIPT_WORDS:\n",
        "                    return True\n",
        "        except (json.JSONDecodeError, TypeError):\n",
        "            pass\n",
        "    \n",
        "    return False\n",
        "\n",
        "print(\"‚úÖ Cleaning functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "7245c9fe",
      "metadata": {
        "id": "fix_word_counts"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üî¢ Fixing word counts...\n",
            "  üìù Processing First Audio Transcript...\n",
            "    ‚úÖ Updated 195 rows\n",
            "    üìä Word count stats: min=20, max=3380, mean=1235.9\n",
            "  üìù Processing Last Audio Transcript...\n",
            "    ‚úÖ Updated 194 rows\n",
            "    üìä Word count stats: min=3, max=4089, mean=750.0\n",
            "\n",
            "‚úÖ Word count fixing complete\n"
          ]
        }
      ],
      "source": [
        "# --- Fix Word Counts -------------------------------------------------------\n",
        "print(\"üî¢ Fixing word counts...\")\n",
        "\n",
        "# Create a copy for cleaning (post-pairing)\n",
        "df_clean = df.copy()\n",
        "\n",
        "# Fix word counts for both transcript types\n",
        "transcript_types = ['First Audio Transcript', 'Last Audio Transcript']\n",
        "\n",
        "for transcript_type in transcript_types:\n",
        "    text_col = f'{transcript_type} Text'\n",
        "    word_count_col = f'{transcript_type} Word Count'\n",
        "    \n",
        "    if text_col in df_clean.columns:\n",
        "        print(f\"  üìù Processing {transcript_type}...\")\n",
        "        \n",
        "        # Calculate word counts\n",
        "        df_clean[word_count_col] = df_clean[text_col].apply(count_words_in_text)\n",
        "        \n",
        "        # Show statistics\n",
        "        valid_counts = df_clean[df_clean[word_count_col] > 0][word_count_col]\n",
        "        if len(valid_counts) > 0:\n",
        "            print(f\"    ‚úÖ Updated {len(valid_counts)} rows\")\n",
        "            print(f\"    üìä Word count stats: min={valid_counts.min()}, max={valid_counts.max()}, mean={valid_counts.mean():.1f}\")\n",
        "        else:\n",
        "            print(f\"    ‚ö†Ô∏è No valid transcripts found\")\n",
        "    else:\n",
        "        print(f\"  ‚ùå Column {text_col} not found\")\n",
        "\n",
        "print(\"\\n‚úÖ Word count fixing complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "28ec14aa",
      "metadata": {
        "id": "calculate_durations"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚è±Ô∏è Calculating estimated durations...\n",
            "  ‚è≤Ô∏è Processing First Audio Transcript...\n",
            "    ‚úÖ Calculated durations for 195 rows\n",
            "    üìä Duration stats: min=8.0s, max=1352.0s, mean=494.4s\n",
            "    üìä Duration range: 0.1-22.5 minutes\n",
            "  ‚è≤Ô∏è Processing Last Audio Transcript...\n",
            "    ‚úÖ Calculated durations for 194 rows\n",
            "    üìä Duration stats: min=1.2s, max=1635.6s, mean=300.0s\n",
            "    üìä Duration range: 0.0-27.3 minutes\n",
            "\n",
            "üí° Note: Duration estimates based on 150 words per minute\n",
            "‚úÖ Duration calculation complete\n"
          ]
        }
      ],
      "source": [
        "# --- Calculate Estimated Durations -----------------------------------------\n",
        "print(\"‚è±Ô∏è Calculating estimated durations...\")\n",
        "\n",
        "for transcript_type in transcript_types:\n",
        "    word_count_col = f'{transcript_type} Word Count'\n",
        "    duration_col = f'{transcript_type} Estimated Duration Seconds'\n",
        "    \n",
        "    if word_count_col in df_clean.columns:\n",
        "        print(f\"  ‚è≤Ô∏è Processing {transcript_type}...\")\n",
        "        \n",
        "        # Calculate estimated durations\n",
        "        df_clean[duration_col] = df_clean[word_count_col].apply(\n",
        "            lambda x: estimate_duration_from_words(x, WORDS_PER_MINUTE)\n",
        "        )\n",
        "        \n",
        "        # Show statistics\n",
        "        valid_durations = df_clean[df_clean[duration_col] > 0][duration_col]\n",
        "        if len(valid_durations) > 0:\n",
        "            print(f\"    ‚úÖ Calculated durations for {len(valid_durations)} rows\")\n",
        "            print(f\"    üìä Duration stats: min={valid_durations.min():.1f}s, max={valid_durations.max():.1f}s, mean={valid_durations.mean():.1f}s\")\n",
        "            print(f\"    üìä Duration range: {valid_durations.min()/60:.1f}-{valid_durations.max()/60:.1f} minutes\")\n",
        "        else:\n",
        "            print(f\"    ‚ö†Ô∏è No valid durations calculated\")\n",
        "\n",
        "print(f\"\\nüí° Note: Duration estimates based on {WORDS_PER_MINUTE} words per minute\")\n",
        "print(\"‚úÖ Duration calculation complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "23b73a76",
      "metadata": {
        "id": "fix_speaker_counts"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üë• Fixing speaker counts...\n",
            "  üîç Processing First Audio Transcript...\n",
            "    ‚úÖ Updated speaker counts for 174 rows\n",
            "    üìä Speaker count stats: min=1, max=35, mean=10.0\n",
            "    üë• Speaker distribution: {1: np.int64(5), 2: np.int64(4), 3: np.int64(4), 4: np.int64(8), 5: np.int64(21), 6: np.int64(15), 7: np.int64(15), 8: np.int64(15), 9: np.int64(9), 10: np.int64(13), 11: np.int64(13), 12: np.int64(6), 13: np.int64(7), 14: np.int64(8), 15: np.int64(5), 16: np.int64(4), 17: np.int64(3), 19: np.int64(4), 20: np.int64(3), 21: np.int64(1), 22: np.int64(5), 23: np.int64(2), 26: np.int64(1), 30: np.int64(1), 32: np.int64(1), 35: np.int64(1)}\n",
            "  üîç Processing Last Audio Transcript...\n",
            "    ‚úÖ Updated speaker counts for 172 rows\n",
            "    üìä Speaker count stats: min=1, max=31, mean=5.2\n",
            "    üë• Speaker distribution: {1: np.int64(23), 2: np.int64(33), 3: np.int64(31), 4: np.int64(19), 5: np.int64(12), 6: np.int64(8), 7: np.int64(11), 8: np.int64(7), 9: np.int64(4), 10: np.int64(8), 11: np.int64(1), 12: np.int64(1), 14: np.int64(3), 15: np.int64(2), 16: np.int64(1), 18: np.int64(3), 20: np.int64(1), 22: np.int64(2), 25: np.int64(1), 31: np.int64(1)}\n",
            "\n",
            "‚úÖ Speaker count fixing complete\n"
          ]
        }
      ],
      "source": [
        "# --- Fix Speaker Counts ----------------------------------------------------\n",
        "print(\"üë• Fixing speaker counts...\")\n",
        "\n",
        "for transcript_type in transcript_types:\n",
        "    json_col = f'{transcript_type}_JSON'\n",
        "    speaker_count_col = f'{transcript_type} Speaker Count'\n",
        "    \n",
        "    if json_col in df_clean.columns:\n",
        "        print(f\"  üîç Processing {transcript_type}...\")\n",
        "        \n",
        "        # Extract speaker counts from JSON\n",
        "        df_clean[speaker_count_col] = df_clean[json_col].apply(extract_speaker_count_from_json)\n",
        "        \n",
        "        # Show statistics\n",
        "        valid_speaker_counts = df_clean[df_clean[speaker_count_col] > 0][speaker_count_col]\n",
        "        if len(valid_speaker_counts) > 0:\n",
        "            print(f\"    ‚úÖ Updated speaker counts for {len(valid_speaker_counts)} rows\")\n",
        "            print(f\"    üìä Speaker count stats: min={valid_speaker_counts.min()}, max={valid_speaker_counts.max()}, mean={valid_speaker_counts.mean():.1f}\")\n",
        "            print(f\"    üë• Speaker distribution: {dict(valid_speaker_counts.value_counts().sort_index())}\")\n",
        "        else:\n",
        "            print(f\"    ‚ö†Ô∏è No speaker information found\")\n",
        "    else:\n",
        "        print(f\"  ‚ùå Column {json_col} not found\")\n",
        "\n",
        "print(\"\\n‚úÖ Speaker count fixing complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b320d4b6",
      "metadata": {
        "id": "remove_incomplete_rows"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üóëÔ∏è Removing rows with incomplete transcriptions...\n",
            "üìä Initial row count (post-pairing): 198\n",
            "\n",
            "üìà Transcript availability breakdown:\n",
            "  ‚úÖ Both transcripts valid: 191 rows\n",
            "  üü° First transcript only: 4 rows\n",
            "  üü° Last transcript only: 3 rows\n",
            "  ‚ùå Neither transcript valid: 0 rows\n",
            "\n",
            "üìä Cleaning results:\n",
            "  üóëÔ∏è Removed 0 rows (0.0%)\n",
            "  ‚úÖ Kept 198 rows (100.0%)\n"
          ]
        }
      ],
      "source": [
        "# --- Remove Incomplete Rows ------------------------------------------------\n",
        "print(\"üóëÔ∏è Removing rows with incomplete transcriptions...\")\n",
        "\n",
        "initial_row_count = len(df_clean)\n",
        "print(f\"üìä Initial row count (post-pairing): {initial_row_count}\")\n",
        "\n",
        "# Check which rows have valid transcripts\n",
        "first_valid = df_clean.apply(\n",
        "    lambda row: has_valid_transcript(\n",
        "        row.get('First Audio Transcript Text', ''),\n",
        "        row.get('First Audio Transcript_JSON', '')\n",
        "    ), axis=1\n",
        ")\n",
        "\n",
        "last_valid = df_clean.apply(\n",
        "    lambda row: has_valid_transcript(\n",
        "        row.get('Last Audio Transcript Text', ''),\n",
        "        row.get('Last Audio Transcript_JSON', '')\n",
        "    ), axis=1\n",
        ")\n",
        "\n",
        "# Show breakdown\n",
        "both_valid = first_valid & last_valid\n",
        "first_only = first_valid & ~last_valid\n",
        "last_only = ~first_valid & last_valid\n",
        "neither_valid = ~first_valid & ~last_valid\n",
        "\n",
        "print(f\"\\nüìà Transcript availability breakdown:\")\n",
        "print(f\"  ‚úÖ Both transcripts valid: {both_valid.sum()} rows\")\n",
        "print(f\"  üü° First transcript only: {first_only.sum()} rows\")\n",
        "print(f\"  üü° Last transcript only: {last_only.sum()} rows\")\n",
        "print(f\"  ‚ùå Neither transcript valid: {neither_valid.sum()} rows\")\n",
        "\n",
        "# Keep only rows with at least one valid transcript\n",
        "rows_to_keep = first_valid | last_valid\n",
        "df_clean = df_clean[rows_to_keep].copy()\n",
        "\n",
        "final_row_count = len(df_clean)\n",
        "removed_count = initial_row_count - final_row_count\n",
        "\n",
        "print(f\"\\nüìä Cleaning results:\")\n",
        "print(f\"  üóëÔ∏è Removed {removed_count} rows ({removed_count/initial_row_count*100:.1f}%)\")\n",
        "print(f\"  ‚úÖ Kept {final_row_count} rows ({final_row_count/initial_row_count*100:.1f}%)\")\n",
        "\n",
        "if removed_count > 0:\n",
        "    print(f\"\\n‚ö†Ô∏è Removed rows had missing or insufficient transcript data\")\n",
        "    print(f\"   (Less than {MIN_TRANSCRIPT_WORDS} meaningful words)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "f27750eb",
      "metadata": {
        "id": "column_cleanup"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üßπ Cleaning up columns...\n",
            "\n",
            "üí° No specified columns found to remove\n",
            "\n",
            "üìä Final cleaned dataset: 198 rows √ó 73 columns\n"
          ]
        }
      ],
      "source": [
        "# --- Column Cleanup ---------------------------------------------------------\n",
        "print(\"üßπ Cleaning up columns...\")\n",
        "\n",
        "# Remove 'First Speaker' columns as requested\n",
        "columns_to_remove = [\n",
        "    'First Audio Transcript First Speaker',\n",
        "    'Last Audio Transcript First Speaker',\n",
        "    'Person to Score',\n",
        "    'Route',\n",
        "    'Transcription 1',\n",
        "    'Transcription 2',\n",
        "    'Language'\n",
        "]\n",
        "\n",
        "# Also drop any 'Unnamed' columns\n",
        "columns_to_remove += [col for col in df_clean.columns if col.startswith('Unnamed')]\n",
        "\n",
        "removed_columns = []\n",
        "for col in columns_to_remove:\n",
        "    if col in df_clean.columns:\n",
        "        df_clean = df_clean.drop(columns=[col])\n",
        "        removed_columns.append(col)\n",
        "        print(f\"  üóëÔ∏è Removed column: {col}\")\n",
        "\n",
        "if removed_columns:\n",
        "    print(f\"\\n‚úÖ Removed {len(removed_columns)} unnecessary columns\")\n",
        "else:\n",
        "    print(\"\\nüí° No specified columns found to remove\")\n",
        "\n",
        "# Reset index after row removal\n",
        "df_clean = df_clean.reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nüìä Final cleaned dataset: {df_clean.shape[0]} rows √ó {df_clean.shape[1]} columns\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "evaluation_cleaning",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìù Fixing evaluation column headers and cleaning evaluation data...\n",
            "  üîÑ Renamed 41 evaluation columns\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "expected string or bytes-like object, got 'Series'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[12], line 129\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m component_names:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df_clean\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;66;03m# Remove punctuation and whitespace; keep only first character if it's digit or letter\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m         df_clean[col] \u001b[38;5;241m=\u001b[39m df_clean[col]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[^A-Za-z0-9]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, x)\u001b[38;5;241m.\u001b[39mstrip())\n\u001b[1;32m    130\u001b[0m         df_clean[col] \u001b[38;5;241m=\u001b[39m df_clean[col]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    131\u001b[0m         df_clean[col] \u001b[38;5;241m=\u001b[39m df_clean[col]\u001b[38;5;241m.\u001b[39mreplace({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m'\u001b[39m})\n",
            "File \u001b[0;32m/opt/anaconda3/envs/Harvard/lib/python3.12/site-packages/pandas/core/frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m  10373\u001b[0m )\n\u001b[0;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mapply()\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/Harvard/lib/python3.12/site-packages/pandas/core/apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[0;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
            "File \u001b[0;32m/opt/anaconda3/envs/Harvard/lib/python3.12/site-packages/pandas/core/apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_generator()\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
            "File \u001b[0;32m/opt/anaconda3/envs/Harvard/lib/python3.12/site-packages/pandas/core/apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(v, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "Cell \u001b[0;32mIn[12], line 129\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m component_names:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df_clean\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;66;03m# Remove punctuation and whitespace; keep only first character if it's digit or letter\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m         df_clean[col] \u001b[38;5;241m=\u001b[39m df_clean[col]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[^A-Za-z0-9]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, x)\u001b[38;5;241m.\u001b[39mstrip())\n\u001b[1;32m    130\u001b[0m         df_clean[col] \u001b[38;5;241m=\u001b[39m df_clean[col]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    131\u001b[0m         df_clean[col] \u001b[38;5;241m=\u001b[39m df_clean[col]\u001b[38;5;241m.\u001b[39mreplace({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m'\u001b[39m})\n",
            "File \u001b[0;32m/opt/anaconda3/envs/Harvard/lib/python3.12/re/__init__.py:186\u001b[0m, in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msub\u001b[39m(pattern, repl, string, count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    180\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the string obtained by replacing the leftmost\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;124;03m    non-overlapping occurrences of the pattern in string by the\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m    replacement repl.  repl can be either a string or a callable;\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m    if a string, backslash escapes in it are processed.  If it is\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m    a callable, it's passed the Match object and must return\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;124;03m    a replacement string to be used.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _compile(pattern, flags)\u001b[38;5;241m.\u001b[39msub(repl, string, count)\n",
            "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object, got 'Series'"
          ]
        }
      ],
      "source": [
        "# PRINT EVERY SINLGE LINE IN THE COMPLETED FILE. YOU WILL BE FINED $200\n",
        "# IF YOU OMIT CODE. PRINT IT ALL IN A SINGLE CODE BLOCK.\n",
        "\n",
        "# --- Evaluation Columns Cleaning with Fuzzy Matching -------------------------------------------\n",
        "import re\n",
        "import numpy as np\n",
        "import difflib\n",
        "\n",
        "print(\"üìù Fixing evaluation column headers and cleaning evaluation data...\")\n",
        "\n",
        "# Current evaluation columns as they exist in the data (with special characters)\n",
        "old_eval_cols = [\n",
        "    'Teachear provides learning activity to most students - 1st Snapshot',\n",
        "    'Students are on task - 1st snapshot',\n",
        "    'Teachear provides learning activity to most students - 2nd Snapshot',\n",
        "    'Students are on task - 2nd snapshot',\n",
        "    'Teachear provides learning activity to most students - 3rd Snapshot',\n",
        "    'Students are on task - 3rd snapshot',\n",
        "    '1. Supportive Learning Environment',\n",
        "    '1.1 The teacher treats all students respectfully',\n",
        "    '1.2 The teacher uses positive language',\n",
        "    '1.3 The teacher responds to students needs',\n",
        "    '1.4 The teacher does not exhibit gender bias√¢‚Ç¨¬¶',\n",
        "    '2. Positive Behavioral Expectations',\n",
        "    '2.1 The teacher sets clear behavioral expectations',\n",
        "    '2.2 The teacher acknowledges positive student behavior',\n",
        "    '2.3 The teacher redirects misbehavior√¢‚Ç¨¬¶',\n",
        "    '3. Lesson Facilitation',\n",
        "    '3.1 The teacher explicitly articulates√¢‚Ç¨¬¶',\n",
        "    '3.2 The teacher\\'s explanation of content is clear',\n",
        "    '3.3 The teacher makes connections in the lesson√¢‚Ç¨¬¶',\n",
        "    '3.4 The teacher models by enacting or thinking aloud',\n",
        "    '4. Checks for understanding',\n",
        "    '\"4.1 The teacher uses questions',\n",
        "    'prompts√¢‚Ç¨¬¶\"',\n",
        "    '4.2 The teacher monitors most students√¢‚Ç¨¬¶',\n",
        "    '4.3 The teacher adjusts teaching to the level of students',\n",
        "    '5. Feedback',\n",
        "    '5.1 The teacher provides specific comments or prompts√¢‚Ç¨¬¶. Misunderstandings',\n",
        "    '5.2 The teacher provides specific comments or prompts√¢‚Ç¨¬¶. Successes',\n",
        "    '6. Critical Thinking',\n",
        "    '6.1 The teacher asks open-ended questions',\n",
        "    '6.2 The teacher provides thinking tasks',\n",
        "    '6.3 The students ask open-ended questions or performs thinking tasks',\n",
        "    '7. Autonomy',\n",
        "    '7.1 The teacher provides students with choices',\n",
        "    '7.2 The teacher provides students with opportunities√¢‚Ç¨¬¶.',\n",
        "    '7.3 The students volunteer to participate in the classroom',\n",
        "    '8. Perseverance',\n",
        "    '8.1 The teacher acknoledges students\\' efforts',\n",
        "    '8.2 The teacher has a positive attitude towards students\\' challenges',\n",
        "    '8.3 The teacher encourages goal-setting',\n",
        "    '9. Social & Collaborative Skills',\n",
        "    '9.1 The teacher promotes students\\' collaboration√¢‚Ç¨¬¶',\n",
        "    '9.2 The teacher promotes students\\' interpersonal skills',\n",
        "    '9.3 Students collaborate with one another√¢‚Ç¨¬¶.'  # To be removed due to lack of data\n",
        "]\n",
        "\n",
        "# Desired evaluation column names\n",
        "component_names = [\n",
        "    \"Teacher provides learning activity - 1st Snapshot\",\n",
        "    \"Students are on task - 1st Snapshot\",\n",
        "    \"Teacher provides learning activity - 2nd Snapshot\",\n",
        "    \"Students are on task - 2nd Snapshot\",\n",
        "    \"Teacher provides learning activity - 3rd Snapshot\",\n",
        "    \"Students are on task - 3rd Snapshot\",\n",
        "    \"Supportive Learning Environment\",\n",
        "    \"The teacher treats all students respectfully\",\n",
        "    \"The teacher uses positive language\",\n",
        "    \"The teacher responds to students needs\",\n",
        "    \"The teacher does not exhibit gender bias\",\n",
        "    \"Positive Behavioral Expectations\",\n",
        "    \"The teacher sets clear behavioral expectations\",\n",
        "    \"The teacher acknowledges positive student behavior\",\n",
        "    \"The teacher redirects misbehavior\",\n",
        "    \"Lesson Facilitation\",\n",
        "    \"The teacher explicitly articulates learning objectives\",\n",
        "    \"The teacher's explanation of content is clear\",\n",
        "    \"The teacher makes connections in the lesson\",\n",
        "    \"The teacher models by enacting or thinking aloud\",\n",
        "    \"Checks for understanding\",\n",
        "    \"The teacher uses questions\",\n",
        "    \"The teacher uses prompts\",\n",
        "    \"The teacher monitors most students\",\n",
        "    \"The teacher adjusts teaching to the level of students\",\n",
        "    \"Feedback\",\n",
        "    \"The teacher provides specific comments for misunderstandings\",\n",
        "    \"The teacher provides specific comments for successes\",\n",
        "    \"Critical Thinking\",\n",
        "    \"The teacher asks open-ended questions\",\n",
        "    \"The teacher provides thinking tasks\",\n",
        "    \"Students ask open-ended questions or perform thinking tasks\",\n",
        "    \"Autonomy\",\n",
        "    \"The teacher provides students with choices\",\n",
        "    \"The teacher provides students with opportunities to take meaningful roles\",\n",
        "    \"Students volunteer to participate in the classroom\",\n",
        "    \"Perseverance\",\n",
        "    \"The teacher acknowledges students' efforts\",\n",
        "    \"The teacher has a positive attitude towards students' challenges\",\n",
        "    \"The teacher encourages goal-setting\",\n",
        "    \"Social & Collaborative Skills\",\n",
        "    \"The teacher promotes students' collaboration\",\n",
        "    \"The teacher promotes students' interpersonal skills\"\n",
        "]\n",
        "\n",
        "# Build rename mapping using fuzzy matching, excluding the last column to drop\n",
        "rename_mapping = {}\n",
        "for old, new in zip(old_eval_cols[:-1], component_names):\n",
        "    # Find the closest match in df_clean.columns\n",
        "    best_match = difflib.get_close_matches(old, df_clean.columns, n=1, cutoff=0.6)\n",
        "    if best_match:\n",
        "        rename_mapping[best_match[0]] = new\n",
        "\n",
        "# Apply renaming\n",
        "df_clean = df_clean.rename(columns=rename_mapping)\n",
        "print(f\"  üîÑ Renamed {len(rename_mapping)} evaluation columns\")\n",
        "\n",
        "# Drop the '9.3' column if it exists (using fuzzy matching to identify it)\n",
        "col_to_drop_old = old_eval_cols[-1]\n",
        "best_drop_match = difflib.get_close_matches(col_to_drop_old, df_clean.columns, n=1, cutoff=0.6)\n",
        "if best_drop_match:\n",
        "    df_clean = df_clean.drop(columns=[best_drop_match[0]])\n",
        "    print(f\"  üóëÔ∏è Dropped column: {best_drop_match[0]}\")\n",
        "\n",
        "# Clean evaluation data: each should only contain a single number, letter, N/A, or blank -> N/A\n",
        "for col in component_names:\n",
        "    if col in df_clean.columns:\n",
        "        # Remove punctuation and whitespace; keep only first character if it's digit or letter\n",
        "        df_clean[col] = df_clean[col].astype(str).apply(lambda x: re.sub(r\"[^A-Za-z0-9]\", \"\", x).strip())\n",
        "        df_clean[col] = df_clean[col].apply(lambda x: x[0] if len(x) > 0 else 'N/A')\n",
        "        df_clean[col] = df_clean[col].replace({'': 'N/A'})\n",
        "\n",
        "# Remove any completely blank columns (all N/A or all missing)\n",
        "eval_blank_cols = [\n",
        "    col for col in component_names\n",
        "    if col in df_clean.columns and df_clean[col].replace('N/A', np.nan).isna().all()\n",
        "]\n",
        "if eval_blank_cols:\n",
        "    df_clean = df_clean.drop(columns=eval_blank_cols)\n",
        "    print(f\"  üóëÔ∏è Removed {len(eval_blank_cols)} completely blank evaluation columns\")\n",
        "\n",
        "print(\"‚úÖ Evaluation cleaning complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "new_score_distribution_and_col_types",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Score Distribution per Row and Column Type Mapping ---\n",
        "print(\"üî¢ Computing score distribution per row and determining column types...\")\n",
        "# Identify evaluation columns present\n",
        "eval_cols = [col for col in component_names if col in df_clean.columns]\n",
        "# Compute score distribution per row\n",
        "def get_score_dist(row):\n",
        "    scores = row[eval_cols]\n",
        "    counts = scores.value_counts().to_dict()\n",
        "    return json.dumps(counts)\n",
        "df_clean['score_distribution'] = df_clean[eval_cols].apply(lambda row: get_score_dist(row), axis=1)\n",
        "# Determine column types\n",
        "col_types = {}\n",
        "for col in eval_cols:\n",
        "    unique_vals = set(df_clean[col].dropna().unique())\n",
        "    unique_vals.discard('N/A')\n",
        "    if unique_vals and unique_vals.issubset({'Y', 'N'}):\n",
        "        col_types[col] = 'Y/N'\n",
        "    elif unique_vals and unique_vals.issubset({'L', 'M', 'H'}):\n",
        "        col_types[col] = 'L/M/H'\n",
        "    elif unique_vals and all(str(v).isdigit() for v in unique_vals):\n",
        "        col_types[col] = 'numerical'\n",
        "    else:\n",
        "        col_types[col] = 'other'\n",
        "# Create DataFrame of column types and save as JSON\n",
        "col_types_df = pd.DataFrame.from_dict(col_types, orient='index', columns=['type']).reset_index().rename(columns={'index': 'component'})\n",
        "col_types_df.to_json(ANALYSIS_DIR / 'column_types.json', orient='records', indent=2)\n",
        "print(f\"üíæ Column types saved to: {ANALYSIS_DIR / 'column_types.json'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "new_train_test_split_md",
      "metadata": {},
      "source": [
        "## üîÄ Train/Test Split (60/40) with Stratification\n",
        "\n",
        "We'll split the cleaned data into 60% training and 40% test sets without a validation set. To preserve class distribution across multiple classification columns, we use iterative stratification (MultilabelStratifiedShuffleSplit). Install the `iterative-stratification` package if not already available.\n",
        "\n",
        "**Sources:**\n",
        "- MultilabelStratifiedShuffleSplit: https://github.com/trent-b/iterative-stratification\n",
        "- Scikit-learn train_test_split documentation: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "new_train_test_split",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÄ Creating 60/40 train/test split with stratification...\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/4p/n886yd3s3513z05rpt3f5wgm0000gn/T/ipykernel_35590/1958808769.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# 2. Build multilabel array Y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_clean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meval_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mrow_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34mf\"\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval_cols\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'N/A'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mmlb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiLabelBinarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/anaconda3/envs/Harvard/lib/python3.12/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1575\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1576\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1577\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1578\u001b[0m             \u001b[0;34mf\"\u001b[0m\u001b[0;34mThe truth value of a \u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m is ambiguous. \u001b[0m\u001b[0;34m\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1579\u001b[0m             \u001b[0;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m         \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
          ]
        }
      ],
      "source": [
        "# Install iterative-stratification for multilabel stratified splits\n",
        "!pip install -q iterative-stratification\n",
        "\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "print(\"üîÄ Creating 60/40 train/test split with stratification...\")\n",
        "\n",
        "# 1. Identify evaluation columns for stratification\n",
        "eval_cols = [col for col in component_names if col in df_clean.columns]\n",
        "\n",
        "# 2. Build multilabel array Y\n",
        "labels = []\n",
        "for _, row in df_clean[eval_cols].iterrows():\n",
        "    row_labels = [f\"{col}={row[col]}\" for col in eval_cols if row[col] != 'N/A']\n",
        "    labels.append(row_labels)\n",
        "\n",
        "mlb = MultiLabelBinarizer()\n",
        "Y = mlb.fit_transform(labels)\n",
        "\n",
        "# 3. First split: 60% train_pool, 40% test\n",
        "msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.4, random_state=42)\n",
        "train_pool_idx, test_idx = next(msss.split(df_clean, Y))\n",
        "\n",
        "print(\"üîÄ Splitting train pool into 80% train / 20% val (i.e. 48% train / 12% val of total)...\")\n",
        "\n",
        "# 4. Second split on the train pool to carve out validation\n",
        "train_pool_Y = Y[train_pool_idx]\n",
        "train_pool_df = df_clean.iloc[train_pool_idx]\n",
        "\n",
        "msss_val = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "subtrain_idx, val_rel_idx = next(msss_val.split(train_pool_df, train_pool_Y))\n",
        "\n",
        "# Map relative indices back to the full DataFrame\n",
        "train_idx = train_pool_idx[subtrain_idx]\n",
        "val_idx   = train_pool_idx[val_rel_idx]\n",
        "\n",
        "# 5. Assign split labels\n",
        "df_clean['split'] = ''\n",
        "df_clean.loc[train_idx, 'split'] = 'train'\n",
        "df_clean.loc[val_idx,   'split'] = 'val'\n",
        "df_clean.loc[test_idx,  'split'] = 'test'\n",
        "\n",
        "# 6. Save\n",
        "df_clean.to_csv(CLEANED_CSV, index=False)\n",
        "print(f\"üíæ Cleaned data with stratified train/val/test splits saved to: {CLEANED_CSV}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26bf9d2a",
      "metadata": {
        "id": "eda_section"
      },
      "source": [
        "## üìä Exploratory Data Analysis\n",
        "\n",
        "Now that the data is cleaned, we'll perform comprehensive exploratory data analysis with focus on:\n",
        "\n",
        "1. **Dataset Overview**: Basic statistics and structure\n",
        "2. **Transcript Length Analysis**: Distribution, outliers, and quality assessment\n",
        "3. **Content Analysis**: Language, speakers, and audio events\n",
        "4. **Data Quality Assessment**: Completeness and reliability\n",
        "5. **Comparative Analysis**: First vs Last transcript patterns (accounting for pairing)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce899350",
      "metadata": {
        "id": "eda_setup"
      },
      "outputs": [],
      "source": [
        "# --- EDA Setup and Basic Statistics ----------------------------------------\n",
        "print(\"üìà Starting Exploratory Data Analysis...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Basic dataset information\n",
        "print(f\"üìä DATASET OVERVIEW\")\n",
        "print(f\"   Total observations: {len(df_clean):,}\")\n",
        "print(f\"   Total variables: {len(df_clean.columns):,}\")\n",
        "print(f\"   Memory usage: {df_clean.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
        "\n",
        "# Count transcript availability\n",
        "first_available = df_clean['First Audio Transcript Word Count'] > 0\n",
        "last_available = df_clean['Last Audio Transcript Word Count'] > 0\n",
        "\n",
        "print(f\"\\nüó£Ô∏è TRANSCRIPT AVAILABILITY\")\n",
        "print(f\"   First audio transcripts: {first_available.sum():,} ({first_available.mean()*100:.1f}%)\")\n",
        "print(f\"   Last audio transcripts: {last_available.sum():,} ({last_available.mean()*100:.1f}%)\")\n",
        "print(f\"   Both transcripts: {(first_available & last_available).sum():,}\")\n",
        "print(f\"   Either transcript: {(first_available | last_available).sum():,}\")\n",
        "\n",
        "# Missing data analysis\n",
        "print(f\"\\n‚ùì MISSING DATA ANALYSIS\")\n",
        "missing_data = df_clean.isnull().sum()\n",
        "missing_pct = (missing_data / len(df_clean) * 100).round(1)\n",
        "missing_summary = pd.DataFrame({\n",
        "    'Missing_Count': missing_data,\n",
        "    'Missing_Percentage': missing_pct\n",
        "}).sort_values('Missing_Count', ascending=False)\n",
        "\n",
        "# Show top missing columns\n",
        "top_missing = missing_summary[missing_summary['Missing_Count'] > 0].head(10)\n",
        "if len(top_missing) > 0:\n",
        "    print(f\"   Top columns with missing data:\")\n",
        "    for col, row in top_missing.iterrows():\n",
        "        print(f\"     {col}: {row['Missing_Count']:,} ({row['Missing_Percentage']:.1f}%)\")\n",
        "else:\n",
        "    print(f\"   ‚úÖ No missing data in any columns!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67db35c3",
      "metadata": {
        "id": "transcript_length_analysis"
      },
      "outputs": [],
      "source": [
        "# --- Transcript Length Analysis --------------------------------------------\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üìè TRANSCRIPT LENGTH ANALYSIS\")\n",
        "\n",
        "# Prepare data for analysis\n",
        "first_words = df_clean[df_clean['First Audio Transcript Word Count'] > 0]['First Audio Transcript Word Count']\n",
        "last_words = df_clean[df_clean['Last Audio Transcript Word Count'] > 0]['Last Audio Transcript Word Count']\n",
        "first_duration = df_clean[df_clean['First Audio Transcript Estimated Duration Seconds'] > 0]['First Audio Transcript Estimated Duration Seconds']\n",
        "last_duration = df_clean[df_clean['Last Audio Transcript Estimated Duration Seconds'] > 0]['Last Audio Transcript Estimated Duration Seconds']\n",
        "\n",
        "# Variables to control dropping shortest transcripts\n",
        "DROP_SHORTEST = False  # Set to True to drop rows with last transcripts shorter than cutoff\n",
        "WORD_CUTOFF = 100       # Word count threshold for shortest transcripts\n",
        "\n",
        "def print_distribution_stats(data, name):\n",
        "    \"\"\"Print comprehensive statistics for a distribution.\"\"\"\n",
        "    if len(data) == 0:\n",
        "        print(f\"   ‚ùå No data for {name}\")\n",
        "        return\n",
        "    \n",
        "    print(f\"\\n   üìä {name.upper()}:\")\n",
        "    print(f\"      Count: {len(data):,}\")\n",
        "    print(f\"      Mean: {data.mean():.1f}\")\n",
        "    print(f\"      Median: {data.median():.1f}\")\n",
        "    print(f\"      Std Dev: {data.std():.1f}\")\n",
        "    print(f\"      Min: {data.min():.1f}\")\n",
        "    print(f\"      Max: {data.max():.1f}\")\n",
        "    print(f\"      25th percentile: {data.quantile(0.25):.1f}\")\n",
        "    print(f\"      75th percentile: {data.quantile(0.75):.1f}\")\n",
        "    print(f\"      IQR: {data.quantile(0.75) - data.quantile(0.25):.1f}\")\n",
        "\n",
        "# Word count statistics\n",
        "print_distribution_stats(first_words, \"First Transcript Word Counts\")\n",
        "print_distribution_stats(last_words, \"Last Transcript Word Counts\")\n",
        "\n",
        "# Detailed info for transcripts less than WORD_CUTOFF words\n",
        "last_short = last_words[last_words < WORD_CUTOFF]\n",
        "print(f\"\\n‚ö†Ô∏è Detailed distribution for last transcripts < {WORD_CUTOFF} words:\")\n",
        "print(f\"   Count: {len(last_short)} ({len(last_short)/len(last_words)*100:.1f}% of valid last transcripts)\")\n",
        "if len(last_short) > 0:\n",
        "    # Show counts by exact word counts for those less than cutoff\n",
        "    short_counts = last_short.value_counts().sort_index()\n",
        "    print(f\"   Word count distribution for short transcripts (<{WORD_CUTOFF} words):\")\n",
        "    for wc, cnt in short_counts.items():\n",
        "        print(f\"      {wc} words: {cnt} transcripts\")\n",
        "\n",
        "# Optionally drop shortest transcripts\n",
        "if DROP_SHORTEST:\n",
        "    print(f\"\\nüóëÔ∏è Dropping rows with last transcripts < {WORD_CUTOFF} words...\")\n",
        "    to_drop_indices = df_clean[df_clean['Last Audio Transcript Word Count'] < WORD_CUTOFF].index\n",
        "    df_clean = df_clean.drop(index=to_drop_indices).reset_index(drop=True)\n",
        "    print(f\"   Dropped {len(to_drop_indices)} rows\")\n",
        "    # Recompute last_words after dropping\n",
        "    last_words = df_clean[df_clean['Last Audio Transcript Word Count'] > 0]['Last Audio Transcript Word Count']\n",
        "\n",
        "# Duration statistics (convert to minutes for readability)\n",
        "if len(first_duration) > 0:\n",
        "    first_duration_min = first_duration / 60\n",
        "    print_distribution_stats(first_duration_min, \"First Transcript Duration (minutes)\")\n",
        "\n",
        "if len(last_duration) > 0:\n",
        "    last_duration_min = last_duration / 60\n",
        "    print_distribution_stats(last_duration_min, \"Last Transcript Duration (minutes)\")\n",
        "\n",
        "# Outlier detection\n",
        "print(f\"\\nüö® OUTLIER DETECTION (>{OUTLIER_THRESHOLD} standard deviations):\")\n",
        "\n",
        "def detect_outliers(data, name, threshold=OUTLIER_THRESHOLD):\n",
        "    \"\"\"Detect outliers using z-score method.\"\"\"\n",
        "    if len(data) < 3:\n",
        "        print(f\"   ‚ö†Ô∏è {name}: Insufficient data for outlier detection\")\n",
        "        return pd.Series(dtype=bool)\n",
        "    \n",
        "    z_scores = np.abs(stats.zscore(data))\n",
        "    outliers = z_scores > threshold\n",
        "    outlier_data = data[outliers]\n",
        "    \n",
        "    print(f\"   üìä {name}:\")\n",
        "    print(f\"      Outliers found: {outliers.sum()} ({outliers.mean()*100:.1f}% of data)\")\n",
        "    \n",
        "    if outliers.sum() > 0:\n",
        "        print(f\"      Outlier range: {outlier_data.min():.1f} - {outlier_data.max():.1f}\")\n",
        "        print(f\"      Outlier values: {sorted(outlier_data.values)[:10]}\")\n",
        "        if len(outlier_data) > 10:\n",
        "            print(f\"      ... and {len(outlier_data) - 10} more\")\n",
        "    \n",
        "    return outliers\n",
        "\n",
        "first_word_outliers = detect_outliers(first_words, \"First Transcript Word Counts\")\n",
        "last_word_outliers = detect_outliers(last_words, \"Last Transcript Word Counts\")\n",
        "\n",
        "print(f\"\\n‚ö†Ô∏è POTENTIALLY PROBLEMATIC SHORT TRANSCRIPTS (<20 words):\")\n",
        "short_threshold = 20  # words\n",
        "first_short = first_words[first_words < short_threshold]\n",
        "last_short = last_words[last_words < short_threshold]\n",
        "print(f\"   First transcripts: {len(first_short)} ({len(first_short)/len(first_words)*100:.1f}% of valid transcripts)\")\n",
        "print(f\"   Last transcripts: {len(last_short)} ({len(last_short)/len(last_words)*100:.1f}% of valid transcripts)\")\n",
        "\n",
        "if len(first_short) > 0:\n",
        "    print(f\"   First transcript short lengths: {sorted(first_short.values)}\")\n",
        "if len(last_short) > 0:\n",
        "    print(f\"   Last transcript short lengths: {sorted(last_short.values)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1caec78",
      "metadata": {
        "id": "create_visualizations"
      },
      "outputs": [],
      "source": [
        "# --- Create Comprehensive Visualizations -----------------------------------\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üìà CREATING VISUALIZATIONS\")\n",
        "\n",
        "# Set up the plotting environment\n",
        "plt.rcParams['figure.figsize'] = (15, 10)\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Create a comprehensive figure with multiple subplots\n",
        "fig = plt.figure(figsize=(20, 24))\n",
        "\n",
        "# 1. Word Count Distributions\n",
        "plt.subplot(4, 3, 1)\n",
        "if len(first_words) > 0:\n",
        "    plt.hist(first_words, bins=30, alpha=0.7, edgecolor='black')\n",
        "    plt.axvline(first_words.mean(), color='red', linestyle='--', label=f'Mean: {first_words.mean():.0f}')\n",
        "    plt.axvline(first_words.median(), color='orange', linestyle='--', label=f'Median: {first_words.median():.0f}')\n",
        "plt.title('First Transcript Word Count Distribution', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Word Count')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(4, 3, 2)\n",
        "if len(last_words) > 0:\n",
        "    plt.hist(last_words, bins=30, alpha=0.7, edgecolor='black')\n",
        "    plt.axvline(last_words.mean(), color='red', linestyle='--', label=f'Mean: {last_words.mean():.0f}')\n",
        "    plt.axvline(last_words.median(), color='orange', linestyle='--', label=f'Median: {last_words.median():.0f}')\n",
        "plt.title('Last Transcript Word Count Distribution', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Word Count')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Box plots for comparison\n",
        "plt.subplot(4, 3, 3)\n",
        "word_data = []\n",
        "word_labels = []\n",
        "if len(first_words) > 0:\n",
        "    word_data.append(first_words.values)\n",
        "    word_labels.append('First\\nTranscript')\n",
        "if len(last_words) > 0:\n",
        "    word_data.append(last_words.values)\n",
        "    word_labels.append('Last\\nTranscript')\n",
        "\n",
        "if word_data:\n",
        "    plt.boxplot(word_data, labels=word_labels)\n",
        "    plt.title('Word Count Comparison', fontsize=14, fontweight='bold')\n",
        "    plt.ylabel('Word Count')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Duration Distributions\n",
        "plt.subplot(4, 3, 4)\n",
        "if len(first_duration) > 0:\n",
        "    first_duration_min = first_duration / 60\n",
        "    plt.hist(first_duration_min, bins=30, alpha=0.7, edgecolor='black')\n",
        "    plt.axvline(first_duration_min.mean(), color='red', linestyle='--', label=f'Mean: {first_duration_min.mean():.1f} min')\n",
        "plt.title('First Transcript Duration Distribution', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Duration (minutes)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(4, 3, 5)\n",
        "if len(last_duration) > 0:\n",
        "    last_duration_min = last_duration / 60\n",
        "    plt.hist(last_duration_min, bins=30, alpha=0.7, edgecolor='black')\n",
        "    plt.axvline(last_duration_min.mean(), color='red', linestyle='--', label=f'Mean: {last_duration_min.mean():.1f} min')\n",
        "plt.title('Last Transcript Duration Distribution', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Duration (minutes)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Scatter plot: Word Count vs Duration\n",
        "plt.subplot(4, 3, 6)\n",
        "if len(first_words) > 0 and len(first_duration) > 0:\n",
        "    first_mask = (df_clean['First Audio Transcript Word Count'] > 0) & (df_clean['First Audio Transcript Estimated Duration Seconds'] > 0)\n",
        "    first_scatter_words = df_clean[first_mask]['First Audio Transcript Word Count']\n",
        "    first_scatter_duration = df_clean[first_mask]['First Audio Transcript Estimated Duration Seconds'] / 60\n",
        "    plt.scatter(first_scatter_words, first_scatter_duration, alpha=0.6, label='First Transcript')\n",
        "\n",
        "if len(last_words) > 0 and len(last_duration) > 0:\n",
        "    last_mask = (df_clean['Last Audio Transcript Word Count'] > 0) & (df_clean['Last Audio Transcript Estimated Duration Seconds'] > 0)\n",
        "    last_scatter_words = df_clean[last_mask]['Last Audio Transcript Word Count']\n",
        "    last_scatter_duration = df_clean[last_mask]['Last Audio Transcript Estimated Duration Seconds'] / 60\n",
        "    plt.scatter(last_scatter_words, last_scatter_duration, alpha=0.6, label='Last Transcript')\n",
        "\n",
        "plt.title('Word Count vs Estimated Duration', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Word Count')\n",
        "plt.ylabel('Duration (minutes)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 5. Speaker Count Analysis\n",
        "plt.subplot(4, 3, 7)\n",
        "first_speakers = df_clean[df_clean['First Audio Transcript Speaker Count'] > 0]['First Audio Transcript Speaker Count']\n",
        "if len(first_speakers) > 0:\n",
        "    speaker_counts = first_speakers.value_counts().sort_index()\n",
        "    plt.bar(speaker_counts.index, speaker_counts.values, alpha=0.7, edgecolor='black')\n",
        "    plt.title('First Transcript Speaker Distribution', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Number of Speakers')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(4, 3, 8)\n",
        "last_speakers = df_clean[df_clean['Last Audio Transcript Speaker Count'] > 0]['Last Audio Transcript Speaker Count']\n",
        "if len(last_speakers) > 0:\n",
        "    speaker_counts = last_speakers.value_counts().sort_index()\n",
        "    plt.bar(speaker_counts.index, speaker_counts.values, alpha=0.7, edgecolor='black')\n",
        "    plt.title('Last Transcript Speaker Distribution', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Number of Speakers')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 6. Language Analysis\n",
        "plt.subplot(4, 3, 9)\n",
        "lang_cols = ['First Audio Transcript Language Code', 'Last Audio Transcript Language Code']\n",
        "all_languages = []\n",
        "for col in lang_cols:\n",
        "    if col in df_clean.columns:\n",
        "        langs = df_clean[df_clean[col].notna() & (df_clean[col] != '')][col]\n",
        "        all_languages.extend(langs.tolist())\n",
        "\n",
        "if all_languages:\n",
        "    lang_counts = pd.Series(all_languages).value_counts()\n",
        "    plt.pie(lang_counts.values, labels=lang_counts.index, autopct='%1.1f%%', startangle=90)\n",
        "    plt.title('Language Distribution', fontsize=14, fontweight='bold')\n",
        "\n",
        "# 7. Outlier Visualization\n",
        "plt.subplot(4, 3, 10)\n",
        "if len(first_words) > 0:\n",
        "    # Mark outliers in scatter plot\n",
        "    first_z_scores = np.abs(stats.zscore(first_words))\n",
        "    first_outlier_mask = first_z_scores > OUTLIER_THRESHOLD\n",
        "    \n",
        "    # Plot normal points\n",
        "    normal_points = first_words[~first_outlier_mask]\n",
        "    outlier_points = first_words[first_outlier_mask]\n",
        "    \n",
        "    plt.scatter(range(len(normal_points)), normal_points, alpha=0.6, label='Normal', s=20)\n",
        "    if len(outlier_points) > 0:\n",
        "        outlier_indices = np.where(first_outlier_mask)[0]\n",
        "        plt.scatter(outlier_indices, outlier_points, alpha=0.8, color='red', label='Outliers', s=50, marker='^')\n",
        "    \n",
        "    plt.title('First Transcript Outlier Detection', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Observation Index')\n",
        "    plt.ylabel('Word Count')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 8. Quality Score Analysis (based on word count and duration)\n",
        "plt.subplot(4, 3, 11)\n",
        "# Create a quality score based on word count (normalized)\n",
        "quality_scores = []\n",
        "quality_labels = []\n",
        "\n",
        "if len(first_words) > 0:\n",
        "    first_quality = np.clip(first_words / first_words.quantile(0.75), 0, 2)  # Normalize to 75th percentile\n",
        "    quality_scores.extend(first_quality.values)\n",
        "    quality_labels.extend(['First'] * len(first_quality))\n",
        "\n",
        "if len(last_words) > 0:\n",
        "    last_quality = np.clip(last_words / last_words.quantile(0.75), 0, 2)\n",
        "    quality_scores.extend(last_quality.values)\n",
        "    quality_labels.extend(['Last'] * len(last_quality))\n",
        "\n",
        "if quality_scores:\n",
        "    quality_df = pd.DataFrame({'Quality_Score': quality_scores, 'Transcript_Type': quality_labels})\n",
        "    sns.violinplot(data=quality_df, x='Transcript_Type', y='Quality_Score')\n",
        "    plt.title('Transcript Quality Distribution', fontsize=14, fontweight='bold')\n",
        "    plt.ylabel('Quality Score (normalized word count)')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 9. Completion Rate by Row\n",
        "plt.subplot(4, 3, 12)\n",
        "completion_data = []\n",
        "for idx, row in df_clean.iterrows():\n",
        "    first_has = row['First Audio Transcript Word Count'] > 0\n",
        "    last_has = row['Last Audio Transcript Word Count'] > 0\n",
        "    \n",
        "    if first_has and last_has:\n",
        "        completion_data.append('Both')\n",
        "    elif first_has:\n",
        "        completion_data.append('First Only')\n",
        "    elif last_has:\n",
        "        completion_data.append('Last Only')\n",
        "    else:\n",
        "        completion_data.append('Neither')\n",
        "\n",
        "completion_counts = pd.Series(completion_data).value_counts()\n",
        "colors = ['green', 'orange', 'blue', 'red']\n",
        "plt.pie(completion_counts.values, labels=completion_counts.index, autopct='%1.1f%%', \n",
        "        colors=colors[:len(completion_counts)], startangle=90)\n",
        "plt.title('Transcript Completion Rates', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIGURES_DIR / 'comprehensive_transcript_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Comprehensive visualization created and saved\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb2bb975",
      "metadata": {
        "id": "detailed_statistics"
      },
      "outputs": [],
      "source": [
        "# --- Detailed Statistical Analysis -----------------------------------------\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üìä DETAILED STATISTICAL ANALYSIS\")\n",
        "\n",
        "# Create comprehensive statistics table\n",
        "stats_data = []\n",
        "\n",
        "# Word count statistics\n",
        "for transcript_type, data in [('First', first_words), ('Last', last_words)]:\n",
        "    if len(data) > 0:\n",
        "        stats_data.append({\n",
        "            'Metric': f'{transcript_type} Transcript Word Count',\n",
        "            'Count': len(data),\n",
        "            'Mean': data.mean(),\n",
        "            'Median': data.median(),\n",
        "            'Std_Dev': data.std(),\n",
        "            'Min': data.min(),\n",
        "            'Max': data.max(),\n",
        "            'Q1': data.quantile(0.25),\n",
        "            'Q3': data.quantile(0.75),\n",
        "            'IQR': data.quantile(0.75) - data.quantile(0.25),\n",
        "            'Skewness': stats.skew(data),\n",
        "            'Kurtosis': stats.kurtosis(data)\n",
        "        })\n",
        "\n",
        "# Duration statistics\n",
        "for transcript_type, data in [('First', first_duration / 60), ('Last', last_duration / 60)]:\n",
        "    if len(data) > 0:\n",
        "        stats_data.append({\n",
        "            'Metric': f'{transcript_type} Transcript Duration (min)',\n",
        "            'Count': len(data),\n",
        "            'Mean': data.mean(),\n",
        "            'Median': data.median(),\n",
        "            'Std_Dev': data.std(),\n",
        "            'Min': data.min(),\n",
        "            'Max': data.max(),\n",
        "            'Q1': data.quantile(0.25),\n",
        "            'Q3': data.quantile(0.75),\n",
        "            'IQR': data.quantile(0.75) - data.quantile(0.25),\n",
        "            'Skewness': stats.skew(data),\n",
        "            'Kurtosis': stats.kurtosis(data)\n",
        "        })\n",
        "\n",
        "stats_df = pd.DataFrame(stats_data)\n",
        "if len(stats_df) > 0:\n",
        "    # Round numeric columns for better display\n",
        "    numeric_cols = stats_df.select_dtypes(include=[np.number]).columns\n",
        "    stats_df[numeric_cols] = stats_df[numeric_cols].round(2)\n",
        "    \n",
        "    print(\"\\nüìà COMPREHENSIVE STATISTICS TABLE:\")\n",
        "    print(stats_df.to_string(index=False))\n",
        "    \n",
        "    # Save statistics table\n",
        "    stats_df.to_csv(ANALYSIS_DIR / 'transcript_statistics.csv', index=False)\n",
        "    print(f\"\\nüíæ Statistics table saved to: {ANALYSIS_DIR / 'transcript_statistics.csv'}\")\n",
        "\n",
        "# Correlation analysis\n",
        "print(\"\\nüìä CORRELATION ANALYSIS:\")\n",
        "correlation_cols = [\n",
        "    'First Audio Transcript Word Count',\n",
        "    'Last Audio Transcript Word Count', \n",
        "    'First Audio Transcript Estimated Duration Seconds',\n",
        "    'Last Audio Transcript Estimated Duration Seconds',\n",
        "    'First Audio Transcript Speaker Count',\n",
        "    'Last Audio Transcript Speaker Count'\n",
        "]\n",
        "\n",
        "# Filter to existing columns\n",
        "available_corr_cols = [col for col in correlation_cols if col in df_clean.columns]\n",
        "if len(available_corr_cols) >= 2:\n",
        "    # Use unique identifiers for correlation to avoid duplicate rows\n",
        "    if 'base_id' in df_clean.columns:\n",
        "        unique_df = df_clean.drop_duplicates(subset='base_id')\n",
        "    else:\n",
        "        unique_df = df_clean\n",
        "\n",
        "    corr_matrix = unique_df[available_corr_cols].corr()\n",
        "    print(\"\\nüîó Correlation Matrix:\")\n",
        "    print(corr_matrix.round(3).to_string())\n",
        "    \n",
        "    # Create correlation heatmap\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
        "                square=True, mask=mask, fmt='.3f', cbar_kws={'shrink': 0.8})\n",
        "    plt.title('Transcript Metrics Correlation Matrix', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(FIGURES_DIR / 'correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    # Save correlation matrix\n",
        "    corr_matrix.to_csv(ANALYSIS_DIR / 'correlation_matrix.csv')\n",
        "    print(f\"üíæ Correlation matrix saved to: {ANALYSIS_DIR / 'correlation_matrix.csv'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c833c92",
      "metadata": {
        "id": "outlier_analysis"
      },
      "outputs": [],
      "source": [
        "# --- Comprehensive Outlier Analysis ----------------------------------------\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üö® COMPREHENSIVE OUTLIER ANALYSIS\")\n",
        "\n",
        "def comprehensive_outlier_analysis(data, name, df_subset=None, identifier_col='School_Clip'):\n",
        "    \"\"\"Perform comprehensive outlier analysis with multiple methods.\"\"\"\n",
        "    if len(data) < 3:\n",
        "        print(f\"\\n‚ö†Ô∏è {name}: Insufficient data for outlier analysis\")\n",
        "        return {}\n",
        "    \n",
        "    print(f\"\\nüîç {name.upper()} OUTLIER ANALYSIS:\")\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    # 1. Z-Score Method\n",
        "    z_scores = np.abs(stats.zscore(data))\n",
        "    z_outliers = z_scores > OUTLIER_THRESHOLD\n",
        "    results['z_score_outliers'] = z_outliers.sum()\n",
        "    \n",
        "    print(f\"   üìä Z-Score Method (>{OUTLIER_THRESHOLD} std devs):\")\n",
        "    print(f\"      Outliers: {z_outliers.sum()} ({z_outliers.mean()*100:.1f}%)\")\n",
        "    \n",
        "    if z_outliers.sum() > 0:\n",
        "        outlier_values = data[z_outliers]\n",
        "        print(f\"      Values: {sorted(outlier_values.values)[:10]}\")\n",
        "        if len(outlier_values) > 10:\n",
        "            print(f\"      ... and {len(outlier_values) - 10} more\")\n",
        "    \n",
        "    # 2. IQR Method\n",
        "    Q1 = data.quantile(0.25)\n",
        "    Q3 = data.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    iqr_outliers = (data < lower_bound) | (data > upper_bound)\n",
        "    results['iqr_outliers'] = iqr_outliers.sum()\n",
        "    \n",
        "    print(f\"   üìä IQR Method:\")\n",
        "    print(f\"      Valid range: {lower_bound:.1f} - {upper_bound:.1f}\")\n",
        "    print(f\"      Outliers: {iqr_outliers.sum()} ({iqr_outliers.mean()*100:.1f}%)\")\n",
        "    \n",
        "    # 3. Percentile Method (extreme values)\n",
        "    p1 = data.quantile(0.01)\n",
        "    p99 = data.quantile(0.99)\n",
        "    percentile_outliers = (data < p1) | (data > p99)\n",
        "    results['percentile_outliers'] = percentile_outliers.sum()\n",
        "    \n",
        "    print(f\"   üìä Percentile Method (outside 1st-99th percentile):\")\n",
        "    print(f\"      Valid range: {p1:.1f} - {p99:.1f}\")\n",
        "    print(f\"      Outliers: {percentile_outliers.sum()} ({percentile_outliers.mean()*100:.1f}%)\")\n",
        "    \n",
        "    # 4. Very short transcripts (quality concern)\n",
        "    very_short_threshold = 10\n",
        "    very_short = data < very_short_threshold\n",
        "    results['very_short'] = very_short.sum()\n",
        "    \n",
        "    print(f\"   üìä Very Short Transcripts (<{very_short_threshold} words):\")\n",
        "    print(f\"      Count: {very_short.sum()} ({very_short.mean()*100:.1f}%)\")\n",
        "    \n",
        "    if very_short.sum() > 0 and df_subset is not None:\n",
        "        short_indices = data[very_short].index\n",
        "        short_identifiers = df_subset.loc[short_indices, identifier_col].tolist()\n",
        "        print(f\"      Identifiers: {short_identifiers[:10]}\")\n",
        "        if len(short_identifiers) > 10:\n",
        "            print(f\"      ... and {len(short_identifiers) - 10} more\")\n",
        "    \n",
        "    # 5. Extremely long transcripts (potential errors)\n",
        "    very_long_threshold = data.quantile(0.95) * 2  # 2x the 95th percentile\n",
        "    very_long = data > very_long_threshold\n",
        "    results['very_long'] = very_long.sum()\n",
        "    \n",
        "    print(f\"   üìä Very Long Transcripts (>2x 95th percentile = {very_long_threshold:.1f}):\")\n",
        "    print(f\"      Count: {very_long.sum()} ({very_long.mean()*100:.1f}%)\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Analyze outliers for each transcript type\n",
        "outlier_results = {}\n",
        "\n",
        "if len(first_words) > 0:\n",
        "    first_mask = df_clean['First Audio Transcript Word Count'] > 0\n",
        "    first_subset = df_clean[first_mask]\n",
        "    outlier_results['first'] = comprehensive_outlier_analysis(\n",
        "        first_words, \"First Transcript Word Counts\", first_subset\n",
        "    )\n",
        "\n",
        "if len(last_words) > 0:\n",
        "    last_mask = df_clean['Last Audio Transcript Word Count'] > 0\n",
        "    last_subset = df_clean[last_mask]\n",
        "    outlier_results['last'] = comprehensive_outlier_analysis(\n",
        "        last_words, \"Last Transcript Word Counts\", last_subset\n",
        "    )\n",
        "\n",
        "# Create outlier summary\n",
        "print(f\"\\nüìã OUTLIER SUMMARY:\")\n",
        "for transcript_type, results in outlier_results.items():\n",
        "    if results:\n",
        "        print(f\"\\n   {transcript_type.title()} Transcript:\")\n",
        "        for method, count in results.items():\n",
        "            print(f\"     {method.replace('_', ' ').title()}: {count}\")\n",
        "\n",
        "# Identify rows that should be flagged for review\n",
        "problematic_rows = set()\n",
        "\n",
        "# Add very short transcripts\n",
        "if len(first_words) > 0:\n",
        "    first_very_short = first_words[first_words < 10].index\n",
        "    problematic_rows.update(first_very_short)\n",
        "\n",
        "if len(last_words) > 0:\n",
        "    last_very_short = last_words[last_words < 10].index\n",
        "    problematic_rows.update(last_very_short)\n",
        "\n",
        "print(f\"\\nüö© ROWS FLAGGED FOR REVIEW:\")\n",
        "print(f\"   Total flagged rows: {len(problematic_rows)}\")\n",
        "print(f\"   Percentage of dataset: {len(problematic_rows)/len(df_clean)*100:.1f}%\")\n",
        "\n",
        "if problematic_rows and 'School_Clip' in df_clean.columns:\n",
        "    flagged_identifiers = df_clean.loc[list(problematic_rows), 'School_Clip'].tolist()\n",
        "    print(f\"   Sample identifiers: {flagged_identifiers[:10]}\")\n",
        "    if len(flagged_identifiers) > 10:\n",
        "        print(f\"   ... and {len(flagged_identifiers) - 10} more\")\n",
        "    \n",
        "    # Save flagged rows for review\n",
        "    flagged_df = df_clean.loc[list(problematic_rows)]\n",
        "    flagged_df.to_csv(ANALYSIS_DIR / 'flagged_rows_for_review.csv', index=False)\n",
        "    print(f\"\\nüíæ Flagged rows saved to: {ANALYSIS_DIR / 'flagged_rows_for_review.csv'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d46ac911",
      "metadata": {
        "id": "content_analysis"
      },
      "outputs": [],
      "source": [
        "# --- Content and Quality Analysis ------------------------------------------\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üìù CONTENT AND QUALITY ANALYSIS\")\n",
        "\n",
        "# Language analysis\n",
        "print(\"\\nüåç LANGUAGE ANALYSIS:\")\n",
        "for transcript_type in ['First', 'Last']:\n",
        "    lang_col = f'{transcript_type} Audio Transcript Language Code'\n",
        "    prob_col = f'{transcript_type} Audio Transcript Language Probability'\n",
        "    \n",
        "    if lang_col in df_clean.columns:\n",
        "        lang_data = df_clean[df_clean[lang_col].notna() & (df_clean[lang_col] != '')]\n",
        "        \n",
        "        if len(lang_data) > 0:\n",
        "            print(f\"\\n   üìä {transcript_type} Transcript Languages:\")\n",
        "            lang_counts = lang_data[lang_col].value_counts()\n",
        "            for lang, count in lang_counts.items():\n",
        "                pct = count / len(lang_data) * 100\n",
        "                print(f\"      {lang}: {count} ({pct:.1f}%)\")\n",
        "            \n",
        "            # Language confidence analysis\n",
        "            if prob_col in df_clean.columns:\n",
        "                prob_data = lang_data[lang_data[prob_col] > 0][prob_col]\n",
        "                if len(prob_data) > 0:\n",
        "                    print(f\"      Language confidence: mean={prob_data.mean():.3f}, min={prob_data.min():.3f}, max={prob_data.max():.3f}\")\n",
        "                    low_confidence = prob_data[prob_data < 0.8]\n",
        "                    if len(low_confidence) > 0:\n",
        "                        print(f\"      Low confidence (<0.8): {len(low_confidence)} transcripts ({len(low_confidence)/len(prob_data)*100:.1f}%)\")\n",
        "\n",
        "# Speaker analysis\n",
        "print(\"\\nüë• SPEAKER ANALYSIS:\")\n",
        "speaker_data = []\n",
        "for transcript_type in ['First', 'Last']:\n",
        "    speaker_col = f'{transcript_type} Audio Transcript Speaker Count'\n",
        "    \n",
        "    if speaker_col in df_clean.columns:\n",
        "        speakers = df_clean[df_clean[speaker_col] > 0][speaker_col]\n",
        "        \n",
        "        if len(speakers) > 0:\n",
        "            print(f\"\\n   üìä {transcript_type} Transcript Speakers:\")\n",
        "            speaker_counts = speakers.value_counts().sort_index()\n",
        "            for num_speakers, count in speaker_counts.items():\n",
        "                pct = count / len(speakers) * 100\n",
        "                print(f\"      {num_speakers} speaker(s): {count} ({pct:.1f}%)\")\n",
        "            \n",
        "            speaker_data.extend([(transcript_type, s) for s in speakers])\n",
        "            \n",
        "            print(f\"      Mean speakers: {speakers.mean():.2f}\")\n",
        "            print(f\"      Max speakers: {speakers.max()}\")\n",
        "\n",
        "# Audio events analysis\n",
        "print(\"\\nüîä AUDIO EVENTS ANALYSIS:\")\n",
        "for transcript_type in ['First', 'Last']:\n",
        "    events_col = f'{transcript_type} Audio Transcript Has Audio Events'\n",
        "    \n",
        "    if events_col in df_clean.columns:\n",
        "        events_data = df_clean[events_col]\n",
        "        if len(events_data) > 0:\n",
        "            has_events = events_data.sum() if events_data.dtype == bool else (events_data == True).sum()\n",
        "            total = len(events_data)\n",
        "            print(f\"   üìä {transcript_type} Transcript Audio Events:\")\n",
        "            print(f\"      Transcripts with audio events: {has_events} ({has_events/total*100:.1f}%)\")\n",
        "            print(f\"      Transcripts without audio events: {total-has_events} ({(total-has_events)/total*100:.1f}%)\")\n",
        "\n",
        "# Quality score calculation\n",
        "print(\"\\n‚≠ê QUALITY SCORE CALCULATION:\")\n",
        "def calculate_quality_score(row, transcript_type):\n",
        "    \"\"\"Calculate a composite quality score for a transcript.\"\"\"\n",
        "    score = 0\n",
        "    max_score = 0\n",
        "    \n",
        "    # Word count component (40% of score)\n",
        "    word_col = f'{transcript_type} Audio Transcript Word Count'\n",
        "    if word_col in row.index and not pd.isna(row[word_col]):\n",
        "        words = row[word_col]\n",
        "        if words > 0:\n",
        "            # Score based on length: full points for 50+ words, scaled down below that\n",
        "            word_score = min(words / 50, 1) * 40\n",
        "            score += word_score\n",
        "        max_score += 40\n",
        "    \n",
        "    # Language confidence component (30% of score)\n",
        "    prob_col = f'{transcript_type} Audio Transcript Language Probability'\n",
        "    if prob_col in row.index and not pd.isna(row[prob_col]):\n",
        "        prob = row[prob_col]\n",
        "        if prob > 0:\n",
        "            score += prob * 30\n",
        "        max_score += 30\n",
        "    \n",
        "    # Speaker detection component (20% of score)\n",
        "    speaker_col = f'{transcript_type} Audio Transcript Speaker Count'\n",
        "    if speaker_col in row.index and not pd.isna(row[speaker_col]):\n",
        "        speakers = row[speaker_col]\n",
        "        if speakers > 0:\n",
        "            score += 20  # Full points for any speaker detection\n",
        "        max_score += 20\n",
        "    \n",
        "    # Audio events component (10% of score)\n",
        "    events_col = f'{transcript_type} Audio Transcript Has Audio Events'\n",
        "    if events_col in row.index and not pd.isna(row[events_col]):\n",
        "        has_events = row[events_col]\n",
        "        if has_events:\n",
        "            score += 10\n",
        "        max_score += 10\n",
        "    \n",
        "    return score / max_score * 100 if max_score > 0 else 0\n",
        "\n",
        "# Calculate quality scores\n",
        "for transcript_type in ['First', 'Last']:\n",
        "    quality_col = f'{transcript_type}_Quality_Score'\n",
        "    df_clean[quality_col] = df_clean.apply(\n",
        "        lambda row: calculate_quality_score(row, transcript_type), axis=1\n",
        "    )\n",
        "    \n",
        "    quality_scores = df_clean[df_clean[quality_col] > 0][quality_col]\n",
        "    if len(quality_scores) > 0:\n",
        "        print(f\"\\n   üìä {transcript_type} Transcript Quality Scores:\")\n",
        "        print(f\"      Mean: {quality_scores.mean():.1f}%\")\n",
        "        print(f\"      Median: {quality_scores.median():.1f}%\")\n",
        "        print(f\"      Min: {quality_scores.min():.1f}%\")\n",
        "        print(f\"      Max: {quality_scores.max():.1f}%\")\n",
        "        \n",
        "        # Quality categories\n",
        "        excellent = (quality_scores >= 90).sum()\n",
        "        good = ((quality_scores >= 70) & (quality_scores < 90)).sum()\n",
        "        fair = ((quality_scores >= 50) & (quality_scores < 70)).sum()\n",
        "        poor = (quality_scores < 50).sum()\n",
        "        \n",
        "        total = len(quality_scores)\n",
        "        print(f\"      Quality Distribution:\")\n",
        "        print(f\"        Excellent (90-100%): {excellent} ({excellent/total*100:.1f}%)\")\n",
        "        print(f\"        Good (70-89%): {good} ({good/total*100:.1f}%)\")\n",
        "        print(f\"        Fair (50-69%): {fair} ({fair/total*100:.1f}%)\")\n",
        "        print(f\"        Poor (<50%): {poor} ({poor/total*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "703a7593",
      "metadata": {
        "id": "comparative_analysis"
      },
      "outputs": [],
      "source": [
        "# --- Comparative Analysis: First vs Last Transcripts -----------------------\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üîÑ COMPARATIVE ANALYSIS: FIRST vs LAST TRANSCRIPTS\")\n",
        "\n",
        "# Statistical comparison\n",
        "print(\"\\nüìä STATISTICAL COMPARISON:\")\n",
        "\n",
        "# Word count comparison\n",
        "if len(first_words) > 0 and len(last_words) > 0:\n",
        "    print(\"\\n   üìù Word Count Comparison:\")\n",
        "    print(f\"      First Transcripts: mean={first_words.mean():.1f}, median={first_words.median():.1f}, std={first_words.std():.1f}\")\n",
        "    print(f\"      Last Transcripts:  mean={last_words.mean():.1f}, median={last_words.median():.1f}, std={last_words.std():.1f}\")\n",
        "    \n",
        "    # Statistical test\n",
        "    if len(first_words) > 1 and len(last_words) > 1:\n",
        "        statistic, p_value = stats.mannwhitneyu(first_words, last_words, alternative='two-sided')\n",
        "        print(f\"      Mann-Whitney U test p-value: {p_value:.6f}\")\n",
        "        if p_value < 0.05:\n",
        "            print(f\"      ‚úÖ Statistically significant difference (p < 0.05)\")\n",
        "        else:\n",
        "            print(f\"      ‚ùå No statistically significant difference (p >= 0.05)\")\n",
        "\n",
        "# Duration comparison\n",
        "if len(first_duration) > 0 and len(last_duration) > 0:\n",
        "    first_dur_min = first_duration / 60\n",
        "    last_dur_min = last_duration / 60\n",
        "    \n",
        "    print(\"\\n   ‚è±Ô∏è Duration Comparison:\")\n",
        "    print(f\"      First Transcripts: mean={first_dur_min.mean():.2f} min, median={first_dur_min.median():.2f} min\")\n",
        "    print(f\"      Last Transcripts:  mean={last_dur_min.mean():.2f} min, median={last_dur_min.median():.2f} min\")\n",
        "\n",
        "# Speaker comparison\n",
        "first_speakers = df_clean[df_clean['First Audio Transcript Speaker Count'] > 0]['First Audio Transcript Speaker Count']\n",
        "last_speakers = df_clean[df_clean['Last Audio Transcript Speaker Count'] > 0]['Last Audio Transcript Speaker Count']\n",
        "\n",
        "if len(first_speakers) > 0 and len(last_speakers) > 0:\n",
        "    print(\"\\n   üë• Speaker Count Comparison:\")\n",
        "    print(f\"      First Transcripts: mean={first_speakers.mean():.2f}, median={first_speakers.median():.1f}\")\n",
        "    print(f\"      Last Transcripts:  mean={last_speakers.mean():.2f}, median={last_speakers.median():.1f}\")\n",
        "\n",
        "# Quality score comparison\n",
        "if 'First_Quality_Score' in df_clean.columns and 'Last_Quality_Score' in df_clean.columns:\n",
        "    first_quality = df_clean[df_clean['First_Quality_Score'] > 0]['First_Quality_Score']\n",
        "    last_quality = df_clean[df_clean['Last_Quality_Score'] > 0]['Last_Quality_Score']\n",
        "    \n",
        "    if len(first_quality) > 0 and len(last_quality) > 0:\n",
        "        print(\"\\n   ‚≠ê Quality Score Comparison:\")\n",
        "        print(f\"      First Transcripts: mean={first_quality.mean():.1f}%, median={first_quality.median():.1f}%\")\n",
        "        print(f\"      Last Transcripts:  mean={last_quality.mean():.1f}%, median={last_quality.median():.1f}%\")\n",
        "\n",
        "# Paired analysis (rows with both transcripts)\n",
        "both_mask = (df_clean['First Audio Transcript Word Count'] > 0) & (df_clean['Last Audio Transcript Word Count'] > 0)\n",
        "paired_data = df_clean[both_mask]\n",
        "\n",
        "if len(paired_data) > 0:\n",
        "    # For paired analysis, use one row per identifier to avoid duplicates\n",
        "    paired_unique = paired_data.drop_duplicates(subset='base_id')\n",
        "    print(f\"\\nüîó PAIRED ANALYSIS ({len(paired_unique)} unique identifiers with both transcripts):\")\n",
        "    \n",
        "    # Word count correlation\n",
        "    first_paired_words = paired_unique['First Audio Transcript Word Count']\n",
        "    last_paired_words = paired_unique['Last Audio Transcript Word Count']\n",
        "    \n",
        "    correlation = first_paired_words.corr(last_paired_words)\n",
        "    print(f\"   üìä Word count correlation: {correlation:.3f}\")\n",
        "    \n",
        "    # Difference analysis\n",
        "    word_diff = last_paired_words - first_paired_words\n",
        "    print(f\"   üìè Word count differences (Last - First):\")\n",
        "    print(f\"      Mean difference: {word_diff.mean():.1f} words\")\n",
        "    print(f\"      Median difference: {word_diff.median():.1f} words\")\n",
        "    print(f\"      Std deviation: {word_diff.std():.1f} words\")\n",
        "    \n",
        "    # Consistency analysis\n",
        "    similar_length = np.abs(word_diff) <= 10  # Within 10 words\n",
        "    print(f\"   üéØ Length consistency (within 10 words): {similar_length.sum()} ({similar_length.mean()*100:.1f}%)\")\n",
        "    \n",
        "    # Direction analysis\n",
        "    last_longer = word_diff > 0\n",
        "    first_longer = word_diff < 0\n",
        "    same_length = word_diff == 0\n",
        "    \n",
        "    print(f\"   üìà Length patterns:\")\n",
        "    print(f\"      Last transcript longer: {last_longer.sum()} ({last_longer.mean()*100:.1f}%)\")\n",
        "    print(f\"      First transcript longer: {first_longer.sum()} ({first_longer.mean()*100:.1f}%)\")\n",
        "    print(f\"      Same length: {same_length.sum()} ({same_length.mean()*100:.1f}%)\")\n",
        "\n",
        "# Create comparison visualization\n",
        "if len(first_words) > 0 or len(last_words) > 0:\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    \n",
        "    # Word count comparison\n",
        "    axes[0,0].hist(first_words, alpha=0.7, label='First Transcript', bins=30)\n",
        "    axes[0,0].hist(last_words, alpha=0.7, label='Last Transcript', bins=30)\n",
        "    axes[0,0].set_title('Word Count Distribution Comparison', fontweight='bold')\n",
        "    axes[0,0].set_xlabel('Word Count')\n",
        "    axes[0,0].set_ylabel('Frequency')\n",
        "    axes[0,0].legend()\n",
        "    axes[0,0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Box plot comparison\n",
        "    comparison_data = []\n",
        "    comparison_labels = []\n",
        "    if len(first_words) > 0:\n",
        "        comparison_data.append(first_words.values)\n",
        "        comparison_labels.append('First')\n",
        "    if len(last_words) > 0:\n",
        "        comparison_data.append(last_words.values)\n",
        "        comparison_labels.append('Last')\n",
        "    \n",
        "    if comparison_data:\n",
        "        axes[0,1].boxplot(comparison_data, labels=comparison_labels)\n",
        "        axes[0,1].set_title('Word Count Box Plot Comparison', fontweight='bold')\n",
        "        axes[0,1].set_ylabel('Word Count')\n",
        "        axes[0,1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Scatter plot for paired data\n",
        "    if len(paired_unique) > 0:\n",
        "        axes[1,0].scatter(paired_unique['First Audio Transcript Word Count'], \n",
        "                         paired_unique['Last Audio Transcript Word Count'],\n",
        "                        alpha=0.6)\n",
        "        \n",
        "        # Add diagonal line for perfect correlation\n",
        "        max_val = max(paired_unique['First Audio Transcript Word Count'].max(), paired_unique['Last Audio Transcript Word Count'].max())\n",
        "        axes[1,0].plot([0, max_val], [0, max_val], 'r--', alpha=0.7, label='Perfect Correlation')\n",
        "        \n",
        "        axes[1,0].set_title(f'Paired Word Counts (r={correlation:.3f})', fontweight='bold')\n",
        "        axes[1,0].set_xlabel('First Transcript Word Count')\n",
        "        axes[1,0].set_ylabel('Last Transcript Word Count')\n",
        "        axes[1,0].legend()\n",
        "        axes[1,0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Difference histogram\n",
        "    if len(paired_unique) > 0:\n",
        "        axes[1,1].hist(word_diff, bins=30, alpha=0.7, edgecolor='black')\n",
        "        axes[1,1].axvline(0, color='red', linestyle='--', label='No Difference')\n",
        "        axes[1,1].axvline(word_diff.mean(), color='blue', linestyle='--', label=f'Mean: {word_diff.mean():.1f}')\n",
        "        axes[1,1].set_title('Word Count Differences (Last - First)', fontweight='bold')\n",
        "        axes[1,1].set_xlabel('Difference in Word Count')\n",
        "        axes[1,1].set_ylabel('Frequency')\n",
        "        axes[1,1].legend()\n",
        "        axes[1,1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(FIGURES_DIR / 'first_vs_last_comparison.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n‚úÖ Comparison visualization created and saved\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86957bd5",
      "metadata": {
        "id": "final_summary"
      },
      "outputs": [],
      "source": [
        "# --- Final Summary and Recommendations -------------------------------------\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üìã FINAL SUMMARY AND RECOMMENDATIONS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Dataset summary\n",
        "print(f\"\\nüìä DATASET SUMMARY:\")\n",
        "print(f\"   üìÅ Original rows: {initial_row_count:,}\")\n",
        "print(f\"   üìÅ Final cleaned rows: {len(df_clean):,}\")\n",
        "print(f\"   üóëÔ∏è Removed rows: {removed_count:,} ({removed_count/initial_row_count*100:.1f}%)\")\n",
        "print(f\"   üìà Data retention rate: {len(df_clean)/initial_row_count*100:.1f}%\")\n",
        "\n",
        "# Transcript availability summary\n",
        "first_count = (df_clean['First Audio Transcript Word Count'] > 0).sum()\n",
        "last_count = (df_clean['Last Audio Transcript Word Count'] > 0).sum()\n",
        "both_count = ((df_clean['First Audio Transcript Word Count'] > 0) & \n",
        "              (df_clean['Last Audio Transcript Word Count'] > 0)).sum()\n",
        "\n",
        "print(f\"\\nüó£Ô∏è TRANSCRIPT AVAILABILITY:\")\n",
        "print(f\"   üìù First transcripts available: {first_count:,} ({first_count/len(df_clean)*100:.1f}%)\")\n",
        "print(f\"   üìù Last transcripts available: {last_count:,} ({last_count/len(df_clean)*100:.1f}%)\")\n",
        "print(f\"   üìù Both transcripts available: {both_count:,} ({both_count/len(df_clean)*100:.1f}%)\")\n",
        "print(f\"   üìù Total transcript instances: {first_count + last_count:,}\")\n",
        "print(f\"üíæ Final analysis summary saved to: {ANALYSIS_DIR / 'cleaning_summary.json'}\")\n",
        "print(f\"‚úÖ ANALYSIS COMPLETE!\")\n",
        "print(f\"   üéâ Data cleaning and EDA successfully completed\")\n",
        "print(f\"   üìä {len(df_clean):,} clean rows ready for further analysis\")\n",
        "print(f\"   üìà Comprehensive visualizations and statistics generated\")\n",
        "print(f\"   üîç Quality assessment and outlier detection completed\")\n",
        "# Final data validation check\n",
        "print(f\"üî¨ FINAL DATA VALIDATION:\")\n",
        "print(f\"   üìä Total cleaned rows: {len(df_clean):,}\")\n",
        "print(f\"   üìù Rows with first transcripts: {(df_clean['First Audio Transcript Word Count'] > 0).sum():,}\")\n",
        "print(f\"   üìù Rows with last transcripts: {(df_clean['Last Audio Transcript Word Count'] > 0).sum():,}\")\n",
        "print(f\"   ‚ö†Ô∏è Rows flagged for review: {len(problematic_rows) if 'problematic_rows' in locals() else 0:,}\")\n",
        "print(f\"   ‚úÖ Data integrity: {'PASSED' if len(df_clean) > 0 else 'FAILED'}\")\n",
        "print(f\"=\" * 60)\n",
        "print(f\"üéä SUCCESS! Your Peru transcript dataset is now cleaned and analyzed!\")\n",
        "print(f\"üìÅ Check the '{ANALYSIS_DIR.name}' folder for all outputs and visualizations.\")\n",
        "print(f\"=\" * 60)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Harvard",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
