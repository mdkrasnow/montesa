# data structure:

## Headers:

School_Clip,Teacher provides learning activity - 1st Snapshot,Students are on task - 1st Snapshot,Teacher provides learning activity - 2nd Snapshot,Students are on task - 2nd Snapshot,Teacher provides learning activity - 3rd Snapshot,Students are on task - 3rd Snapshot,Supportive Learning Environment,The teacher treats all students respectfully,The teacher uses positive language,The teacher responds to students needs,The teacher does not exhibit gender bias,Positive Behavioral Expectations,The teacher sets clear behavioral expectations,The teacher acknowledges positive student behavior,The teacher redirects misbehavior,Lesson Facilitation,The teacher explicitly articulates learning objectives,The teacher's explanation of content is clear,The teacher makes connections in the lesson,The teacher models by enacting or thinking aloud,Checks for understanding,The teacher uses questions,The teacher uses prompts,The teacher monitors most students,The teacher adjusts teaching to the level of students,Feedback,The teacher provides specific comments for misunderstandings,The teacher provides specific comments for successes,Critical Thinking,The teacher asks open-ended questions,The teacher provides thinking tasks,Students ask open-ended questions or perform thinking tasks,Autonomy,The teacher provides students with choices,The teacher provides students with opportunities to take meaningful roles,Students volunteer to participate in the classroom,Perseverance,The teacher acknowledges students' efforts,The teacher has a positive attitude towards students' challenges,The teacher encourages goal-setting,Social & Collaborative Skills,The teacher promotes students' collaboration,The teacher promotes students' interpersonal skills,Source Table,Context,First Video Clip,First Audio Clip,Last Audio Clip,last_progress_save,First Audio Transcript,First Audio Transcript_JSON,First Audio Transcript Text,First Audio Transcript Language Code,First Audio Transcript Language Probability,First Audio Transcript Word Count,First Audio Transcript Duration Seconds,First Audio Transcript Speaker Count,First Audio Transcript Has Audio Events,Last Audio Transcript,Last Audio Transcript_JSON,Last Audio Transcript Text,Last Audio Transcript Language Code,Last Audio Transcript Language Probability,Last Audio Transcript Word Count,Last Audio Transcript Duration Seconds,Last Audio Transcript Speaker Count,Last Audio Transcript Has Audio Events,base_id,First Audio Transcript Estimated Duration Seconds,Last Audio Transcript Estimated Duration Seconds,score_distribution,split


# Information on how we want the evaluation to work:

Okay, so essentially for this code we need to change the run evaluation functionality and logic. So first off, our main goal is to create two evaluations. One for the first clip and one for the second clip for each identifier because the data comes in pairs per identifier. So there's two rows of data, each one corresponding to either the first clip or the second clip. And both rows contain the transcript information for both the first clip and the second clip. So just so you know the data structure there. The entire goal is to create a new evaluation data set and data file for each model that we're running. And we want to do this by running the evaluate function that is on each of the classes for the models. So these models all have an evaluate function which generates an evaluation based on the specified framework and returns the entire generated evaluation with all of the components. And essentially what we want to do is we want to then reparse. So we want to call this evaluate function for each row in the original data file properly using the write transcript and audio file for the write clip. So if it's clip one we want to use the transcript and audio file from the first clip and the same for clip two. So we want to use the second transcript audio file for the identifier is clip two. And then once we have this new data that is generated we want to parse the JSON output and store it in a new data file which contains just the evaluation data, the identifiers and any other metadata necessary and the model that which these were generated on. And then we want to compare on a row by row basis on the distance between each of the human evaluations which is the original data set and then each of the models as generations and then provide summary statistics and etc. for that which we attempt to do in the original evaluation notebook. But right now the current method does not do this. It doesn't actually run the evaluate function properly. It doesn't know the write output structure which needs to be fixed. And essentially what I will provide you with is the teach one JSON which is the framework for which we will be evaluating and running evaluations. And I will provide you with the cleaning and EDA template as well so that you can identify all of these code to see what the structure of the data is, what is the structure of these models. So I'll provide you with the base model evaluator which is a good representation of what the models are structured from a high level. There are only very small minor implementation details that are differences. But essentially the evaluate function or evaluate function will return the same structure for all the models. And then we want to parse this, put it back into a new data frame and then save it as CSV and then compute all of our statistics etc. from that. So, yeah.
