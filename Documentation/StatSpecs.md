Thanks for the clarification. I’ll now begin a detailed research task to deeply examine your codebase and evaluation data files, especially the notebooks and CSVs in `model_evaluation_data`, to understand the model comparison framework and build an extremely thorough plan. This plan will:

* Identify and characterize all models automatically from the file structure.
* Explore your custom distance function and how it’s used to evaluate models.
* Propose a full statistical comparison framework across test, training, and validation sets, using pairwise and baseline comparisons with false positive control.
* Recommend suitable statistical tests based on the actual evaluation metric distributions.
* Suggest visualizations to illustrate statistically significant improvements.
* Recommend reliability metrics and identify how to report consistency across runs.

I’ll let you know once this deep dive and plan are ready for your review.


# Plan for Comparative Data Analysis of Model Performance

## Goals and Hypotheses

Our goal is to **thoroughly analyze and compare the performance of all six AI evaluator models** in order to substantiate key claims for the research paper. We aim to demonstrate the hypothesis that **“AI can achieve reliable teaching evaluations, but only when used properly (with a high-quality rubric and appropriate model training)”**. In particular, we expect the **base model with a poorly constructed rubric** (the one using `low_Teach_1.json`) to perform significantly worse than the improved models. This would illustrate that a naive use of AI (with minimal prompt engineering or rubric design) yields subpar alignment with human evaluations, whereas properly designed rubrics and training dramatically improve reliability. We will verify whether there are **statistically significant differences in performance between the models** on a held-out test set of classroom observations. If the improvements are real, the advanced models should consistently outperform the baseline on the test data, whereas if differences are not significant, it suggests the improvements might be negligible or due to noise. Additionally, we will check performance on the training/validation set to ensure that gains observed are not due to overfitting or peculiarities in one dataset. Overall, our analysis will gather evidence that **the best-performing AI models meet or exceed human-level reliability criteria**, reinforcing the message that AI-based evaluators *can* be as good as human raters when configured correctly. Below we outline a detailed plan, including model descriptions, metrics to evaluate, statistical tests, and visualizations to support these claims.

## Model Variants and Configurations

We have **six distinct model configurations** (labelled 1 through 6) that were developed and need to be compared. Each model differs in the rubric used, the training data preprocessing, or the underlying AI model version. According to the repository documentation, the models and their specifications are as follows:

* **Model 1: 1-BaseEvaluator\_evaluations** – This is the **baseline model**. It uses the **“low\_Teach\_1.json” rubric**, which was generated naively by feeding the Teach 1.0 manual through an AI (Claude) with minimal prompt engineering. This rubric allows N/A values for any component and is considered low-quality (essentially a first draft). The model uses the *Gemini 2.0 (flash)* LLM and was applied to the full set of cleaned Peru transcripts. We expect this model to have the lowest performance, since the rubric it relies on is coarse and not well-aligned with expert definitions.

* **Model 2: 2-BaseEvaluator-Validate** – This model uses an **improved rubric “med\_Teach\_1.json”**, which has better prompt-engineered descriptions and is more grounded in the Teach manual content. It still allows N/A for components but is a step up in rubric quality. The underlying AI model remains *Gemini 2.0*. It is applied to the same Peru dataset of transcripts as Model 1. We expect Model 2 to outperform Model 1 due to the more thoughtfully constructed rubric, which should guide the AI to closer human-aligned judgments.

* **Model 3: 3-BaseEvaluator-Validate** – This variant uses the **“high\_Teach\_1.json” rubric**, representing a further refined rubric. The high-quality rubric was manually polished for formatting and clarity, and it incorporates distributional information from the training data while **disallowing any N/A scores**. Model 3 still uses *Gemini 2.0* and was run on the full cleaned transcript dataset (including ones with potential N/A in human labels). We anticipate Model 3 to be better than Model 2, since the rubric is the best-curated version, setting a clearer standard for the AI. The difference between Model 2 and Model 3 isolates the impact of rubric quality (medium vs. high) on performance.

* **Model 4: 4-BaseEvaluator-Validate-no-NA-data** – This model uses the same **high-quality rubric** as Model 3 but is applied on a filtered dataset with **no N/A values (RMNA\_cleaned\_transcripts)**. Essentially, any transcript segments that had missing human ratings or “N/A” in certain components were removed from training/evaluation for this model. It still uses *Gemini 2.0*. The idea is that by training/evaluating only on complete data (no missing labels), the model might learn more consistently and not be confused by undefined components. We expect Model 4 to show a further performance improvement over Model 3, if handling N/A values was a significant issue. In other words, Model 4 tests whether excluding incomplete data leads to a more reliable model.

* **Model 5: 5-BaseEvaluator-Validate-no-NA-data-Gemini-2.5-flash** – This model builds on Model 4’s setup (high-quality rubric, no-NA dataset) but swaps in a **more capable LLM, Gemini 2.5 (flash)**, instead of Gemini 2.0. All else being equal, a newer/larger model should better understand the context and rubric, potentially yielding higher agreement with human evaluations. Model 5 is expected to be among the top performers, illustrating the benefit of using a stronger AI model once the rubric and data are optimal. The difference between Model 4 and Model 5 isolates the impact of the model architecture upgrade (2.0 vs 2.5) on evaluation accuracy.

* **Model 6: 6-BaseEvaluator-Validate-timestamped-data-g-20-flash** – This final model variant returns to *Gemini 2.0* but introduces **formatted transcripts with timestamps** (TIMESTAMPED\_cleaned\_transcripts) as input. It uses the same high-quality rubric as Models 3–5. The inclusion of timestamps means the transcript text is augmented with temporal markers and possibly structured turn-taking info, which might help the model interpret context (for example, knowing when in the lesson certain behaviors occur). Model 6 tests whether formatting the input data for better context comprehension can improve the model’s performance with the rubric, even without upgrading the model architecture. We expect Model 6 to perform similarly to Model 5 if the formatting helps, though Model 5 might still have an edge due to the more powerful LLM.

In summary, these six models represent a sequence of incremental improvements:

1. **Baseline:** naive rubric, base model.
2. **Better Rubric (med):** moderate rubric improvements.
3. **Best Rubric (high):** fully refined rubric.
4. **No-NA Data:** removing ambiguous data issues.
5. **Stronger LLM:** using a more capable model.
6. **Formatted Input:** improving input representation.

This ordered progression will allow us to analyze how each change affects performance. **All six models were evaluated on a common set of classroom audio transcripts with human-coded Teach framework scores**, enabling direct comparisons. The repository confirms the list of evaluated models, and our analysis will leverage the results saved for each model (in the `model_evaluation_data` and `model_reports` files). By examining these, we can extract each model’s agreement with human evaluations and see the performance trend from Model 1 through Model 6.

## Data Sets: Training/Validation vs. Test

It is important to distinguish between the **training/validation data** used during model development and the **held-out test data** used for final evaluation. In this project, the AI models were fine-tuned or configured using real observation data, and then evaluated on a set of human-scored classroom sessions for validation. We need to confirm how “Train/Validation” and “Test” are defined in this context:

* **Training Data:** The models were primarily trained on a large corpus of real observation data. This training set would have exact human labels. There may also have been a **validation subset** of real data used during development to tune parameters (for example, to decide on rubric tweaks or to do early stopping in fine-tuning). If any of the real human-labeled observations were used in that process, they would be considered a “validation” set. However, to ensure a fair test, it sounds like the team set aside a portion of real classroom transcripts as a **test set** for final evaluation only.

* **Test Set:** This is a set of **held-out classroom transcripts with human evaluations** that were *not* seen by the models during training or rubric development. The test set is used to measure how well each AI model replicates human observer scores on new data. From the repository, it appears the Peru dataset of human-scored transcripts was used for validation; for example, we see 40 instances in the evaluation reports, suggesting the test/validation set comprised 40 classroom “segments” (each segment roughly a 15-minute video observation). We will treat these 40 segments as our primary evaluation set for computing model vs. human agreement. It’s likely that these 40 segments themselves were the reliability exam videos (or drawn from them), given the structure of the Teach reliability test (3 segments per exam attempt, possibly multiple attempts). In any case, these represent the ground-truth labels to test the AI. We will confirm if all models were evaluated on the exact same 40 segments – the evidence suggests yes (Model 1’s report and Model 6’s report both show count = 40).

By comparing these, we can check if any model has a large drop in performance from training to test, which would indicate overfitting or reliance on training-specific cues. Ideally, a well-generalized model should perform nearly as well on test data as it did on validation data. We will include both in our analysis to show that the improvements are **tangible on unseen data** and not just artifacts of over-training.

## Metrics for Performance Comparison

To rigorously compare the models, we will compute several **performance metrics** that capture how close each model’s evaluations are to the human (“master”) evaluations. All metrics will be computed for each model on the same dataset (especially the test set). Key metrics include:

* **Overall Distance:** This is a custom metric defined in our code that quantifies the average discrepancy between the model’s ratings and the human ratings across all rubric items. Each rubric component (e.g. “The teacher encourages goal-setting”) has an ordinal or binary value in the Teach framework (for example, 1–5 scale for high-inference domains, or Yes/No for binary snapshots). The distance function converts the model’s output and human’s output for each component into numeric form and computes how far apart they are. For binary snapshot items, “distance” might be 0 if they match or 1 if they differ; for 5-point scale items, distance could be the absolute difference normalized to \[0,1] (so a difference of 1 point out of 4 possible is 0.25, etc.). The overall distance is then averaged over all components. A distance of 0 would indicate perfect agreement on everything, while 1 would indicate maximal disagreement. In practice, we see moderate values: e.g. Model 1 (baseline) had a mean overall distance \~0.530, whereas Model 6 had \~0.343. Lower overall distance means the model is closer to human evaluation. We will use this as a primary metric (lower is better) and statistically compare distances between models.

* **LLM-Human Agreement (Fraction of Agreement):** This is essentially the complement of the distance metric, representing the proportion of rubric items on which the model and human **agree** (either exactly or within an allowable difference). In the model reports, *agreement* appears as 1 – distance (since Model 1 shows \~0.470 agreement meaning \~47% agreement, which complements the \~53% distance, and Model 6 shows \~0.657 or 65.7% agreement complementing 34.3% distance). We will double-check if *agreement* was calculated exactly as “exact matches proportion” or in a weighted way. Given the Teach criteria, it might actually be counting “within one point” as agreement for the high-inference scores. However, the simpler interpretation is likely each item either matches (distance 0) or not (distance >0), thereby giving a fraction agreed. In any case, **higher agreement percentage is better**, and it is directly relatable to accuracy in reproducing human labels. We will use agreement in parallel with distance (they contain the same information, just inverted). For readability, we might report agreement percentages (e.g. “Model 5 agrees with human on \~93% of rubric decisions”) since that is intuitive.

* **Teach **Reliability Exam Pass Rate****: This metric follows the official Teach observer certification criteria. The World Bank’s Teach manual requires an observer to be “reliable” on at least 8 out of 10 elements in a 15-minute segment to pass that segment. The 10 elements include the composite **Time on Task (ToT)** element (which itself requires exact match on 2 of 3 snapshots) and the 9 high-inference domains (which require the observer’s score to be within ±1 of the master score for each domain). We have implemented this logic in our evaluation code: for each segment, we check if the model meets the criteria on ToT and each of the 9 domains, count how many of the 10 are “passed”, and mark the segment as passed if ≥8. The **segment pass rate** is the percentage of segments (out of 40) that the model passed. This is a very tangible measure: it tells us how often the AI would have met the standard required for a human evaluator to be certified on a given video. We have computed this for each model. For example, Model 5’s pass rate is **93.4%** – it passed 93.4% of the segments by the 8/10 rule – whereas Model 1’s pass rate is only **60.9%**. This huge gap (61% vs 93%) is central evidence for our claim that the base model is not reliably meeting human-level performance, while the improved models are. We will use pass rate as a key metric for comparison, since it directly ties to the “reliability exam” standard. Additionally, our code simulated the full certification exam (3 segments per attempt, needing all 3 passed, with up to 2 attempts) for each model. The summary indicates all models eventually got certified within two attempts, but the weaker models would likely fail on the first try. We may derive from segment pass rates the probability of passing an exam in one attempt (which is the cube of the segment pass proportion, assuming random segment draw). For instance, Model 5 with \~93.4% per-segment reliability has about an 81% chance to pass all 3 segments in one go (0.934^3 ≈ 0.817), whereas Model 1 with 60.9% per-segment reliability has only \~22.6% chance to pass all three in one attempt (0.609^3 ≈ 0.226). This stark contrast underlines how “proper use of AI” (Model 5) approaches the consistency of a trained human, while the naive approach would frequently fail the exam. We will present the **pass rate for each model** and use it to illustrate improvements.

* **Domain- and Component-Specific Metrics:** To get deeper insight, we will also analyze performance **per Teach domain and per specific rubric component**. Our evaluation reports provide the average distance on each component for each model. For example, in Model 1’s output, some components like “The teacher responds to students’ needs” had an extremely high average distance (\~0.79), meaning the model often disagreed with the human on that item, while other items like “Teacher does not exhibit gender bias” had a low distance (\~0.15) indicating consistent agreement. In Model 6 (an improved model), we see generally lower distances across components, but notably “responds to students’ needs” is still high (\~0.775), suggesting this particular aspect is inherently challenging for the AI (perhaps due to subtle cues required). We will tabulate or visualize the **distance per component** for each model to see how errors decrease in each area. Likewise, we’ll examine domain-level averages (the framework clusters items into domains like Time on Task, Classroom Culture, Instruction, Socioemotional Skills). The reports show, for instance, Model 1 had highest error in Classroom Culture (avg distance \~0.65) and Time on Task (\~0.62), whereas Model 6 brought Time on Task error down to \~0.29 and Classroom Culture to \~0.34. This indicates major improvement in the consistency of snapshot coding and classroom culture evaluation with the advanced models. By comparing domain-specific performance, we can identify if improvements benefited some areas more than others. We will especially check if **Socioemotional Skills** domain remains relatively harder for the AI across models (often the most subjective elements like encouragement, collaboration, etc. reside there). These granular metrics will be used to support any qualitative claims, such as “the AI improved most in maintaining student engagement (Time on Task) and basic classroom management behaviors, but still struggled in nuanced responsiveness to student needs.”

* **Inter-Rater Reliability (Cohen’s Kappa):** We attempted to calculate Cohen’s kappa for each rubric component as another reliability metric. Kappa measures agreement adjusted for chance, which is useful for categorical ratings. However, in our case, many components have little variation (e.g., if in all 40 segments the human gave the same score or if the model gave nearly the same every time, kappa can be ill-defined). Indeed, the reports show `nan` for every component’s kappa for Model 1 and Model 6, meaning the computation wasn’t feasible (likely due to one of the raters being constant or zero variance in those categories). This is not surprising, as some binary items might have all “Yes” from humans, making kappa undefined. Instead of per-item kappa, we might compute an **overall kappa** or an average kappa for broad categories if possible. Another approach is to compute **weighted kappa for the 9 high-inference domains** combined, treating each domain’s rating pair as an observation – this could yield a single kappa value for each model vs human. If feasible, we will calculate that to report an overall inter-rater reliability figure (for instance, we might find that Model 5 vs Human has a weighted kappa around, say, 0.8 indicating strong agreement, whereas Model 1 might be around 0.4–0.5 indicating moderate agreement). If the data complexity makes kappa hard to interpret, we will lean on the **percent agreement (within one)** which is effectively captured by the pass rate metric discussed above. In summary, we won’t rely on per-item kappa due to data sparsity, but we will mention inter-rater reliability in terms of these other measures (agreement %, etc.) that align with human-to-human reliability expectations.

* **Bias or Systematic Differences:** As an additional analysis, we will check for any systematic bias in the models’ scoring. For example, does the base model tend to over-score or under-score certain domains relative to humans? We can compute the **mean score given by the model vs mean score by human** for each domain to see if there is an offset. If a model consistently gave higher scores on average than the human did, it might indicate leniency bias; if lower, a harshness bias. Also, for binary snapshot items, we can compare how often the model said “Yes” vs the human. This could be visualized as well (e.g., the proportion of segments where the model marked students on-task vs the human proportion). These checks will ensure that improved agreement isn’t hiding some compensation (like always giving middle scores to be within one point, etc.). Ideally, a high agreement model should closely mirror the distribution of human scores. We’ll report if any model has notable distribution differences. Given that med\_Teach and high\_Teach rubrics included distribution information to guide the model, we expect the later models to better match human score distributions than the naive Model 1.

In summary, our analysis will quantify **how close each model’s evaluations are to human evaluations** using these metrics: overall distance (error), agreement rate, pass rate (reliability threshold), and breakdowns by component or domain. We will compile these metrics in tables and figures for comparison.

## Comparative Analysis on the Test Set

**The primary analysis will focus on the held-out test set** (the 40 human-evaluated segments). For each model, we will calculate all the metrics above on this same set, enabling a fair head-to-head comparison. The core question is: **are the differences between models statistically significant, and how large are those differences?**

1. **Performance Ranking:** First, we will establish the ranking of models by performance on the test set. Using the pass rate (or equivalently overall distance) as a summary metric, the expectation (based on preliminary results) is: **Model 5 is best (highest pass rate, lowest distance), followed closely by Model 6, then Model 4, Model 3, Model 2, and Model 1 last**. Concretely, we have evidence that Model 5 achieved \~93.4% pass rate, Model 6 \~92.2%, Model 4 \~91.5%, Model 3 \~87.8%, Model 2 \~81.7%, and Model 1 \~60.9%. This ordering shows a big leap from Model 1 to Model 2 (60.9→81.7), further improvement through Model 4 (\~91%), and a slight edge with Model 5 (\~93%) – Model 6 is roughly on par with Model 4/5. We will verify these numbers by recomputing if necessary (from the raw outputs), but they are already summarized in the repository’s results. We will present a table of these values in the paper for clarity and cite that **the baseline model dramatically lags behind (only \~61% of segments passed), whereas all improved models pass \~82–93% of segments**, with the best achieving over 93%. This directly supports the claim that the base approach is not sufficient, and improvements yield tangible gains in alignment.

2. **Statistical Significance Testing:** To demonstrate that these differences are *not just by chance*, we will perform statistical tests. Since each model was evaluated on the **same 40 segments**, we can use a **repeated-measures analysis** (paired comparison) which is more powerful than treating them as independent. In other words, for each of the 40 segments, we have six data points (pass/fail or a set of distances) corresponding to the six models. We can thus control for segment variability.

   * For a **binary outcome** like segment pass/fail (met the 8/10 criteria or not), we will use **Cochran’s Q test** to assess if there is an overall difference in the proportion of segments passed across the six models. Cochran’s Q is the repeated-measures equivalent of a chi-square test for related proportions (essentially a generalization of McNemar’s test to >2 treatments). The null hypothesis would be that all models have the same pass probability on the segments. Given the numbers (e.g. Model 1 passed 24 out of 40 segments ≈60%, Model 5 passed 37 of 40 ≈93%, etc.), we expect Cochran’s Q to be highly significant (p ≪ 0.01), indicating that at least one model differs. After finding a significant overall result, we will conduct **pairwise McNemar tests** between models (for each pair of models, compare segment-by-segment where one passes and the other fails, etc.) to see which differences are significant. We will apply a **Bonferroni or Holm correction** for the multiple comparisons. However, to reduce the number of tests, we can focus on specific planned comparisons that reflect our hypotheses:

     * **Baseline vs Improved Rubric:** Model 1 vs Model 2 (effect of going from low\_Teach to med\_Teach rubric).
     * **Med vs High Rubric:** Model 2 vs Model 3.
     * **High Rubric vs High+NoNA:** Model 3 vs Model 4.
     * **Gemini 2.0 vs 2.5:** Model 4 vs Model 5.
     * **Formatted vs Unformatted:** Model 4 vs Model 6 (both 2.0).
     * **Best vs Baseline:** Model 5 vs Model 1 (this is essentially the combination of all improvements).
     * Possibly **Model 5 vs Model 6:** to see if there is a significant difference between using a stronger model vs using timestamped input (though their performance is very close, we suspect no significant difference here, which is itself an interesting finding: the formatting tweak nearly matched the larger model’s gain).

     Each McNemar test will yield a p-value for whether the win/loss differences are significant. For example, we expect Model 1 vs Model 5 will be **highly significant**, as there are likely many segments that Model 5 passed which Model 1 failed (and very few the opposite). On the other hand, Model 5 vs Model 6 might not be significant if their pass/fail patterns are almost the same segments (we might find only 1 or 2 segments that one passes and the other doesn’t). We will report these results. If the number of pairwise tests becomes large, we will apply a correction (Holm’s method to maintain overall 0.05 level, for instance) to avoid false positives. Our emphasis will be on the major differences (baseline vs others, med vs high rubric, etc.) to support the narrative of improvement.

   * For a **continuous outcome** like the per-segment distance or number of matching elements, we can use a **repeated-measures ANOVA** or a non-parametric **Friedman test**. Each segment yields an overall distance value for each model (the mean distance across all components for that segment). We can run a one-way ANOVA with “model” as the within-subject factor (with 6 levels). The ANOVA will tell us if there are overall differences in mean distance. We anticipate a very significant effect of model on distance, given, for instance, Model 1’s average distance \~0.53 vs Model 5’s \~0.21 (if converted from 93% agreement, that’s \~0.07 distance if using strict match rate, but in terms of distance as defined it was \~0.26 perhaps when within-one differences count as partial distance). Actually, from reports: Model 5’s overall distance isn’t explicitly listed in text, but likely around 0.25 or lower given its high pass rate. In any case, differences on the order of 0.2 absolute in distance are large. If ANOVA assumptions don’t hold (distance might violate normality slightly as it’s bounded), Friedman test can be used to confirm the ranks differ across models. After that, we will do **post-hoc paired t-tests or Wilcoxon signed-rank tests** between specific model pairs to identify where the significant gaps are. This is analogous to the McNemar tests but treats the actual magnitude of disagreement. For example, we’ll test if the mean distance for Model 1 is greater than for Model 2, etc. We expect to see significant p-values aligning with our earlier list: e.g., Model 1 >> Model 3 (meaning much larger error), Model 3 > Model 4 (some improvement by removing NAs), etc. We will report confidence intervals for the difference in means as well, to show effect sizes (e.g., “Using the high-quality rubric (Model 3) reduced the average error by X percentage points compared to the medium rubric (Model 2), 95% CI \[a,b], p<0.01”). This will substantiate the claim of improvement with statistical rigor.

   * **Note on multiple metrics:** We should be cautious not to run significance tests on every single metric and component, which could lead to a multiple comparisons problem. Instead, our plan is to use the **overall segment-level measures** (like pass/fail per segment or average distance per segment) for the main significance tests. That covers the primary question of whether models differ in overall performance. For the detailed component-level analysis, we will mostly present descriptive statistics or graphs (e.g., “Model 1 vs Model 5 distances for each component”) rather than significance test each component—since there are dozens of components, testing each would indeed inflate false positives. If we do dive into component-level significance, we would limit to a few interesting ones and note it as exploratory. The main statistical assertions in the paper will concern **overall performance differences**, supported by the tests described above. This aligns with best practice: focus tests on key hypotheses (like baseline vs improved differences) rather than every possible comparison.

3. **Quantifying Improvement:** Beyond p-values, we will quantify how *much* better one model is than another:

   * We can calculate the **absolute percentage increase in agreement or pass rate**. For example, Model 3 (high rubric) vs Model 1 (low rubric): if Model 1 had 61% pass and Model 3 had \~88%, that’s a **27 percentage point increase**, which is a massive improvement in reliability. We will highlight such figures in text.
   * We can also present an **odds ratio** interpretation for passing a segment: e.g., “Model 5 is **13 times** more likely to pass a given segment than Model 1,” obtained from (0.934/(1-0.934)) / (0.609/(1-0.609)). This gives a sense of how drastically the odds improve.
   * If using distances, we could say “the average discrepancy was reduced by \~0.19 on a 0–1 scale from Model 1 to Model 3, which corresponds to, for instance, turning about two ‘off by one’ errors into exact matches per segment on average.” We’ll ensure these interpretations are clear and tied to educational meaning (i.e., how many more rubric elements the AI gets right).
   * It’s also useful to compare the models’ performance to **human inter-rater reliability benchmarks**. If available, we will mention what human-to-human agreement is typically for Teach. (Teach manual might say something like human observers usually achieve \~85% segment pass rate on the exam to be certified). If our best model is at \~93% segment pass, that actually *exceeds* typical human reliability standards – a strong result. We’ll note that (this suggests the model might even be more consistent than a second human in some cases, although careful: it was calibrated to the “master” scores directly, so it should match the master closely by design).

4. **Visualization for Test Set Results:** We will create clear visualizations to accompany this analysis:

   * A **bar chart** of each model’s **pass rate on the test set** with error bars (confidence intervals). This will visually underscore the improvement: the bar for Model 1 will be much shorter (\~60%) while Models 4–6 will be near the top (\~90%+). We will include 95% CI or perhaps Wilson intervals for the proportion; since n=40, the CI for 60.9% is roughly ±15% and for 93.4% is ±8%, to illustrate uncertainty. Non-overlap of these error bars between, say, Model 1 and others will show significance at a glance.
   * Alternatively (or additionally), a **line graph** showing the trend as we move from Model 1 to 5 (and 6). We can plot the pass rate or agreement % on the y-axis and the model number (1 through 6) on x-axis, connecting them in order of development. We expect an upward curve that plateaus. For instance, a sharp rise from 1→2→3, a jump 3→4, then a slight rise 4→5, and maybe no rise or a tiny dip 5→6. This will convey diminishing returns: biggest gains came from improving the rubric and data (Model 1 to 4), and using a bigger model (5) gave a smaller additional boost, while reformatting input (6) roughly matched that. Such a plot will help readers see the relative impact of each intervention.
   * A **box plot or violin plot** of the **per-segment agreement rate** by model. Each model’s distribution of “number of elements (out of 10) that matched” can be shown. For example, for Model 1, many segments might cluster around 7/10 correct (failing many since need 8), whereas for Model 5, most are at 9/10 or 10/10 correct. A boxplot would show median and quartiles: we’d likely see Model 1’s median around 8 (just at the threshold) but a wide spread downwards, while Model 5’s median perhaps at 9 or 10. If the spread is not too hard to interpret, this graph complements the pass/fail bar by showing distribution of scores. It also indicates consistency – if Model 5’s IQR is narrow at high scores and Model 1’s is wide, it means the advanced model not only passes more often but does so with more consistency across different segments.
   * A **scatter plot** comparing each segment’s score by Model 1 vs Model 5 (or vs human). We could, for instance, plot for each of the 40 segments the human score on a domain vs the model’s score. But since there are multiple components, this may be too granular. Instead, perhaps a scatter where x-axis is human’s total number of elements passed (which for a master coder might always be 10 by definition of master), and y-axis is model’s number of elements passed. But since master is “ground truth”, maybe this isn’t needed (human baseline is fixed as the reference).
   * Instead, a **heatmap of confusion** could be interesting for specific items: e.g., how often did the model score each category when the human score was X. But given many categories across 9 domains, this might be too detailed for the main analysis. We may include an appendix figure for one domain as an example if needed (like a confusion matrix for the 5-point “Supportive Learning Environment” ratings).
   * **Domain-level bar charts:** We will also visualize each model’s performance on each domain. One way is a grouped bar chart for each domain showing the average distance or the percent of segments reliable on that domain (within one point). For instance, a chart for “Supportive Learning Environment” could have six bars (one per model) indicating what percent of the 40 segments the model was within ±1 of the human on that domain. We would see these percentages climb from Model 1 to 5. Doing this for all 9 domains would be a lot in one figure, so alternatively we might select a couple of representative domains to display (say one that improved dramatically and one that was already high). Or use small subplots for each domain. The domain view will help identify where each model struggled. For example, we suspect **“Classroom Culture” and “Social & Collaborative Skills”** might have been harder for the baseline and only reach high reliability in the later models. If the data show that, we will mention it in text. (From the distance data: Model 1’s worst domain was Classroom Culture 0.65 avg dist, which improved to \~0.34 in Model 6, a big improvement; Time on Task also improved from 0.62 to 0.29). We can say *“The improvements were especially pronounced in maintaining Time on Task and classroom culture elements, where the base model frequently disagreed with human coders, but the later models got those right much more often.”*
   * **Specific component comparison:** If space permits, we might include a short table or a couple of examples highlighting specific rubric items. For instance, we could present: *“Teacher responds to students’ needs: Model 1 agreed in only 12 out of 40 cases (30%), Model 5 in 28 out of 40 (70%) – a large gain but still not perfect, indicating this subtle behavior remains challenging.”* Similarly, *“Teacher encourages goal-setting: Model 1 often missed this (distance 0.71), whereas Model 6 was much closer (distance 0.28), showing the AI learned to better gauge that element.”* These examples give a qualitative feel for what the AI was getting wrong initially and how that improved.

Overall, the test set analysis will form the core evidence. We will emphasize the **statistically significant improvement from the baseline to the advanced models**. The base evaluator with the low-quality rubric will be shown to be *significantly worse* than all others (very low p-values, large effect sizes), reinforcing “needs to be used properly.” We will also likely find that among the top models (4, 5, 6), differences are smaller. We’ll note if, for example, Model 5 vs 6 is not significant – implying that adding timestamps gave roughly comparable benefit to upgrading the model, an interesting point that could mean a cheaper model with better data formatting can compete with a larger model. If any pair of improved models are not statistically distinguishable, we’ll mention that as a sign of diminishing returns (e.g., rubric and data cleaning might have already achieved near-maximal reliability, so upgrading the model from 2.0 to 2.5 only marginally helped).

## Performance on Training/Validation Set

Next, we plan to examine how each model performed on the **training/validation set** (if such evaluation is available or can be done). The purpose of this analysis is twofold: (1) to ensure that the performance improvements we see on the test set were also present (to some degree) in the training phase – which would indicate a consistent pattern – and (2) to detect any signs of **overfitting or generalization issues** by comparing train vs test results.

Depending on how the project was set up, the “Train/Validation” evaluation mode may refer to evaluating the models on the **same set of transcripts that were used to fine-tune or validate rubric prompts**. If that set includes any human-labeled data (e.g., maybe a subset of the 40 or additional segments used for rubric development), we will gather the metrics for that as well. The code does allow evaluation in train/val mode, so likely there is a defined split (perhaps 70% of the real data was considered “Train/Val” and 30% reserved as “Test”).

Our approach:

* **Compute the same metrics on Train/Val:** For each model, run the evaluation on the training/validation split of the data. We will get pass rates, distances, etc., analogous to the test results.  train/val includes real labeled data (for example, maybe they did a 20/20 split of the 40 segments into train vs test), then this becomes very relevant: we can then see how performance on known data vs new data differs.

* **Compare Train vs Test performance:** We will create a comparison (maybe a table or plot) of each model’s pass rate on training vs on test. If the model is not overfitting, these numbers should be relatively close. Perhaps the training pass rate might be slightly higher because the model had exposure (for instance, Model 3 might pass 90% of the segments it was tuned on vs 88% of new ones – a small gap). If we find a model whose training performance is *much* higher than test, that could indicate overfitting. For example, if Model 5 had, say, 99% pass on training but 93% on test, that’s not alarming (that difference is reasonable), but if it was 100% vs 80%, that would be concerning. We suspect that because the improvements we made (better rubric, more data, larger model) are general, they likely improved both train and test similarly. Model 1 might even have struggled on the training data because the rubric was poor and the model was not fine-tuned at all (if Model 1 was essentially zero-shot with rubric prompts, it wouldn’t “overfit” per se; it would just perform how it performs consistently).

* **Statistical tests for train vs test:** If we have a clear train vs test split of real data, we can test for each model whether there is a significant difference in performance between those splits. For example, using McNemar if the same segments were rated (not applicable if splits are different sets), or a two-sample test if different segments (but then they’re independent groups). However, typically the train vs test sets are disjoint, so we’d compare proportions independent: a chi-square or Fisher’s exact test for each model’s pass rate difference between train and test sets. But with likely small sample (if only 20 in train and 20 in test, for instance), power is limited. Instead of formal tests, it might be sufficient to say “All models achieved comparable reliability on the training data as on test data, suggesting no severe overfitting. For instance, Model 5 had a 95% pass rate on the validation segments it saw during development vs 93% on new test segments – essentially the same within margin of error.” If one model did show a drop (e.g., if Model 6’s performance falls a bit on unseen data relative to training), we will note that and possibly hypothesize why (maybe the timestamp formatting was tailored to training data specifics but test had slightly different distribution of transcripts, etc.).

* **Why include training performance:** This analysis will help strengthen our claims by showing that improvements are **robust and not just luck on one dataset**. If Model 5 is truly better, it should be better both on known and unknown sets. Also, if the base model was poor, it likely was poor everywhere. We already suspect Model 1’s performance is inherently low because of its rubric, not because of not generalizing; so it likely did poorly on any data. We might find Model 1’s agreement with the humans on training/val was also around 50-60%. We’ll confirm that. If training data includes some easier segments (maybe more straightforward lessons), possibly Model 1 could be a tad higher there, but fundamentally limited by rubric quality.

* **Visualization:** If applicable, we will extend the earlier bar chart to have two bars per model: one for train, one for test. For example, a grouped bar chart where each model has a pair of bars (train vs test pass rate). If the bars are similar height for each model, that’s a good sign. If we notice any model’s test bar is much lower than its train bar, we’ll flag that. (One scenario: Model 6 might have been slightly “over-tuned” on timestamp formatting if the train transcripts had a pattern that test didn’t; but this is speculative. We’ll see the actual numbers.)

In conclusion of this section, we expect to report that **the model ranking and performance gaps observed on the test set mirror those on the validation set**, confirming that the improvements (rubric quality, data cleaning, model size, etc.) consistently boosted performance rather than just overfitting to one particular subset. This strengthens the argument that the enhancements have general merit.

## Statistical Rigor and Significance Testing

We will ensure that our analysis uses **appropriate statistical methods** and avoids false-positive conclusions:

* As described, we will rely on **omnibus tests (ANOVA, Cochran’s Q)** to first detect overall differences among the models. This protects us from making a slew of pairwise comparisons if, hypothetically, there were no overall differences (though given our data, differences do exist). Only upon a significant overall result will we delve into **pairwise comparisons**, and even then we will use either a correction (like Bonferroni) or **planned contrasts** to limit the number of tests. For example, rather than comparing every pair of models (15 pairs total), we can test a structured hypothesis sequence:

  * Contrast 1: Model 1 vs the average of Models 2–6 (testing “does using AI properly matter at all?” – we expect yes, big difference).
  * Contrast 2: Within the AI-used-properly group, test rubric improvements: Model 1 vs Model 3 (low vs high rubric).
  * Contrast 3: Model 3 vs Model 4 (effect of removing NAs).
  * Contrast 4: Model 4 vs Model 5 (effect of larger LLM).
  * Contrast 5: Model 4 vs Model 6 (effect of formatted input).
    These are just examples of planned tests that align with our incremental changes. Using such contrasts focuses on the meaningful questions and inherently reduces multiple testing.
* We will report **p-values** and consider a threshold (e.g., α = 0.05) after appropriate corrections. If some differences are marginal (say p = 0.07 for a small improvement), we will report them as such and perhaps say “did not reach statistical significance, suggesting the improvement might not be guaranteed beyond sample variation.” However, the critical differences (like Model 1 vs others) will likely be p < 0.001 given the magnitude.
* **Confidence intervals (CIs)** will be provided for key metrics. For instance, a 95% CI for Model 1’s pass rate (60.9% on n=40) is roughly ±15% (so \~46% to \~75%), whereas for Model 5’s 93.4% it’s ±8% (\~85% to \~100%). If we compute the difference in pass rates (32.5 percentage points between Model 5 and Model 1), we can give a CI on that difference as well. This is helpful because it shows the plausible range of improvement. If that CI does not include 0 (which it won’t), it’s another way of stating significance.
* For continuous measures like distance, we can compute the **effect size** in terms of Cohen’s *d* for paired data (which could be computed as mean difference divided by standard deviation of differences). We suspect extremely large effect sizes for baseline vs best (maybe *d* on the order of 1.5 or 2, which is huge). For intermediate improvements, effect sizes might be moderate (e.g., med vs high rubric might be *d* \~0.5–0.8).
* We will check assumptions (for ANOVA, normally distributed differences, etc.). With 40 data points, normal approximation is not bad, and we can supplement with a non-parametric test if needed to be safe (e.g., sign test or Wilcoxon for pair differences).
* We will clearly state how many tests we did and that we applied corrections or used planned comparisons, to reassure reviewers that we did not p-hack our way to significance. The improvements were hypothesized in advance (we expected each change to help), so one could argue for one-tailed tests in those specific directions; however, we might keep it two-tailed for simplicity unless a directional hypothesis is very clear.

By adhering to these practices, our analysis will stand on solid statistical ground and convincingly demonstrate which differences are real. For instance, we anticipate writing something like: *“All pairwise differences between the baseline Model 1 and any improved model were statistically significant (p < 0.001 after Bonferroni correction), confirming that even the smallest enhancement led to a measurable gain. However, between the top two models (Model 5 vs Model 6), the difference in pass rate was not significant (p = 0.5), indicating their performance is statistically on par.”* This gives nuanced insight: improvements plateaued at the end.

Additionally, we will interpret the statistical findings in practical terms. For example, *“Model 3’s agreement was 88%, significantly higher than Model 2’s 82% (χ² test, p = 0.02), meaning that using the fully refined rubric yields about 6 more correct decisions out of 100 than the medium-quality rubric – a non-trivial improvement for observers.”* Grounding stats in real-world impact will help readers appreciate why each percentage point matters in teacher evaluations (e.g., missing even one element could fail a teacher’s certification, so raising reliability from 82% to 88% could drastically increase the consistency of evaluations across many classrooms).

## Visualization Plan

Clear and informative visualizations will accompany the analysis to help readers quickly grasp the differences between models. Below is a plan for specific figures:

* **Figure 1: Model Performance Overview (Test Set)** – a bar chart showing each model’s **segment pass rate (%)** on the test set, with error bars. This figure will directly illustrate the performance hierarchy: expect a short bar for Model 1 (\~61%) and much taller bars for Models 4–6 (\~91–93%). We will label each bar with the model name (or number) and the exact percentage above it for clarity. Error bars will indicate the 95% confidence interval of the proportion (using Wilson score interval for better accuracy given the sample size). This visualization supports the claim “the base AI without proper configuration is far less reliable than the others” by showing its bar way below the others. It also shows that beyond a certain point (Model 4 onward) the gains were smaller – a viewer can see the last three bars are all high and close together, whereas the first bar is dramatically lower. We might color-code the bars by category: e.g., baseline in red, rubric improvements in blue, data tweak in green, model upgrade in purple, etc., to visually tie back to interventions, but we’ll keep it simple so as not to confuse. A caption will note something like, “Pass rate = percentage of 15-min segments on which the model met the Teach reliability criteria (8 of 10 elements correct). Error bars show 95% confidence intervals. All models 2–6 significantly outperform Model 1, and Models 4–6 achieve human-level reliability on \~90%+ of segments.”

* **Figure 2: Improvement Trend by Model Version** – a line graph highlighting the trend as we move through the sequence of models. On the x-axis we’ll place the model numbers 1 through 6 (with maybe abbreviated names), and on the y-axis the overall agreement metric (we could use pass rate again or perhaps overall agreement %). We will plot a point for each model and connect them in order. This will produce a curve that likely rises sharply from 1 to 2 to 3, then a bump at 4, slight rise to 5, and flat or tiny dip to 6. This figure emphasizes the diminishing returns effect and that **most of the gain was achieved by Model 4**, with Model 5 only marginally higher. It answers the question: “How much did each incremental change contribute?” For clarity, we might annotate the segments of the line with labels like “Rubric improved here”, “Removed NAs here”, “LLM upgrade here”, etc. For instance, the jump between 1 and 2 can be labeled “Better rubric prompts”, 3 is “Manual rubric refinement”, 4 is “Excluded NAs”, 5 is “Gemini 2.5 model”, 6 is “Timestamped transcripts”. This provides a visual narrative of the project’s iterative approach.

* **Figure 3: Distribution of Agreement per Segment** – this could be a box plot of the **number of elements out of 10 that each model got correct** (within the allowed difference) per segment. Each model would have a box spanning the interquartile range of scores (0–10 scale for each segment’s “score” where 10 means perfect agreement on that segment). For example, Model 1 might have a median around 7 or 8, with a lower quartile maybe 6 (some segments it did very poorly), whereas Model 5 might have median 10 (half the segments it was spot-on or very close) and lower quartile perhaps 9. Whiskers could show the range (we might see Model 1 actually had a minimum of maybe 4 out of 10 on some tough segment, and Model 5 minimum maybe 7 or 8 out of 10). We can overlay individual points as well since n=40, to see the spread. This figure complements the bar chart by conveying consistency: not only is the median higher for improved models, but the variability is lower (no extreme failures). It also highlights if there are any outlier segments that were problematic for all models (e.g., if one segment was weird and even Model 5 only got 5/10 on it, it would show up as an outlier point for all possibly – that might indicate an out-of-scope case). If we notice such a segment, we might discuss it qualitatively (maybe that transcript was very unusual or low-quality audio, etc.). This helps demonstrate that the advanced models perform reliably *segment after segment*, whereas the baseline was hit-or-miss.

* **Figure 4: Domain-Level Performance** – a multi-panel figure or set of small charts, one for each Teach domain, comparing models. We could have four panels corresponding to the four domains (Time on Task, Classroom Culture, Instruction, Socioemotional Skills). In each panel, the y-axis is the average distance (or error rate) for components in that domain, and x-axis is the model number. We’ll effectively break down the overall distance into domains. We expect to see, for instance, in the Time on Task panel: Model 1 error high, then steep drop by Model 3 or 4, and staying low by Model 5/6 (since snapshot agreement improved a lot with better rubric and training). In the Socioemotional Skills panel, we might see error start high and remain somewhat higher even in Model 5, indicating that domain remained challenging. If the patterns are too detailed, we might simplify by just reporting domain stats in text. Alternatively, we can present a radar/spider chart where each axis is a domain and lines for Model 1 vs Model 5 show how error shrank across all domains. However, spider charts can be hard to read precisely. A simpler approach: a table listing each domain’s average score for each model (but tables are less visual). Given space constraints, we may choose one or two domains to visually highlight: for example, *Time on Task vs Socioemotional Skills*, showing how the former achieved near-perfect reliability in advanced models (distance near 0.0) while the latter still had some discrepancies (distance maybe \~0.2–0.3 for best model). This would underscore that **AI handled objective, observable behaviors (like students on-task, teacher providing activity) very well with improvements, but more subjective skills (like responsiveness or encouraging perseverance) still left room for error** – a nuance important for the discussion.

* **Figure 5: Reliability Exam Simulation (if needed)** – if we want to emphasize how consistently each model passes the official exam criteria, we could include a small chart of the **probability of certification** for each model. For example, a bar or line showing probability of passing all 3 segments in one attempt: Model 1 \~22%, Model 2 \~54%, Model 3 \~68%, Model 4 \~77%, Model 5 \~81%, Model 6 \~78% (numbers illustrative). And perhaps probability of passing within two attempts: Model 1 \~40%, Model 2 \~>90%, etc. However, since the text summary already can convey “6 of 6 models got certified in simulation”, a figure might not be necessary. We could alternatively incorporate this info into Figure 1’s annotation (like a note that Model 1 would rarely pass a full exam while others would). If a visual is desired, a simple bar chart of **“Exam pass rate in one try”** could be powerful: it would basically be the cube of the per-segment pass rates. Model 1’s bar at \~22% would look dramatically low compared to Model 5’s \~82%. This essentially reframes the earlier data but in terms of full exam success. The drawback is it might be somewhat redundant with segment pass rate, but it emphasizes the compounding effect of needing consistency across three segments.

* **Figure 6: Examples of Specific Components** (optional/appendix): If we want to illustrate one example in detail, we might pick a single Teach element and show a bar chart of how each model scored on that element across segments. For instance, “Students are on task – 3rd Snapshot” could be chosen if it was something with variation. However, this might be too granular for the main paper. Another idea is a **confusion matrix for a single domain** – e.g., for “Positive Behavioral Expectations” (which is scored 1–5 by human), show how often Model 1 vs Model 5 gave each score when the true score was X. Model 1 might have more scatter (e.g., often one off or two off), whereas Model 5 maybe almost always within one. This visual can concretely show the calibration improvement of the model’s scoring. Since the Teach rubric often only takes integer scores, such confusion matrices can be constructed.

In all visualizations, we will use **consistent labeling and color schemes**, and include legends where needed. We will also cite the source of data for these visuals (which would be our evaluation CSVs and reports). For instance, a caption might say “Data source: model evaluation results on 40 Peru classroom segments.” This assures readers that the visuals are grounded in the recorded data.

The visualizations will be integrated into the analysis narrative: e.g., *Figure 1* will be referenced when we first compare overall model performance, *Figure 4* when discussing domain differences, etc. They will make the findings easier to digest at a glance, supporting the verbose descriptions.

## Reliability Exam Consistency and Certification

A key part of our analysis is to demonstrate that **the improved AI models can effectively “pass” the Teach reliability exam consistently, similar to a trained human observer**. We will explicitly connect our results to the criteria for certification:

* **All improved models meet the threshold:** Our findings show that Models 2–6 all have segment pass rates well above the requirement (which roughly translates to needing ≥80% of elements correct per segment). In fact, Models 4, 5, 6 have >90% segment reliability, meaning in nearly every 15-min segment they meet the 8/10 standard. Even Model 2 (the one with just a medium rubric) was at \~82%, just above the threshold on average. This implies that any model from 2 onward would likely pass a full 3-segment exam in one attempt or at worst in two attempts. We will emphasize that **Models 4–6 achieved performance comparable to an expert – passing virtually all segments**. This is strong evidence that with the proper setup (rubric and training), the AI *“is good”* in the sense of replicating human judgments.

* **Baseline model’s unreliability:** Model 1, however, only passed 60.9% of segments. This means it failed 39.1% of the time on single segments. We can interpret that as: if a human observer had this level of reliability, they would *not* be certified (since failing even one of the three segments fails the attempt, and Model 1 would on average fail \~1.2 segments out of 3). Indeed, we can calculate that Model 1 had a very low chance (\~23%) to pass a given 3-segment exam on the first try. Even with a second attempt, its chance to eventually pass is around 40%. So in many cases, the baseline model would **fail to meet the standard required by the Teach program**, underlining that it’s not sufficient to deploy as-is. We will make this point clearly: *“Using the naive rubric, the AI’s agreement was so inconsistent that it would frequently flunk the certification test that human observers must pass. This demonstrates that without careful design, an AI evaluator would not be acceptable for practical use.”* This addresses the cautionary side of our message: AI needs to be configured properly.

* **Certification simulation:** In the repository, a simulation was done where each model was given multiple chances to pass the exam. The result was that all six models were “certified” after at most two attempts. We need to reconcile this with our interpretation that Model 1 is borderline. Likely, in the simulation, Model 1 happened to pass on the second attempt (maybe by luck of which segments were drawn). We will clarify that **“certified = Yes”** in the summary means that in at least one out of two simulated exam attempts the model succeeded. This is a somewhat lenient criterion (since a human would actually get only two tries and could fail both). For the paper, we might state: *“All models were eventually able to get certified in a simulated two-attempt exam, but the baseline model’s success required a second attempt and favorable selection of segments, whereas the top models would pass almost any set of exam videos on the first try.”* This distinction is important – it shows consistency. We could even quantify: Model 5 has \~82% first-try success, Model 1 only \~23%. We can phrase it as “Model 5 would likely pass the exam **on the first try in 4 out of 5 cases**, while Model 1 would pass **in only about 1 out of 5 cases** (and often not even on a second try).” This drives home the reliability difference in practical terms.

* **Demonstrating consistent passing:** To visually or quantitatively demonstrate consistency, we might simulate each model’s exam 1000 times (drawing random sets of 3 segments each time) and see what fraction of attempts result in certification. We expect results like: Model 5 \~82% one-attempt pass, Model 1 \~22%; with two attempts allowed: Model 5 \~ >98%, Model 1 \~ \~40%. We can present these as part of the analysis (perhaps in a small table or just in text form). This shows that for the strong models, passing the exam is almost guaranteed (they’d have to be extremely unlucky to fail two attempts), whereas for the base model, failing is the more likely outcome even after two attempts.

* **Human-level benchmark:** If available, mention how human observers typically perform. For example, if in training sessions humans pass, say, 90% of segments on average, then our Model 4–6 being \~90% indicates they have reached human-level consistency. Model 1 at 61% is far below that. This context can be used to argue that *“the best AI model can *consistently* emulate an expert rater, passing the reliability exam with ease, whereas the baseline AI would not meet the bar set for humans.”*

* **Areas of concern:** Even though all improved models passed the exam threshold on average, we will identify if any model had particular trouble spots that could risk failing a segment. For instance, if Model 3 was at 88%, it might still fail 1 out of 8 segments. We should see if those failures cluster in a specific domain. Perhaps Model 3 might often miss one particular element (like always mis-scoring “Encourages goal-setting”), causing segment failures. If we identify such a pattern, we’ll mention it. For example, *“Model 3’s few segment failures often involved the same element – in about half of the failed segments, the model was more than one point off on the ‘goal-setting’ item. This suggests that addressing that single item (perhaps via additional training examples or rubric clarification) could further boost reliability.”* This kind of insight can be useful for future work.

In terms of **reporting**, we will dedicate a portion of the results to explicitly state how many models met the certification criteria and how reliably:

* *“Models 2–6 achieved certification-level performance, with Models 4–6 passing >90% of segments (far above the 8/10 element threshold). The baseline Model 1, in contrast, fell short, passing only 61% of segments – well below the standard.”*
* *“All models ultimately passed a simulated Teach exam given two attempts, but the baseline required the maximum tries and still had a low overall probability of success (\~40%), whereas the top models would pass nearly 100% of the time within two attempts.”*
* We will reference the official Teach criteria as needed to clarify these points (the description of requiring 8 of 10 elements per segment, 3 segments in a row). This ties our technical metrics back to a real-world acceptance criterion.

## Implementation Considerations (Automation and Reproducibility)

In performing this analysis, we will ensure that our code is written in a **generalizable way** so that it automatically picks up all models and integrates any new models in the future. Specifically:

* We will **avoid hard-coding model names** in the analysis scripts. Instead, we’ll programmatically retrieve the list of models from the results directory. For example, by listing all CSV files in `model_evaluation_data/` we can get filenames like `1-BaseEvaluator_evaluations.csv`, `2-BaseEvaluator-Validate.csv`, ..., `6-BaseEvaluator-Validate-timestamped-data-g-20-flash.csv`. This list can be sorted or ordered by model number (which is the prefix before the first dash). This way, if later on a `7-...` model is added, the analysis will include it automatically. The same goes for reading any report or summary files.

* Each CSV of model evaluations contains the model’s outputs for each segment along with the human reference (the snippet provided is an example for Model 6) – we can verify that each CSV has a column for `model_name` which is constant (e.g., “6-BaseEvaluator-Validate-timestamped-data-g-20-flash” repeated). We can use that or the filename to label data in plots.

* We will load all model evaluation data into a single dataframe for easy comparison. Because all CSVs have the same format (columns for each rubric item), we can stack them and then filter by model as needed. We’ll be careful that some models might have slightly different rubric columns if one allowed N/A and others not – but since the high-level Teach 1.0 framework is the same, the columns should match (the NA just means there were some blanks in data for certain models).

* We will reuse or adapt the code used to compute pass rates and distances (the `evaluate_segment` function, etc.) to ensure we’re applying the exact official criteria. However, since the results already exist (in `model_performance_metrics.csv` and `reliability_exam_summary.txt`), we can cross-verify our calculations with those to avoid discrepancies.

* For significance tests, we will use standard libraries (e.g., SciPy or statsmodels in Python, or R if we export data). The analysis code will clearly label outputs with the model names, which come from the data itself. This avoids any mix-up or mis-ordering.

* We will also incorporate **logging or summary outputs** that list the model names being analyzed and their key properties (maybe read from the model config README). This provides traceability – e.g., our code can print “Analyzing 6 models: \[names]” to confirm it detected all models. Indeed, the notebook already prints configured models and dataset split when run.

* In plots, instead of generic labels like “Model1, Model2”, we will use the actual descriptive names or abbreviated forms (e.g., “M1: Base (low rubric)”, “M5: +Gemini2.5”). This makes the plots self-explanatory. We can fetch the descriptions from the README or define them in a dictionary for the known models. To be future-proof, if a new model 7 appears, it likely would also have a naming scheme we can parse (maybe “7-…”); we might not have a hard-coded description for it, but we can at least display its raw name. If maintaining a mapping, we’ll document that it needs updating when new models are added.

* We should also ensure the **statistical analysis updates automatically**. For example, if a new model 7 is added that actually performs worse than model 5, the code should incorporate it into the ANOVA and detect that. Using dynamic loops over models ensures this. If we had hard-coded a pairwise comparison plan strictly for 6 models, we might need to generalize that. One approach is to always compare each new model to the previous one, etc., or to baseline, etc. For the paper, since we know we had six, we can present results for those six. But for internal purposes, keeping it flexible is good.

* We will document any errors encountered (if any) during this process. For example, if reading a file fails or if a model’s output is missing some components (maybe due to N/A removal), we’ll handle that by aligning with the framework JSON to ensure we only compare on common components. Actually, since high\_Teach removes NAs completely, by Model 4 onward every component has a value. Model 1 and 2 might have “N/A” in some fields (since their rubric allowed it). If our test set had any N/As in human data, those would appear as blank in CSV. But presumably, in the 40 test segments, they might have filled everything (or replaced N/A with something for the sake of evaluation). We will confirm if any components in the CSV are empty for some models. If so, we might exclude those components from distance for fairness or treat N/A as a neutral placeholder that doesn’t count against the model (the code likely ignored N/As in distance by design).

* Lastly, we will use seeding or deterministic methods for any random simulation (like exam draws) if we present those results, so that the analysis is reproducible. Given n=40, we could enumerate combinations for exact probabilities, but random simulation with a fixed seed is fine too.

By following these practices, our analysis will be **robust, reproducible, and easily extensible**. This ensures that as the project grows (adding new models or data), the analysis can be updated with minimal effort, and it instills confidence that our results are not cherry-picked but rather are an accurate reflection of the data.

## Conclusion (Key Takeaways for the Paper)

At the end of this extensive analysis, we expect to conclude with points substantiated by our findings:

* **The importance of using AI properly:** We will have shown that the AI model with an *inadequate rubric and setup (Model 1)* performed far below acceptable levels (failing the reliability standard \~40% of the time). In contrast, with the right rubric design (Model 3 and beyond) and data preparation, the AI’s performance jumped dramatically, eventually matching human expert reliability (Models 4–6 around 90%+ agreement). This validates the claim that *“AI is good, but needs to be used properly.”* The data will illustrate that **rubric quality was a critical factor** – moving from a low-quality rubric to a high-quality one halved the model’s error (distance dropped from \~0.53 to \~0.35, and agreement rose from \~47% to \~66%). Each subsequent fix (removing NAs, using a better model, formatting input) yielded additional gains, albeit smaller than the rubric overhaul.

* **Statistical confirmation of improvements:** We will report that all major improvements were statistically significant. For example, “The difference between Model 1 and Model 3 (low vs high rubric) was highly significant (p<<0.01), as was the jump from Model 3 to Model 4 (p<<0.01), confirming that both rubric refinement and data completeness significantly improve AI-human agreement.” If any differences were not significant (e.g., Model 5 vs 6), we’ll note that as well: “No significant difference was found between Model 5 and Model 6 (p=0.5), indicating that using Gemini 2.5 or adding timestamps produced roughly equivalent outcomes.” This nuance suggests where future effort is best spent (e.g., perhaps focusing on model architecture beyond a point yields diminishing returns compared to other factors).

* **Consistency with training data:** We will assert that the model improvements generalize, because the patterns observed on test data were consistent with training/validation performance (and we did not find evidence of overfitting). If anything, the top models might slightly *under*-perform on new data relative to training (which is normal), but not by a large margin. That means the improvements are **real and robust**.

* **Human-level reliability achieved:** By referencing the Teach criteria, we will be able to say that our best models **meet or even exceed human reliability**. For instance, if typically a human needs 8/10 to pass, our Model 5 got on average 9.3/10 correct – effectively performing like a highly reliable observer. We might mention that the Teach manual expects observers to agree on \~85% of items (not sure the exact figure, but often inter-rater agreement in such tools is around 85%). Our Model 5 was at \~93% agreement, which is very high. This suggests AI can not only match but potentially provide more consistent scoring (since an AI, once calibrated, does not have off days or fatigue). Of course, we’ll temper that by noting AI cannot replace human judgment entirely and that the context of the transcript matters – but purely in terms of scoring consistency, the data is promising.

* **Areas for improvement:** The detailed breakdown might reveal a few weaknesses that remain. We’ll point those out to honestly assess limitations. For example, we might say “While overall reliability is high, the AI still struggled on certain fine-grained behaviors such as detecting whether the teacher responded to all students’ needs. Even the best model disagreed with the human evaluator on that component in \~35% of cases. This might be due to limitations in the transcript content or subtlety in that behavior. Further work may be needed to address these specific components.” This shows the reader that we’ve done a deep dive and are aware of what an AI can or cannot reliably code.

* **Visual evidence:** We will include the charts described (overall performance, trends, distributions, etc.) in the paper’s results section to visually back up each claim. Each will be cited from our data sources, ensuring transparency.

In essence, the plan above ensures that we provide a **comprehensive, statistically sound comparison of the six models**, directly supporting our claims about AI evaluator performance. By following this plan, we will produce compelling evidence that **the enhancements made during development (better rubric, data curation, model updates) led to tangible and significant improvements**, and we’ll quantify those improvements in terms of the Teach framework’s standards. This will form a strong basis for the research paper’s conclusions and recommendations.

Finally, all analysis steps and findings will be documented thoroughly so that readers (and reviewers) can trace every claim back to the data. This level of detail and rigor will bolster the credibility of our work and convincingly answer the question: *Did our improved AI models actually perform better, and by how much?* The answer, supported by the data, will be **yes – dramatically so – and here’s the proof**.

**Sources:**

* Model configuration details and rubric versions
* Performance metrics and reliability results
* Evaluation reports for Model 1 and Model 6 (example of distances by component)
* Teach reliability criteria and implementation reference
