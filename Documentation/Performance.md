## Official Teach Reliability Criteria vs. Implementation

**Official Criteria:** The World Bank’s *Teach* manual defines clear thresholds for observer reliability. To pass each 15-minute video segment in the reliability exam, an observer must be “reliable on 8 out of the 10 elements”. This means:

* **Time on Learning (TOL):** The observer’s codes must exactly match the master codes on at least 2 of the 3 snapshot checks. In other words, for each snapshot the observer must agree **exactly** with the master on both sub-parts (whether the teacher provided a learning activity and the students’ on-task level), and this exact agreement must occur in **2 out of 3 snapshots**.
* **Quality of Teacher Practices (9 elements):** For each of the nine high-inference elements (e.g. Supportive Environment, Lesson Facilitation, etc.), the observer’s score is considered reliable if it is within one point of the master score. For example, if the master score for an element is 3, an observer’s score of 2, 3, or 4 counts as reliable on that element.

To **pass a segment**, the observer must meet the above reliability criteria on at least 8 of the 10 total elements (the 9 QTP elements plus TOL). All three video segments must be passed independently for certification – failing any one segment (e.g. only 70% of elements reliable) means failing the exam. The exam allows a second attempt with three different videos if the first attempt is failed.

**Code Implementation:** The notebook `6_Performance.ipynb` correctly implements these rules. In the code’s `evaluate_segment` function, we see that:

* **TOL exact matches:** It checks each of the three snapshots and counts a “match” only if **both** the teacher-activity flag and student on-task level agree with the human (master) code for that snapshot. It then requires at least 2 out of 3 such snapshot matches to consider the Time on Learning element “reliable” (`time_ok`). This aligns exactly with the manual’s requirement of 2/3 exact agreement for TOL. The code treats the whole TOL element as one point toward reliability, as it should – it gives a score of 1 if `time_ok` is true (i.e. TOL passed) or 0 if not. This ensures the three snapshots collectively count as a single Teach element, matching the official scoring scheme. (Had the code instead counted each snapshot separately, it would mis-weight TOL, but here it correctly uses `time_ok` as one element.)

* **Within-one for 9 QTP elements:** The code then iterates over the 9 quality practice elements and converts the human and model scores to numeric values (using `alpha_to_numeric`). It checks `abs(h – a) <= 1.0` for each element, marking that element as reliable if the model’s score is within one point of the human code. This exactly captures the “within one point” criterion from the Teach guidelines (e.g. model score 2 vs master 3 is acceptable). Any element where the difference is greater than 1 (or if a score is missing) is treated as unreliable (False). The list of quality elements in the code corresponds to the 9 high-inference elements of Teach (Supportive Environment, Positive Behavior Expectations, Lesson Facilitation, etc.), so no elements are omitted or added incorrectly.

* **8/10 threshold per segment:** After evaluating TOL and each QTP element, the function sums up the total number of elements on which the model was reliable. It adds 1 if the TOL criterion was met (`time_ok`) and adds the count of QTP elements that were within-one (`quality_count`). It then returns True (segment passed) if and only if the total reliable count is ≥ 8. This exactly implements the “reliable on 8 of 10 elements” rule from the Teach exam (8 or more out of the 10 possible elements counts as a passing segment). Notably, this logic allows the same flexibility as the official criteria – for instance, a segment can still pass even if the model missed the TOL element, as long as it got at least 8 of the 9 QTP elements within one point (0 + 8 = 8). Likewise, a segment can pass if TOL is correct and at least 7 of 9 QTP elements are within-one (1 + 7 = 8). This aligns with the manual’s intent, even though the notebook’s markdown description phrased it as “8 of 9 high-inference elements”. The **implementation** in code indeed uses 8 out of **10** (counting TOL as one), which is faithful to the official rule – so there is no actual mismatch in the logic. (In other words, the code does *not* mistakenly require 8 of 9 QTP in all cases; it correctly counts TOL as well.)

* **Pass all three segments & exam attempts:** The notebook provides functions to simulate the exam process. In `run_random_exam`, the code randomly selects three segments for an “attempt” and checks each segment with `evaluate_segment`. It immediately fails an attempt if any one of the three segments does not meet the 8/10 criterion, by breaking out as soon as a segment is not reliable. If all three segments in that set pass, the attempt is marked passed. The model is marked `certified=True` only if it achieves a passed attempt within the allowed number of attempts (default two). This mirrors the certification rule that **all three videos must be passed independently** to succeed (e.g. one segment failure means the whole attempt fails). It also reflects the two-chance structure of the exam: the code allows up to 2 attempts and stops if the first attempt is successful, which parallels the official policy of a second exam if the first is failed.

**Conclusion:** The evaluation logic in `6_Performance.ipynb` **adheres strictly** to the Teach reliability exam criteria. The code correctly implements exact agreement on 2 of 3 snapshots for TOL and within-one-point scoring for the 9 QTP elements, and it requires at least 8/10 elements per segment to count as a pass. It also correctly enforces that a model (or observer) must pass all three segments (with up to one retake) to be certified. We find no major discrepancies between the code and the official criteria – the approach is aligned with the World Bank’s Teach manual and training guidance.

**Minor Suggestions:** The implementation is essentially correct, but a couple of minor refinements could improve clarity or fidelity: (1) Clarify documentation phrasing to avoid confusion – for example, explicitly state “8 of 10 elements (including TOL) per segment” rather than “8 of 9 high-inference elements” in the notebook text, to match the formal criterion. (2) In the exam simulation, ensure the second attempt uses a different set of videos than the first, since the official second exam uses a new set of three videos. Currently, the code’s random selection could by chance reuse a segment in both attempts; enforcing unique segments across attempts would more faithfully emulate the real exam procedure (though this does not affect the reliability scoring logic itself). Overall, however, the core reliability checks and pass/fail determination in the code **fully implement the Teach reliability exam requirements**, as evidenced by the one-to-one correspondence with the official guidelines.
